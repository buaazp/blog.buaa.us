{"meta":{"exported_on":1416225132691,"version":"002"},"data":{"posts":[{"id":3,"uuid":"c0de0c46-1205-4277-8724-938c0a9c3b0c","title":"about","slug":"about","markdown":"**buaazp**\n===\n*Life is short, but love is eternal.*\n\nMy blog:\n---\n[http://blog.buaa.us](http://blog.buaa.us)\n\nMy projects:\n---\nzimg: [http://zimg.buaa.us](http://zimg.buaa.us)\n\nstress: [https://github.com/buaazp/stress](https://github.com/buaazp/stress)\n\nGoAgentUI: [http://goagentui.buaa.us](http://goagentui.buaa.us)\n\nHiOS: [https://github.com/buaazp/HiOS](https://github.com/buaazp/HiOS)\n\nContact me:\n---\nweibo: [http://weibo.com/buaazp](http://weibo.com/buaazp)\n\nE-mail: zp@buaa.us","html":"<h1 id=\"buaazp\"><strong>buaazp</strong></h1>\n\n<p><em>Life is short, but love is eternal.</em></p>\n\n<h2 id=\"myblog\">My blog:  </h2>\n\n<p><a href=\"http://blog.buaa.us\">http://blog.buaa.us</a></p>\n\n<h2 id=\"myprojects\">My projects:  </h2>\n\n<p>zimg: <a href=\"http://zimg.buaa.us\">http://zimg.buaa.us</a></p>\n\n<p>stress: <a href=\"https://github.com/buaazp/stress\">https://github.com/buaazp/stress</a></p>\n\n<p>GoAgentUI: <a href=\"http://goagentui.buaa.us\">http://goagentui.buaa.us</a></p>\n\n<p>HiOS: <a href=\"https://github.com/buaazp/HiOS\">https://github.com/buaazp/HiOS</a></p>\n\n<h2 id=\"contactme\">Contact me:  </h2>\n\n<p>weibo: <a href=\"http://weibo.com/buaazp\">http://weibo.com/buaazp</a></p>\n\n<p>E-mail: zp@buaa.us</p>","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402551002633,"created_by":1,"updated_at":1402551071895,"updated_by":1,"published_at":1402551002649,"published_by":1},{"id":6,"uuid":"e988a7aa-b3e1-424b-9327-d1675010eba3","title":"在shell脚本中实现urlencode","slug":"urlencode-in-shell","markdown":"## 在shell脚本中实现urlencode\n\n[@招牌疯子](http://weibo.com/819880808)\n\nurlencode说白了就是字符串替换，用php实现很easy，用脚本来就相对比较麻烦。先看一下这个脚本：\n\n```\nsource $1\nid=$2\npass=$3\nnonce=0\n\npa=`echo \"$nonce$pass\" | openssl rsautl -encrypt -inkey test.pub -pubin |openssl base64`\n\nfunction encodeurl()\n{\n    encoded_str=`echo \"$*\" | awk 'BEGIN {\n        split (\"1 2 3 4 5 6 7 8 9 A B C D E F\", hextab, \" \")\n        hextab [0] = 0\n        for (i=1; i<=255; ++i) {\n            ord [ sprintf (\"%c\", i) \"\" ] = i + 0\n        }\n    }\n    {\n        encoded = \"\"\n        for (i=1; i<=length($0); ++i) {\n            c = substr ($0, i, 1)\n            if ( c ~ /[a-zA-Z0-9.-]/ ) {\n                encoded = encoded c             # safe character\n            } else if ( c == \" \" ) {\n                encoded = encoded \"+\"   # special handling\n            } else {\n                # unsafe character, encode it as a two-digit hex-number\n                lo = ord [c] % 16\n                hi = int (ord [c] / 16);\n                encoded = encoded \"%\" hextab [hi] hextab [lo]\n            }\n        }\n        print encoded\n    }' 2>/dev/null`\n}\n\nencodeurl $pa\np=$encoded_str\n\npostdata=\"id=$id&p=$p\"\ncurl  $phpurl/index.php/login  -d $postdata\n```\n\nid和pass是用户名和密码，经过公钥和base64加密后再进行urlencode，用法就是encodeurl $pa然后取函数体中的$encoded_str就行了。","html":"<h2 id=\"shellurlencode\">在shell脚本中实现urlencode</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>urlencode说白了就是字符串替换，用php实现很easy，用脚本来就相对比较麻烦。先看一下这个脚本：</p>\n\n<pre><code>source $1  \nid=$2  \npass=$3  \nnonce=0\n\npa=`echo \"$nonce$pass\" | openssl rsautl -encrypt -inkey test.pub -pubin |openssl base64`\n\nfunction encodeurl()  \n{\n    encoded_str=`echo \"$*\" | awk 'BEGIN {\n        split (\"1 2 3 4 5 6 7 8 9 A B C D E F\", hextab, \" \")\n        hextab [0] = 0\n        for (i=1; i&lt;=255; ++i) {\n            ord [ sprintf (\"%c\", i) \"\" ] = i + 0\n        }\n    }\n    {\n        encoded = \"\"\n        for (i=1; i&lt;=length($0); ++i) {\n            c = substr ($0, i, 1)\n            if ( c ~ /[a-zA-Z0-9.-]/ ) {\n                encoded = encoded c             # safe character\n            } else if ( c == \" \" ) {\n                encoded = encoded \"+\"   # special handling\n            } else {\n                # unsafe character, encode it as a two-digit hex-number\n                lo = ord [c] % 16\n                hi = int (ord [c] / 16);\n                encoded = encoded \"%\" hextab [hi] hextab [lo]\n            }\n        }\n        print encoded\n    }' 2&gt;/dev/null`\n}\n\nencodeurl $pa  \np=$encoded_str\n\npostdata=\"id=$id&amp;p=$p\"  \ncurl  $phpurl/index.php/login  -d $postdata  \n</code></pre>\n\n<p>id和pass是用户名和密码，经过公钥和base64加密后再进行urlencode，用法就是encodeurl $pa然后取函数体中的$encoded_str就行了。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402647111206,"created_by":1,"updated_at":1402649468077,"updated_by":1,"published_at":1402647111208,"published_by":1},{"id":7,"uuid":"f17a4a72-33c8-4ee5-bd4a-086330acf6d6","title":"探讨iOS应用里的缓存设置","slug":"cache-config-in-ios-applications","markdown":"## 探讨iOS应用里的缓存设置\n\n[@招牌疯子](http://weibo.com/819880808)\n\n今天碰到某度网盘产品讨论在爱疯上的缓存设置及其策略，我表示异常困惑，于是决定在这个常年除了垃圾回帖外就没有别的在更新的博客上写一写，算是别浪费了租用空间的费用。\n\n网盘类的产品，由于预览和保存的需要，必然会从服务器上下载用户从其他端上传过的，部分或者全部的文件内容。好比一部www.av9.cc-ABS130.avi，你在iPad网盘上想点击观看，肯定需要把整个文件都下载下来；但是当你用2M宽带小水管屁颠屁颠地下完之后才发现，iPad自带播放器打不开avi格式，你瞬间想要暴粗口了，明显avi坑了av。\n\n抛开用第三方应用打开不谈，你所下载的980.3M数据一下子变成了无用数据，于是网盘把它们叫做“缓存数据”。再好比你去泰国跟人妖合影的照片，你用网盘一点击，很短的时间它就打开了，当你看见相貌骄人的人妖MM时心情立马比刚才的ABS130要好了几千倍，然而此时你所下载的内容，其实并不是照片本身，而是一个被压缩过的质量略差的缩略图而已，网盘依然把它叫做“缓存数据”；如果你仔细观察会发现照片下方还有一个五角星按钮，这是干嘛的呢，你好奇地一点，屏幕中间赫然弹出一个大笑脸外加黑底白字的告诉你“收藏成功”。\n\n当网盘终于把这个数据叫做“收藏数据”的时候，你会神奇地发现在设置页里头有一个比上面所说的策略复杂一万倍的界面让你来算计、清理你的数据。真是我待缓存如初恋，缓存虐我千百遍也不为过。“坑爹哪！”\n\n虽然我不知道这是哪位蛋疼的产品经理想出来的东西，但是当一些策略会搞得产品经理自己都头大无语的时候，这些策略就是shi一样的。看图：\n\n![baidu_netdisk](http://single1024.qiniudn.com/wp-content/uploads/2012/08/IMG_1620.png)\n\n我相信没有人会有兴趣在着急下载ABS130的时候去搞明白这些玩意到底是怎么加起来才能不超过500M吧，更别提当我们自以为搞懂了胆战心惊地点击这个大红按钮之后，会弹出一段让你更加看不明白的提示语，就差给你弹一句“PM的心思你别猜”了。更让人无语的是，竟然还要以“这东西肯定要做”为理由来设计这个东西，莫非是酷盘最近风生水起就一定要以他们为行业标准么。那我们再截图看看酷盘是什么德行：\n\n![kupan](http://single1024.qiniudn.com/wp-content/uploads/2012/08/IMG_1619.png)\n\n当我看到大红色按钮下方的一排“如需清除本地收藏，请先取消收藏文件”的时候我差点没吐出血来，你们怎么不直接跟用户说“如果怕空间不足建议不要使用本产品，不使用本产品不会占用您的任何手机空间”呢？\n\n好吧，吐槽结束说点正经的。\n\n缓存很容易理解，为了使某些使用频率较高的内容能够更快地访问而保存在本地，避免再次下载浪费流量和时间。说白了，这些数据是设计给程序用的，它们能用多大空间，什么时候被清理，都应该根据程序的需要来调整，根本就不应该让用户去计算！\n\n在PC上，浏览器是最常用到缓存的，你第一次登录微博会发现加载很慢，但是下次登录的时候就快多了，是因为浏览器把微博的很多图片啊js文件啊保存在某个tmp目录下，这情景跟手机上使用网盘【预览】是完全一样的；与之相对，你在某网站上看到一个好用的软件，然后点击下载把它下在了自己的电脑上，手机上是【收藏】，只不过iOS不让你随便选路径罢了；那你见过哪个浏览器的设置里面有一项是说“您的下载和缓存总空间是500M，如果您电脑空间不足，请点击清空缓存释放。但是，清空的不包括您的下载内容哦~”\n\n说白了，缓存其实应该叫“临时文件”，它们并不是用户最需要的东西，该清理时就得毫不留情地清理，如果你的算法设计得足够好，必然能在空间和速度上找到一个平衡点，这正是广义上缓存这个策略存在的意义，而不是偷懒让用户的脑子去计算；收藏才是关键的内容，它们是用户确定之后才保存下来的，你当然不能随便清理，当用户发现自己的手机空间不足了，他应该像在电脑上一样一个一个比较，将自己认为不重要的文件【取消收藏】，而且这部分工作，在计算机行业发展到能够智能准确地识别用户兴趣爱好之前，还是得用户来做。\n\n设置项里面，只需一个清空缓存按钮即可。假如缓存清理了还不够用，那只能挨个取消收藏了（我能再说一句这个收藏的称呼是尼妈想出来糊弄人的吧？）。为此我调研了iOS上流行的一些应用，和几乎全部的网盘类产品，也就某度和酷盘在设置页里故弄玄虚自欺欺人而已。某数字公司起码在这一项上符合我的期望。\n\n![360](http://single1024.qiniudn.com/wp-content/uploads/2012/08/IMG_1618.png)\n\n更让人觉得无语的是，就这些让人啼笑皆非的脑残设计，居然还要浪费好多人吵个大半天也没啥结果。在缓存清理这个事情上，我觉得很明显，既有的和所探讨的设计都是shi一样的存在，不信你拿着本文第一个截图给10个人看，看有几个能搞明白那是啥玩意。我不相信把用户搞得糊里糊涂不明不白的设计不是垃圾。\n\n如果放开了说，某个方案一眼看不出好坏，那之前在微博上看到的某大佬名言“你说这样子好，那你就拿出数据来证明；你要是拿不出数据，那就听我的”就是最好的解决办法，谁是这个项目的头，谁一句话盖棺定论，一来免得浪费时间，二来本来就没有对错，交给市场去检验。既然敢经过层层面试把人家招进来，就要相信人家的能力，更要有允许试错的魄力。","html":"<h2 id=\"ios\">探讨iOS应用里的缓存设置</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>今天碰到某度网盘产品讨论在爱疯上的缓存设置及其策略，我表示异常困惑，于是决定在这个常年除了垃圾回帖外就没有别的在更新的博客上写一写，算是别浪费了租用空间的费用。</p>\n\n<p>网盘类的产品，由于预览和保存的需要，必然会从服务器上下载用户从其他端上传过的，部分或者全部的文件内容。好比一部www.av9.cc-ABS130.avi，你在iPad网盘上想点击观看，肯定需要把整个文件都下载下来；但是当你用2M宽带小水管屁颠屁颠地下完之后才发现，iPad自带播放器打不开avi格式，你瞬间想要暴粗口了，明显avi坑了av。</p>\n\n<p>抛开用第三方应用打开不谈，你所下载的980.3M数据一下子变成了无用数据，于是网盘把它们叫做“缓存数据”。再好比你去泰国跟人妖合影的照片，你用网盘一点击，很短的时间它就打开了，当你看见相貌骄人的人妖MM时心情立马比刚才的ABS130要好了几千倍，然而此时你所下载的内容，其实并不是照片本身，而是一个被压缩过的质量略差的缩略图而已，网盘依然把它叫做“缓存数据”；如果你仔细观察会发现照片下方还有一个五角星按钮，这是干嘛的呢，你好奇地一点，屏幕中间赫然弹出一个大笑脸外加黑底白字的告诉你“收藏成功”。</p>\n\n<p>当网盘终于把这个数据叫做“收藏数据”的时候，你会神奇地发现在设置页里头有一个比上面所说的策略复杂一万倍的界面让你来算计、清理你的数据。真是我待缓存如初恋，缓存虐我千百遍也不为过。“坑爹哪！”</p>\n\n<p>虽然我不知道这是哪位蛋疼的产品经理想出来的东西，但是当一些策略会搞得产品经理自己都头大无语的时候，这些策略就是shi一样的。看图：</p>\n\n<p><img src=\"http://single1024.qiniudn.com/wp-content/uploads/2012/08/IMG_1620.png\" alt=\"baidu_netdisk\" /></p>\n\n<p>我相信没有人会有兴趣在着急下载ABS130的时候去搞明白这些玩意到底是怎么加起来才能不超过500M吧，更别提当我们自以为搞懂了胆战心惊地点击这个大红按钮之后，会弹出一段让你更加看不明白的提示语，就差给你弹一句“PM的心思你别猜”了。更让人无语的是，竟然还要以“这东西肯定要做”为理由来设计这个东西，莫非是酷盘最近风生水起就一定要以他们为行业标准么。那我们再截图看看酷盘是什么德行：</p>\n\n<p><img src=\"http://single1024.qiniudn.com/wp-content/uploads/2012/08/IMG_1619.png\" alt=\"kupan\" /></p>\n\n<p>当我看到大红色按钮下方的一排“如需清除本地收藏，请先取消收藏文件”的时候我差点没吐出血来，你们怎么不直接跟用户说“如果怕空间不足建议不要使用本产品，不使用本产品不会占用您的任何手机空间”呢？</p>\n\n<p>好吧，吐槽结束说点正经的。</p>\n\n<p>缓存很容易理解，为了使某些使用频率较高的内容能够更快地访问而保存在本地，避免再次下载浪费流量和时间。说白了，这些数据是设计给程序用的，它们能用多大空间，什么时候被清理，都应该根据程序的需要来调整，根本就不应该让用户去计算！</p>\n\n<p>在PC上，浏览器是最常用到缓存的，你第一次登录微博会发现加载很慢，但是下次登录的时候就快多了，是因为浏览器把微博的很多图片啊js文件啊保存在某个tmp目录下，这情景跟手机上使用网盘【预览】是完全一样的；与之相对，你在某网站上看到一个好用的软件，然后点击下载把它下在了自己的电脑上，手机上是【收藏】，只不过iOS不让你随便选路径罢了；那你见过哪个浏览器的设置里面有一项是说“您的下载和缓存总空间是500M，如果您电脑空间不足，请点击清空缓存释放。但是，清空的不包括您的下载内容哦~”</p>\n\n<p>说白了，缓存其实应该叫“临时文件”，它们并不是用户最需要的东西，该清理时就得毫不留情地清理，如果你的算法设计得足够好，必然能在空间和速度上找到一个平衡点，这正是广义上缓存这个策略存在的意义，而不是偷懒让用户的脑子去计算；收藏才是关键的内容，它们是用户确定之后才保存下来的，你当然不能随便清理，当用户发现自己的手机空间不足了，他应该像在电脑上一样一个一个比较，将自己认为不重要的文件【取消收藏】，而且这部分工作，在计算机行业发展到能够智能准确地识别用户兴趣爱好之前，还是得用户来做。</p>\n\n<p>设置项里面，只需一个清空缓存按钮即可。假如缓存清理了还不够用，那只能挨个取消收藏了（我能再说一句这个收藏的称呼是尼妈想出来糊弄人的吧？）。为此我调研了iOS上流行的一些应用，和几乎全部的网盘类产品，也就某度和酷盘在设置页里故弄玄虚自欺欺人而已。某数字公司起码在这一项上符合我的期望。</p>\n\n<p><img src=\"http://single1024.qiniudn.com/wp-content/uploads/2012/08/IMG_1618.png\" alt=\"360\" /></p>\n\n<p>更让人觉得无语的是，就这些让人啼笑皆非的脑残设计，居然还要浪费好多人吵个大半天也没啥结果。在缓存清理这个事情上，我觉得很明显，既有的和所探讨的设计都是shi一样的存在，不信你拿着本文第一个截图给10个人看，看有几个能搞明白那是啥玩意。我不相信把用户搞得糊里糊涂不明不白的设计不是垃圾。</p>\n\n<p>如果放开了说，某个方案一眼看不出好坏，那之前在微博上看到的某大佬名言“你说这样子好，那你就拿出数据来证明；你要是拿不出数据，那就听我的”就是最好的解决办法，谁是这个项目的头，谁一句话盖棺定论，一来免得浪费时间，二来本来就没有对错，交给市场去检验。既然敢经过层层面试把人家招进来，就要相信人家的能力，更要有允许试错的魄力。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402647400128,"created_by":1,"updated_at":1402649457678,"updated_by":1,"published_at":1402647400131,"published_by":1},{"id":8,"uuid":"aa811a98-eea4-481d-b574-db1f1411b508","title":"百度Hi iPhone版的心酸上线史","slug":"release-of-baiduhi-for-iphone","markdown":"## 百度Hi iPhone版的心酸上线史\n\n[@招牌疯子](http://weibo.com/819880808)\n\n今天翻看项目文档，Hi iPhone版一共做了五个排期，从第一版的6月1号内测，到第五版的6月25号内测，总共只延期了不到半个月，不得不说是一个奇迹。然而后面经历了长达三个月的appstore审核之路，让前面的辛苦变得一文不值。\n\n实话实说，这个项目最大的困难来自团队内部，产品定位的原因导致队伍极小，基本都是兼职，在繁杂的开发过程中，后端同事又突然离职，甚至产品还没发布，团队成员已经被分发到了别的项目组。到今天正式上线appstore，终于算是功德圆满，对Hi项目是有感情的，因为曾经那群优秀的同事，和曾经那段快乐的时光。\n\n接下来说一说跟苹果的斗争史吧，由于这是我们第一次做iOS产品，大家都没有什么经验，在没做完之前就一直关注appstore的审核问题，前人报出来的经验和教训，我们也是都列出来仔细检查了，谁能想到，由于这样那样的原因，还是多次被拒，不堪回首。\n\n第一次，是因为提交的测试账号的问题。Hi iPhone版我们沿用了之前IM团队留下来的测试账号，大约有几十个，提交审核的时候给了苹果两个联系人和群数量适中的，其中一个账号的好友个性签名中出现了安卓字样，竟然因为出现竞争对手的名字而被拒，我等真是欲哭无泪，只能再注册新号，每个号里只有一个好友去提交。\n\n第二次被拒，是因为产品需求希望Hi可以保持后台在线，这样用户切出去打个电话啥的切回来还能继续聊天，不至于给他的好友造成频繁上下线的困扰。为了实现这个功能，我们选择使用voip的机制来保持，苹果以该产品不是voip产品为由拒掉。那这次则完全是我们自己的原因了，我还清楚地记得当时大周五的收到拒信，PM整体去青岛旅游了，在火车上打电话各种断线，果断周末加班修改，就是为了能让苹果的人下周一上班就能排上队。\n\n下面简单总结一下iOS审核的经验和tips，其中有些东西也来自后来参与的其他产品，想想都是眼泪。\n\n首先，苹果的给的官方[App Store审核指南](http://www.apple.com.cn/developer/appstore/guidelines.html)一定要详细研读，深刻理解，只要是人家提到不让你干的，你千万别干；人家没有说不让的，你也要谨慎，问问论坛上的大神们，看看别的产品的总结们，总之这东西就是圣旨，不要心存侥幸，最好是根据审核指南做出一份checklist每次提交的时候都检查一遍。\n\n然后是产品第一版，一定要精简功能，只要满足最基本的需求即可，不是我们做不了，而是你做的越多，越容易被他们挑出刺来。说句难听的，企业开发者提交的应用，我还没听说过那支是一次通过的，苹果对企业开发的产品，肯定是不友好的。如果有一次通过的，也请下面留言方便交流经验啊。与之类似，提交的测试账号之类也要越简单越好，免得重蹈我们的覆辙。\n\n第一版审核最难，更新相对要容易一些，所以更多的功能放到升级版本里去。\n\n任何应用里的和宣传用的图片文案，都不能出现苹果、安卓的图标和字样，否则绝对被拒，这一块苹果查的很严。我们的另一款跨平台应用由于为ipa格式的文件设计了漂亮的图标，包含了一个隐约的苹果图案，也被无情地打回。\n\n与苹果自有功能类似或者相同的东西，更要千万谨慎。某网盘产品审核过程中，因为功能与iCloud相似被Metadata Rejected。这里要注意，Metadata Rejected其实就是Rejected，只不过不是让你改代码。而他们所提问的问题，你一定要如实回答，比如“Is there additional charge for mobile access?”，即使你的应用是免费的，也一定要说明白运营商可能会收取部分流量费。如果你确实包含付费功能，千万不要指望引导用户去你的网站付费，苹果的手续费你是一定要给的。曾经看到有人把付费功能先关闭然后提交审核，审核通过后再后台打开付费链接，我当然是不建议这样做了，被查到之后会死的很惨。\n\n再一点，每次被拒之后重新提交，都会重新进入等待队列里，至少两个星期才会变成In Review。所以，在如今这个唯快不破的移动互联网市场，任何需求在两周的等待时间面前都是浮云，只要能通过审核，需求该改的就果断改，该砍的也果断砍掉。再漂亮的功能，上不了线用户看不到，难道不都是白搭吗。\n\n每个产品都有且只有一次申诉的机会，而且必须是在被拒之后才能使用。这个机会仿佛是一个插队卡，你用了就可以跳过两周的等待期直接In Review，所以这个机会一定要用在最紧急，价值最大化的时候。比如为了配合运营活动排期已经不允许等待时。但是正因为只有一次，使用的时候你一定要确保把能预见到的所有问题都解决掉了，否则就是严重的浪费。\n\n然后是团队成员一定要有很强的执行力，一旦遇到reject，从需求到设计到代码到测试都要立刻行动起来，最快速度地完成修改并重新提交。\n\n最后一条，苹果的审核人员也是人，而且不是同一个人，在别的应用里没问题的东西如果在你手里被拒了，你也不要抱怨，该怎么做还是怎么做，我们就是同一款产品的iPhone版没事，iPad就被拒了。而且正因为是人工在审核，每次与他们交流也要至情至理，回复邮件的时候要客气三分，最好找英文好的同学来写（我们是找的外援我会到处乱说吗），相信看到诚恳的回复，他们也会理解你的苦衷吧。\n\n该说的能说的也就这么多了，跟苹果的斗争是永无止尽的，因为人家的设备、系统最重要的是生态圈做的太好了，这里不是你的地盘就只能听人家的。最后感谢所有付出辛苦努力的同学，并祝愿所有同行提交的应用都能最快速度通过。","html":"<h2 id=\"hiiphone\">百度Hi iPhone版的心酸上线史</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>今天翻看项目文档，Hi iPhone版一共做了五个排期，从第一版的6月1号内测，到第五版的6月25号内测，总共只延期了不到半个月，不得不说是一个奇迹。然而后面经历了长达三个月的appstore审核之路，让前面的辛苦变得一文不值。</p>\n\n<p>实话实说，这个项目最大的困难来自团队内部，产品定位的原因导致队伍极小，基本都是兼职，在繁杂的开发过程中，后端同事又突然离职，甚至产品还没发布，团队成员已经被分发到了别的项目组。到今天正式上线appstore，终于算是功德圆满，对Hi项目是有感情的，因为曾经那群优秀的同事，和曾经那段快乐的时光。</p>\n\n<p>接下来说一说跟苹果的斗争史吧，由于这是我们第一次做iOS产品，大家都没有什么经验，在没做完之前就一直关注appstore的审核问题，前人报出来的经验和教训，我们也是都列出来仔细检查了，谁能想到，由于这样那样的原因，还是多次被拒，不堪回首。</p>\n\n<p>第一次，是因为提交的测试账号的问题。Hi iPhone版我们沿用了之前IM团队留下来的测试账号，大约有几十个，提交审核的时候给了苹果两个联系人和群数量适中的，其中一个账号的好友个性签名中出现了安卓字样，竟然因为出现竞争对手的名字而被拒，我等真是欲哭无泪，只能再注册新号，每个号里只有一个好友去提交。</p>\n\n<p>第二次被拒，是因为产品需求希望Hi可以保持后台在线，这样用户切出去打个电话啥的切回来还能继续聊天，不至于给他的好友造成频繁上下线的困扰。为了实现这个功能，我们选择使用voip的机制来保持，苹果以该产品不是voip产品为由拒掉。那这次则完全是我们自己的原因了，我还清楚地记得当时大周五的收到拒信，PM整体去青岛旅游了，在火车上打电话各种断线，果断周末加班修改，就是为了能让苹果的人下周一上班就能排上队。</p>\n\n<p>下面简单总结一下iOS审核的经验和tips，其中有些东西也来自后来参与的其他产品，想想都是眼泪。</p>\n\n<p>首先，苹果的给的官方<a href=\"http://www.apple.com.cn/developer/appstore/guidelines.html\">App Store审核指南</a>一定要详细研读，深刻理解，只要是人家提到不让你干的，你千万别干；人家没有说不让的，你也要谨慎，问问论坛上的大神们，看看别的产品的总结们，总之这东西就是圣旨，不要心存侥幸，最好是根据审核指南做出一份checklist每次提交的时候都检查一遍。</p>\n\n<p>然后是产品第一版，一定要精简功能，只要满足最基本的需求即可，不是我们做不了，而是你做的越多，越容易被他们挑出刺来。说句难听的，企业开发者提交的应用，我还没听说过那支是一次通过的，苹果对企业开发的产品，肯定是不友好的。如果有一次通过的，也请下面留言方便交流经验啊。与之类似，提交的测试账号之类也要越简单越好，免得重蹈我们的覆辙。</p>\n\n<p>第一版审核最难，更新相对要容易一些，所以更多的功能放到升级版本里去。</p>\n\n<p>任何应用里的和宣传用的图片文案，都不能出现苹果、安卓的图标和字样，否则绝对被拒，这一块苹果查的很严。我们的另一款跨平台应用由于为ipa格式的文件设计了漂亮的图标，包含了一个隐约的苹果图案，也被无情地打回。</p>\n\n<p>与苹果自有功能类似或者相同的东西，更要千万谨慎。某网盘产品审核过程中，因为功能与iCloud相似被Metadata Rejected。这里要注意，Metadata Rejected其实就是Rejected，只不过不是让你改代码。而他们所提问的问题，你一定要如实回答，比如“Is there additional charge for mobile access?”，即使你的应用是免费的，也一定要说明白运营商可能会收取部分流量费。如果你确实包含付费功能，千万不要指望引导用户去你的网站付费，苹果的手续费你是一定要给的。曾经看到有人把付费功能先关闭然后提交审核，审核通过后再后台打开付费链接，我当然是不建议这样做了，被查到之后会死的很惨。</p>\n\n<p>再一点，每次被拒之后重新提交，都会重新进入等待队列里，至少两个星期才会变成In Review。所以，在如今这个唯快不破的移动互联网市场，任何需求在两周的等待时间面前都是浮云，只要能通过审核，需求该改的就果断改，该砍的也果断砍掉。再漂亮的功能，上不了线用户看不到，难道不都是白搭吗。</p>\n\n<p>每个产品都有且只有一次申诉的机会，而且必须是在被拒之后才能使用。这个机会仿佛是一个插队卡，你用了就可以跳过两周的等待期直接In Review，所以这个机会一定要用在最紧急，价值最大化的时候。比如为了配合运营活动排期已经不允许等待时。但是正因为只有一次，使用的时候你一定要确保把能预见到的所有问题都解决掉了，否则就是严重的浪费。</p>\n\n<p>然后是团队成员一定要有很强的执行力，一旦遇到reject，从需求到设计到代码到测试都要立刻行动起来，最快速度地完成修改并重新提交。</p>\n\n<p>最后一条，苹果的审核人员也是人，而且不是同一个人，在别的应用里没问题的东西如果在你手里被拒了，你也不要抱怨，该怎么做还是怎么做，我们就是同一款产品的iPhone版没事，iPad就被拒了。而且正因为是人工在审核，每次与他们交流也要至情至理，回复邮件的时候要客气三分，最好找英文好的同学来写（我们是找的外援我会到处乱说吗），相信看到诚恳的回复，他们也会理解你的苦衷吧。</p>\n\n<p>该说的能说的也就这么多了，跟苹果的斗争是永无止尽的，因为人家的设备、系统最重要的是生态圈做的太好了，这里不是你的地盘就只能听人家的。最后感谢所有付出辛苦努力的同学，并祝愿所有同行提交的应用都能最快速度通过。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402647541192,"created_by":1,"updated_at":1402649444884,"updated_by":1,"published_at":1402647541194,"published_by":1},{"id":9,"uuid":"06f5b6e5-5c4a-4bea-b672-57a393f930e8","title":"生成大文件的方法","slug":"ways-of-gen-bigfile","markdown":"## 生成大文件的方法\n\n[@招牌疯子](http://weibo.com/819880808)\n\n今天群里讨论到生成大文件的问题，由于这种小技术经常会用到，所以记一下。\n第一种是组内同学用到的fsutil file createnew命令，实际是操作的磁盘，划出一段空间标记成文件，看起来是生成了，其实只操作了文件头和文件尾，所以内容肯定是空。优点是速度很快；缺点是文件内容固定都是\\0，而且只适用于FAT和NTFS等windows分区，附上一个例子：\n\n```\nfsutil file createnew bigfile.tmp 1000000\n```\n\n第二种也是比较推荐的，用linux/unix下的dd命令，dd命令的具体用法可以自行百度去，我们用到的参数有【if】标准输入源，可以是文件也可以是某个挂载的分区，【of】标准输出目标，【bs】块大小，【count】块数量，【seek】跳过多少块之后才开始写内容。从参数就可以看出来dd命令是真正地在写文件到硬盘上，所以生成文件的速度取决于硬盘读写速度，文件越大速度越慢，我在自己mac pro上生成1G的文件用时20s多一点。dd命令的优点是生成文件的内容可控制，对硬盘分区格式无要求；缺点是生成速度慢。下面通过三个例子来说明一下，注意bs所用的单位，小写适用于mac系统，大写适用于linux系统。\n\n```\ndd if=/dev/zero of=bigfile.tmp bs=1m count=1000\n```\n\n生成一个大小为1G的空文件，文件内容全部是\\0。但是值得注意的是该方法与上述fsutil命令有本质的区别，所以即使生成了同样的空文件依然需要耗时较长。\n\n```\ndd if=/home/zippo/index.html of=bigfile.tmp bs=1k count=1000\n```\n\n每次从index.html文件里读取1k的数据写入目标文件，所以生成文件是index.html部分内容的循环。\n\n```\ndd if=/dev/urandom of=bigfile.tmp bs=1m count=1000\n```\n\n生成一个大小为1G，内容随机的文件，与第一个例子不同的地方在于输入源，/dev/urandom产生随机内容的原理是利用当前系统的熵池来计算出固定数量的随机比特并输出，与之相似的还有一个/dev/random，关于熵池的内容不是本文讨论的范围不再详谈。这条命令在希望产生大量内容不相同的文件时很实用。\n\n最后有一个非常特殊的例子，比如我需要一个1000G的测试数据，但是我总共的磁盘只有100G怎么办呢？\n\n```\ndd if=/dev/zero of=bigfile.tmp bs=1m count=0 seek=1000000\n```\n\n此时文件系统中显示文件大小为1000G，但实际上不占用任何空间，生成速度也是瞬间完成，seek的作用是跳过指定的大小，实际不写入文件，这个小技巧真的很实用，但是需要注意的此命令在mac下不行，实际测试发现mac下仍然会占用空间。\n如有疏漏和错误，请留言告知。","html":"<h2 id=\"\">生成大文件的方法</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>今天群里讨论到生成大文件的问题，由于这种小技术经常会用到，所以记一下。\n第一种是组内同学用到的fsutil file createnew命令，实际是操作的磁盘，划出一段空间标记成文件，看起来是生成了，其实只操作了文件头和文件尾，所以内容肯定是空。优点是速度很快；缺点是文件内容固定都是\\0，而且只适用于FAT和NTFS等windows分区，附上一个例子：</p>\n\n<pre><code>fsutil file createnew bigfile.tmp 1000000  \n</code></pre>\n\n<p>第二种也是比较推荐的，用linux/unix下的dd命令，dd命令的具体用法可以自行百度去，我们用到的参数有【if】标准输入源，可以是文件也可以是某个挂载的分区，【of】标准输出目标，【bs】块大小，【count】块数量，【seek】跳过多少块之后才开始写内容。从参数就可以看出来dd命令是真正地在写文件到硬盘上，所以生成文件的速度取决于硬盘读写速度，文件越大速度越慢，我在自己mac pro上生成1G的文件用时20s多一点。dd命令的优点是生成文件的内容可控制，对硬盘分区格式无要求；缺点是生成速度慢。下面通过三个例子来说明一下，注意bs所用的单位，小写适用于mac系统，大写适用于linux系统。</p>\n\n<pre><code>dd if=/dev/zero of=bigfile.tmp bs=1m count=1000  \n</code></pre>\n\n<p>生成一个大小为1G的空文件，文件内容全部是\\0。但是值得注意的是该方法与上述fsutil命令有本质的区别，所以即使生成了同样的空文件依然需要耗时较长。</p>\n\n<pre><code>dd if=/home/zippo/index.html of=bigfile.tmp bs=1k count=1000  \n</code></pre>\n\n<p>每次从index.html文件里读取1k的数据写入目标文件，所以生成文件是index.html部分内容的循环。</p>\n\n<pre><code>dd if=/dev/urandom of=bigfile.tmp bs=1m count=1000  \n</code></pre>\n\n<p>生成一个大小为1G，内容随机的文件，与第一个例子不同的地方在于输入源，/dev/urandom产生随机内容的原理是利用当前系统的熵池来计算出固定数量的随机比特并输出，与之相似的还有一个/dev/random，关于熵池的内容不是本文讨论的范围不再详谈。这条命令在希望产生大量内容不相同的文件时很实用。</p>\n\n<p>最后有一个非常特殊的例子，比如我需要一个1000G的测试数据，但是我总共的磁盘只有100G怎么办呢？</p>\n\n<pre><code>dd if=/dev/zero of=bigfile.tmp bs=1m count=0 seek=1000000  \n</code></pre>\n\n<p>此时文件系统中显示文件大小为1000G，但实际上不占用任何空间，生成速度也是瞬间完成，seek的作用是跳过指定的大小，实际不写入文件，这个小技巧真的很实用，但是需要注意的此命令在mac下不行，实际测试发现mac下仍然会占用空间。\n如有疏漏和错误，请留言告知。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402647845616,"created_by":1,"updated_at":1402649431395,"updated_by":1,"published_at":1402647845618,"published_by":1},{"id":10,"uuid":"f08d856b-602f-4baa-bb3d-bf5f22bf11f1","title":"手把手教你打造自己的ping工具","slug":"make-your-own-ping","markdown":"## 手把手教你打造自己的ping工具\n\n[@招牌疯子](http://weibo.com/819880808)\n\n记得之前看到过一句话：什么是Geek，Geek就是当看到什么东西不顺手时，第一时间想到的是怎么把它改造到顺手好用。\n\n其实事情是这样的，ping命令大家都用过，Linux下的ping命令本身很强大，但是有一点不太符合用户体验的是，它不会自动结束（哼我当然知道-c选项啊魂淡>_<平心而论这方面windows下的ping就好用一点），因为我们ping一个地址一般只是为了测试下网络是否通畅，每次都得多按一下ctrl+C说多不多，说少也不少，SO，正好前阵子在一本书上看到过ping的原理和实现，于是自己动手改出了一个用着顺手的版本，代码会贴在本文末尾。\n\n1.ping简介\nping是用来查看自己这边跟网络中某个主机之间是否联通的工具，原理是从本地向目标地址发送ICMP报文，若对方收到了会将报文一模一样地发回来。但是值得注意的是，在TCP/IP体系中，ping是位于应用层，并直接操纵网络层的，自己写ping的话不能使用常见的TCP协议。\n下面是我在自己机器上使用ping命令的结果：\n\n```\nzippo@openSUSE:~/develop/linuxc> ping buaa.us\nPING buaa.us (106.187.95.231) 56(84) bytes of data.\n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=1 ttl=47 time=197 ms\n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=2 ttl=47 time=196 ms\n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=3 ttl=47 time=197 ms\n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=4 ttl=47 time=197 ms\n^C\n--- buaa.us ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms\nrtt min/avg/max/mdev = 196.656/197.183/197.578/0.637 ms\n```\n\n其中的icmp_seq是报文顺序号，ttl是生存时间，time是这个报文来回的时间，可以看到位于海外的本站主机延时高达200ms。\n\n2.ICMP协议简介\n[详细的介绍](http://baike.baidu.com/view/30564.htm)建议搜索一下，简单来说ICMP是TCP/IP协议族中的一个，是位于IP层的一个协议，由于涉及到路由选择等问题，ICMP报文需要通过IP协议来发送。其报文数据先加一个ICMP报头形成ICMP报文，再加上IP报头形成一个标准的IP数据报。\n先来看IP报，IP报头由许多部分组成，从linux中/usr/include/netinet/ip.h头文件可以看到其数据结构：\n\n```\nstruct ip\n  {\n#if __BYTE_ORDER == __LITTLE_ENDIAN\n    unsigned int ip_hl:4;        /* header length */\n    unsigned int ip_v:4;        /* version */\n#endif\n#if __BYTE_ORDER == __BIG_ENDIAN\n    unsigned int ip_v:4;        /* version */\n    unsigned int ip_hl:4;        /* header length */\n#endif\n    u_int8_t ip_tos;            /* type of service */\n    u_short ip_len;            /* total length */\n    u_short ip_id;            /* identification */\n    u_short ip_off;            /* fragment offset field */\n#define    IP_RF 0x8000            /* reserved fragment flag */\n#define    IP_DF 0x4000            /* dont fragment flag */\n#define    IP_MF 0x2000            /* more fragments flag */\n#define    IP_OFFMASK 0x1fff        /* mask for fragmenting bits */\n    u_int8_t ip_ttl;            /* time to live */\n    u_int8_t ip_p;            /* protocol */\n    u_short ip_sum;            /* checksum */\n    struct in_addr ip_src, ip_dst;    /* source and dest address */\n  };\n```\n\n那我们的ping只用到了其中的ip_hl和ip_ttl两个成员，直接使用人家现成的即可。\n\n然后是ICMP报文，ICMP的报头共8字节，前4字节由类型、代码、检验和（cheksum）组成；后面四个字节与ICMP的类型有关，我们的ping只使用到了ICMP_ECHO和ICMP_ECHOREPLY两种，在头文件/usr/include/netinet/ip_icmp.h中它们分别被定义成常量8和0，具体ICMP的数据结构也可以在这个头文件中看到，我就不贴了。\n\n3.ICMP报文的构造\n那我们想要自己构造一个ICMP报文，就需要了解它的报头校验和算法：\n\n把被校验的数据进行16位累加，然后取反码，若数据长度为奇数则末尾补0。此算法名叫[网际校验和算法](http://blog.csdn.net/microtong/article/details/3112139)，在网络协议上用得极为广泛，包括你所熟知的TCP、UDP、IPV4等等，更详细的内容可以看[RFC1071](http://www.faqs.org/rfcs/rfc1071.html)标准文档。下面这段是我们用到的校验和算法实现：\n\n```\nunsigned short cal_chksum(unsigned short *addr, int len)\n{\n    int nleft=len;\n    int sum=0;\n    unsigned short *w=addr;\n    unsigned short answer=0;\n    while(nleft>1)\n    {\n        sum+=*w++;\n        nleft-=2;\n    }\n    if(nleft==1)\n    {\n        *(unsigned char *)(&answer)=*(unsigned char *)w;\n        sum+=answer;\n    }\n    sum=(sum>>16)+(sum&0xffff);\n    sum+=(sum>>16);\n    answer=~sum;\n    return answer;\n}\n```\n\nICMP报文还有一个重要的部分是标识符，如果没有标识符，你在自己机器上起了多个ping的时候岂不是要乱套了么（ICMP协议中没有端口概念，只能定位到IP）。为了区分我们用进程ID来作为标识符。\n\n另外为了使我们的ping命令能够显示出耗时信息，由于其自身没有相应的成员，我们采用的办法是发送时取本地时间放入报文中，等收到回来的报文时再用接收时间做差即为往返时间。用到数据结构timeval和gettimeofday()函数。但是由于要加入我们自己的东西，就需要把ICMP数据结构中的icmp->icmp_data强制转换为timeval类型并重新设置报头大小和计算新的校验和，实现如下：\n\n```\nint pack(int pack_no)\n{\n    int i, packsize;\n    struct icmp *icmp;\n    struct timeval *tval;\n \n    icmp=(struct icmp*)sendpacket;\n    icmp->icmp_type=ICMP_ECHO;\n    icmp->icmp_code=0;\n    icmp->icmp_cksum=0;\n    icmp->icmp_seq=pack_no;\n    icmp->icmp_id=pid;\n    packsize=8+datalen;\n    tval=(struct timeval*)icmp->icmp_data;\n    gettimeofday(tval, NULL);\n    icmp->icmp_cksum=cal_chksum((unsigned short *)icmp, packsize);\n    return packsize;\n}\n```\n\n4.发送和接收\nOK，现在我们需要的材料已经齐了，要怎么发送和接收这些报文呢？上文已经提到ICMP协议不属于TCP和UDP等传输层协议，不能使用常用的socket编程（SOCK_STREAM和SOCK_DRAGM），不过没关系，linux还给我们提供另外一种套接口——原始套接口（SOCK_RAW），估计也就是给那些发明新协议的人用的，从名字也可看出来它更加原始，更加自由。\n注意：只有root用户才能创建原始套接口，所以在编译程序时需要切换到root身份。\n创建原始套接口也是使用socket()函数：\n\n```\nint sockfd=socket(AF_INET, SOCK_RAW, protocol->p_proto);\n```\n\n然后原始套接口的发送接收函数也跟TCP/UDP类似：\n\n```\nsendto(sockfd, sendpacket, packetsize, 0, (struct sockaddr *)&dest_addr, sizeof(dest_addr));\nrecvfrom(sockfd, recvpacket, sizeof(recvpacket), 0, (struct sockaddr *)&from, &fromlen);\n```\n\n5.组装实现运行\n万事俱备，定义一个发送次数，我设的4次，循环发送构造好的ICMP报文并尝试接受，最后计算成功率并输出，一个我们自己打造的ping命令就呈现在眼前了，下面是运行结果：\n\n```\nopenSUSE:/home/zippo/develop/linuxc # ./myping buaa.us\nPING buaa.us(106.187.95.231): 64 bytes data in ICMP packets.\n64 bytes from 106.187.95.231: icmp_seq=1 ttl=47 time=197.433 ms\n64 bytes from 106.187.95.231: icmp_seq=2 ttl=47 time=197.063 ms\n64 bytes from 106.187.95.231: icmp_seq=3 ttl=47 time=197.161 ms\n64 bytes from 106.187.95.231: icmp_seq=4 ttl=47 time=196.994 ms\n \n---------PING statitics-----------\n4 packets transmitted, 4 received, %0 lost\n```\n\n可以看到与系统自带的ping命令基本无差别，至此我们的任务也就完成了。\n\n6.完整代码\n\n```\n/*\n本代码来自电子工业出版社出版的《Linux C编程》\n第11章最后的示例代码\n由于其中有部分错误，我做了少量修正并实际编译运行通过\n此代码所有权归原作者所有\n所有改动之处都有注释说明\n如果你在使用过程中发现错误请联系本人：\n新浪微博：@招牌疯子\n邮箱：zp@buaa.us\n2013.4.12\n*/\n\n#include <stdio.h>\n#include <signal.h>\n#include <arpa/inet.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <unistd.h>\n#include <netinet/in.h>\n#include <netinet/ip.h>\n#include <netinet/ip_icmp.h>\n#include <netdb.h>\n#include <setjmp.h>\n#include <errno.h>\n\n#define PACKET_SIZE 4096\n#define MAX_WAIT_TIME 5\n#define MAX_NO_PACKETS 4\n\nchar sendpacket [PACKET_SIZE];\nchar recvpacket [PACKET_SIZE];\nint sockfd, datalen=56;\nint nsend=0, nreceived=0;\nstruct sockaddr_in dest_addr;\npid_t pid;\nstruct sockaddr_in from;\nstruct timeval tvrecv;\n\nvoid statistics(int signo);\nunsigned short cal_chksum(unsigned short *addr, int len);\nint pack(int pack_no);\nvoid send_packet(void);\nvoid recv_packet(void);\nint unpack(char *buf, int len);\nvoid tv_sub(struct timeval *out, struct timeval *in);\n\nvoid statistics(int signo)\n{\n\tprintf(\"\\n---------PING statitics-----------\\n\");\n\tprintf(\"%d packets transmitted, %d received, %%%d lost\\n\", nsend, nreceived, (nsend-nreceived)/nsend*100);\n\tclose(sockfd);\n\texit(1);\n}\n\nunsigned short cal_chksum(unsigned short *addr, int len)\n{\n\tint nleft=len;\n\tint sum=0;\n\tunsigned short *w=addr;\n\tunsigned short answer=0;\n\twhile(nleft>1)\n\t{\n\t\tsum+=*w++;\n\t\tnleft-=2;\n\t}\n\tif(nleft==1)\n\t{\n\t\t*(unsigned char *)(&answer)=*(unsigned char *)w;\n\t\tsum+=answer;\n\t}\n\tsum=(sum>>16)+(sum&0xffff);\n\tsum+=(sum>>16);\n\tanswer=~sum;\n\treturn answer;\n}\n\nint pack(int pack_no)\n{\n\tint i, packsize;\n\tstruct icmp *icmp;\n\tstruct timeval *tval;\n\n\ticmp=(struct icmp*)sendpacket;\n\ticmp->icmp_type=ICMP_ECHO;\n\ticmp->icmp_code=0;\n\ticmp->icmp_cksum=0;\n\ticmp->icmp_seq=pack_no;\n\ticmp->icmp_id=pid;\n\tpacksize=8+datalen;\n\ttval=(struct timeval*)icmp->icmp_data;\n\tgettimeofday(tval, NULL);\n\ticmp->icmp_cksum=cal_chksum((unsigned short *)icmp, packsize);\n\treturn packsize;\n}\n\nvoid send_packet()\n{\n\tint packetsize;\n\twhile(nsend<MAX_NO_PACKETS)\n\t{\n\t\tnsend++;\n\t\tpacketsize=pack(nsend);\n\t\tif(sendto(sockfd, sendpacket, packetsize, 0, (struct sockaddr *)&dest_addr, sizeof(dest_addr))<0)\n\t\t{\n\t\t\tperror(\"sendto error\");\n\t\t\tcontinue;\n\t\t}\n\t\t//此处插入调用recv_packet()函数使统计和显示更准确\n\t\trecv_packet();\n\t\tsleep(1);\n\t}\n}\n\nvoid recv_packet()\n{\n\tint n, fromlen;\n\textern int errno;\n\n\tsignal(SIGALRM, statistics);\n\tfromlen=sizeof(from);\n\twhile(nreceived<nsend)\n\t{\n\t\talarm(MAX_WAIT_TIME);\n\t\tif((n=recvfrom(sockfd, recvpacket, sizeof(recvpacket), 0, (struct sockaddr *)&from, &fromlen))<0)\n\t\t{\n\t\t\tif(errno==EINTR)\n\t\t\t\tcontinue;\n\t\t\tperror(\"recvfrom error\");\n\t\t\tcontinue;\n\t\t}\n\t\tgettimeofday(&tvrecv, NULL);\n\t\tif(unpack(recvpacket, n)==-1)\n\t\t\tcontinue;\n\t\tnreceived++;\n\t}\n}\n\nint unpack(char *buf, int len)\n{\n\tint i, iphdrlen;\n\tstruct ip *ip;\n\tstruct icmp *icmp;\n\tstruct timeval *tvsend;\n\tdouble rtt;\n\n\tip=(struct ip*)buf;\n\tiphdrlen=ip->ip_hl<<2;\n\ticmp=(struct icmp*)(buf+iphdrlen);\n\tlen-=iphdrlen;\n\tif(len<8)\n\t{\n\t\tprintf(\"ICMP packets\\'s length is less than 8\\n\");\n\t\treturn -1;\n\t}\n\tif((icmp->icmp_type==ICMP_ECHOREPLY) && (icmp->icmp_id==pid))\n\t{\n\t\ttvsend=(struct timeval *)icmp->icmp_data;\n\t\ttv_sub(&tvrecv, tvsend);\n\t\t//为了让时间显示更精确，强制转换tv_usec(long)为double类型\n\t\trtt=tvrecv.tv_sec*1000+(double)tvrecv.tv_usec/1000; \n\t\tprintf(\"%d bytes from %s: icmp_seq=%u ttl=%d time=%.3f ms\\n\", len, inet_ntoa(from.sin_addr), icmp->icmp_seq, ip->ip_ttl, rtt);\n\t}\n\telse\n\t\treturn -1;\n}\n\nvoid tv_sub(struct timeval *out, struct timeval *in)\n{\n\tif((out->tv_usec-=in->tv_usec)<0)\n\t{\n\t\t--out->tv_sec;\n\t\tout->tv_usec+=1000000;\n\t}\n\tout->tv_sec-=in->tv_sec;\n}\n\nmain(int argc, char *argv[])\n{\n\tstruct hostent *host;\n\tstruct protoent *protocol;\n\tunsigned long inaddr=01;\n\tint waittime=MAX_WAIT_TIME;\n\tint size=50*1024;\n\n\tif(argc<2)\n\t{\n\t\tprintf(\"usage: %s hostname/IP address\\n\", argv[0]);\n\t\texit(1);\n\t}\n\tif((protocol=getprotobyname(\"icmp\"))==NULL)\n\t{\n\t\tperror(\"getprotocolbyname\");\n\t\texit(1);\n\t}\n\tif((sockfd=socket(AF_INET, SOCK_RAW, protocol->p_proto))<0)\n\t{\n\t\tperror(\"socket error\");\n\t\texit(1);\n\t}\n\tsetuid(getuid());\n\tsetsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &size, sizeof(size));\n\tbzero(&dest_addr, sizeof(dest_addr));\n\tdest_addr.sin_family=AF_INET;\n\tinaddr=inet_addr(argv[1]);\n\tif(inaddr==INADDR_NONE)\n\t{\n\t\tif((host=gethostbyname(argv[1]))==NULL)\n\t\t{\n\t\t\tperror(\"gethostbyname error\");\n\t\t\texit(1);\n\t\t}\n\t\tmemcpy((char *)&dest_addr.sin_addr, host->h_addr, host->h_length);\n\t}\n\telse\n\t{\n\t\t//memcpy((char *)&dest_addr, (char *)&inaddr, host->h_length);\n\t\t//源程序此处为BUG，因为if条件不满足则host->h_length为空值，运行时导致段错误\n\t\t//此处直接用sizeof取inaddr的长度即可\n\t\t//而且dest_addr不能强制转换为char*，应该是作者漏掉了成员.sin_addr\n\t\tmemcpy((char *)&dest_addr.sin_addr, (char *)&inaddr, sizeof(inaddr));\n\t}\n\tpid=getpid();\n\t//由于在ICMP包中加入了时间统计数据，实际发送包大小是56+8字节\n\tprintf(\"PING %s(%s): %d bytes data in ICMP packets.\\n\", argv[1], inet_ntoa(dest_addr.sin_addr), datalen+8);\n\tsend_packet();\n\t//recv_packet()函数放到send_packet()函数里面，让程序可以及时显示发送进度\n\t//recv_packet();\n\tstatistics(SIGALRM);\n\treturn 0;\n}\n```","html":"<h2 id=\"ping\">手把手教你打造自己的ping工具</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>记得之前看到过一句话：什么是Geek，Geek就是当看到什么东西不顺手时，第一时间想到的是怎么把它改造到顺手好用。</p>\n\n<p>其实事情是这样的，ping命令大家都用过，Linux下的ping命令本身很强大，但是有一点不太符合用户体验的是，它不会自动结束（哼我当然知道-c选项啊魂淡>_&lt;平心而论这方面windows下的ping就好用一点），因为我们ping一个地址一般只是为了测试下网络是否通畅，每次都得多按一下ctrl+C说多不多，说少也不少，SO，正好前阵子在一本书上看到过ping的原理和实现，于是自己动手改出了一个用着顺手的版本，代码会贴在本文末尾。</p>\n\n<p>1.ping简介 <br />\nping是用来查看自己这边跟网络中某个主机之间是否联通的工具，原理是从本地向目标地址发送ICMP报文，若对方收到了会将报文一模一样地发回来。但是值得注意的是，在TCP/IP体系中，ping是位于应用层，并直接操纵网络层的，自己写ping的话不能使用常见的TCP协议。 <br />\n下面是我在自己机器上使用ping命令的结果：</p>\n\n<pre><code>zippo@openSUSE:~/develop/linuxc&gt; ping buaa.us  \nPING buaa.us (106.187.95.231) 56(84) bytes of data.  \n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=1 ttl=47 time=197 ms  \n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=2 ttl=47 time=196 ms  \n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=3 ttl=47 time=197 ms  \n64 bytes from li415-231.members.linode.com (106.187.95.231): icmp_seq=4 ttl=47 time=197 ms  \n^C\n--- buaa.us ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms  \nrtt min/avg/max/mdev = 196.656/197.183/197.578/0.637 ms  \n</code></pre>\n\n<p>其中的icmp_seq是报文顺序号，ttl是生存时间，time是这个报文来回的时间，可以看到位于海外的本站主机延时高达200ms。</p>\n\n<p>2.ICMP协议简介 <br />\n<a href=\"http://baike.baidu.com/view/30564.htm\">详细的介绍</a>建议搜索一下，简单来说ICMP是TCP/IP协议族中的一个，是位于IP层的一个协议，由于涉及到路由选择等问题，ICMP报文需要通过IP协议来发送。其报文数据先加一个ICMP报头形成ICMP报文，再加上IP报头形成一个标准的IP数据报。\n先来看IP报，IP报头由许多部分组成，从linux中/usr/include/netinet/ip.h头文件可以看到其数据结构：</p>\n\n<pre><code>struct ip  \n  {\n#if __BYTE_ORDER == __LITTLE_ENDIAN\n    unsigned int ip_hl:4;        /* header length */\n    unsigned int ip_v:4;        /* version */\n#endif\n#if __BYTE_ORDER == __BIG_ENDIAN\n    unsigned int ip_v:4;        /* version */\n    unsigned int ip_hl:4;        /* header length */\n#endif\n    u_int8_t ip_tos;            /* type of service */\n    u_short ip_len;            /* total length */\n    u_short ip_id;            /* identification */\n    u_short ip_off;            /* fragment offset field */\n#define    IP_RF 0x8000            /* reserved fragment flag */\n#define    IP_DF 0x4000            /* dont fragment flag */\n#define    IP_MF 0x2000            /* more fragments flag */\n#define    IP_OFFMASK 0x1fff        /* mask for fragmenting bits */\n    u_int8_t ip_ttl;            /* time to live */\n    u_int8_t ip_p;            /* protocol */\n    u_short ip_sum;            /* checksum */\n    struct in_addr ip_src, ip_dst;    /* source and dest address */\n  };\n</code></pre>\n\n<p>那我们的ping只用到了其中的ip<em>hl和ip</em>ttl两个成员，直接使用人家现成的即可。</p>\n\n<p>然后是ICMP报文，ICMP的报头共8字节，前4字节由类型、代码、检验和（cheksum）组成；后面四个字节与ICMP的类型有关，我们的ping只使用到了ICMP<em>ECHO和ICMP</em>ECHOREPLY两种，在头文件/usr/include/netinet/ip_icmp.h中它们分别被定义成常量8和0，具体ICMP的数据结构也可以在这个头文件中看到，我就不贴了。</p>\n\n<p>3.ICMP报文的构造 <br />\n那我们想要自己构造一个ICMP报文，就需要了解它的报头校验和算法：</p>\n\n<p>把被校验的数据进行16位累加，然后取反码，若数据长度为奇数则末尾补0。此算法名叫<a href=\"http://blog.csdn.net/microtong/article/details/3112139\">网际校验和算法</a>，在网络协议上用得极为广泛，包括你所熟知的TCP、UDP、IPV4等等，更详细的内容可以看<a href=\"http://www.faqs.org/rfcs/rfc1071.html\">RFC1071</a>标准文档。下面这段是我们用到的校验和算法实现：</p>\n\n<pre><code>unsigned short cal_chksum(unsigned short *addr, int len)  \n{\n    int nleft=len;\n    int sum=0;\n    unsigned short *w=addr;\n    unsigned short answer=0;\n    while(nleft&gt;1)\n    {\n        sum+=*w++;\n        nleft-=2;\n    }\n    if(nleft==1)\n    {\n        *(unsigned char *)(&amp;answer)=*(unsigned char *)w;\n        sum+=answer;\n    }\n    sum=(sum&gt;&gt;16)+(sum&amp;0xffff);\n    sum+=(sum&gt;&gt;16);\n    answer=~sum;\n    return answer;\n}\n</code></pre>\n\n<p>ICMP报文还有一个重要的部分是标识符，如果没有标识符，你在自己机器上起了多个ping的时候岂不是要乱套了么（ICMP协议中没有端口概念，只能定位到IP）。为了区分我们用进程ID来作为标识符。</p>\n\n<p>另外为了使我们的ping命令能够显示出耗时信息，由于其自身没有相应的成员，我们采用的办法是发送时取本地时间放入报文中，等收到回来的报文时再用接收时间做差即为往返时间。用到数据结构timeval和gettimeofday()函数。但是由于要加入我们自己的东西，就需要把ICMP数据结构中的icmp->icmp_data强制转换为timeval类型并重新设置报头大小和计算新的校验和，实现如下：</p>\n\n<pre><code>int pack(int pack_no)  \n{\n    int i, packsize;\n    struct icmp *icmp;\n    struct timeval *tval;\n\n    icmp=(struct icmp*)sendpacket;\n    icmp-&gt;icmp_type=ICMP_ECHO;\n    icmp-&gt;icmp_code=0;\n    icmp-&gt;icmp_cksum=0;\n    icmp-&gt;icmp_seq=pack_no;\n    icmp-&gt;icmp_id=pid;\n    packsize=8+datalen;\n    tval=(struct timeval*)icmp-&gt;icmp_data;\n    gettimeofday(tval, NULL);\n    icmp-&gt;icmp_cksum=cal_chksum((unsigned short *)icmp, packsize);\n    return packsize;\n}\n</code></pre>\n\n<p>4.发送和接收 <br />\nOK，现在我们需要的材料已经齐了，要怎么发送和接收这些报文呢？上文已经提到ICMP协议不属于TCP和UDP等传输层协议，不能使用常用的socket编程（SOCK<em>STREAM和SOCK</em>DRAGM），不过没关系，linux还给我们提供另外一种套接口——原始套接口（SOCK_RAW），估计也就是给那些发明新协议的人用的，从名字也可看出来它更加原始，更加自由。 <br />\n注意：只有root用户才能创建原始套接口，所以在编译程序时需要切换到root身份。\n创建原始套接口也是使用socket()函数：</p>\n\n<pre><code>int sockfd=socket(AF_INET, SOCK_RAW, protocol-&gt;p_proto);  \n</code></pre>\n\n<p>然后原始套接口的发送接收函数也跟TCP/UDP类似：</p>\n\n<pre><code>sendto(sockfd, sendpacket, packetsize, 0, (struct sockaddr *)&amp;dest_addr, sizeof(dest_addr));  \nrecvfrom(sockfd, recvpacket, sizeof(recvpacket), 0, (struct sockaddr *)&amp;from, &amp;fromlen);  \n</code></pre>\n\n<p>5.组装实现运行 <br />\n万事俱备，定义一个发送次数，我设的4次，循环发送构造好的ICMP报文并尝试接受，最后计算成功率并输出，一个我们自己打造的ping命令就呈现在眼前了，下面是运行结果：</p>\n\n<pre><code>openSUSE:/home/zippo/develop/linuxc # ./myping buaa.us  \nPING buaa.us(106.187.95.231): 64 bytes data in ICMP packets.  \n64 bytes from 106.187.95.231: icmp_seq=1 ttl=47 time=197.433 ms  \n64 bytes from 106.187.95.231: icmp_seq=2 ttl=47 time=197.063 ms  \n64 bytes from 106.187.95.231: icmp_seq=3 ttl=47 time=197.161 ms  \n64 bytes from 106.187.95.231: icmp_seq=4 ttl=47 time=196.994 ms\n\n---------PING statitics-----------\n4 packets transmitted, 4 received, %0 lost  \n</code></pre>\n\n<p>可以看到与系统自带的ping命令基本无差别，至此我们的任务也就完成了。</p>\n\n<p>6.完整代码</p>\n\n<pre><code>/*\n本代码来自电子工业出版社出版的《Linux C编程》\n第11章最后的示例代码\n由于其中有部分错误，我做了少量修正并实际编译运行通过\n此代码所有权归原作者所有\n所有改动之处都有注释说明\n如果你在使用过程中发现错误请联系本人：\n新浪微博：@招牌疯子\n邮箱：zp@buaa.us\n2013.4.12  \n*/\n\n#include &lt;stdio.h&gt;\n#include &lt;signal.h&gt;\n#include &lt;arpa/inet.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;sys/socket.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;netinet/in.h&gt;\n#include &lt;netinet/ip.h&gt;\n#include &lt;netinet/ip_icmp.h&gt;\n#include &lt;netdb.h&gt;\n#include &lt;setjmp.h&gt;\n#include &lt;errno.h&gt;\n\n#define PACKET_SIZE 4096\n#define MAX_WAIT_TIME 5\n#define MAX_NO_PACKETS 4\n\nchar sendpacket [PACKET_SIZE];  \nchar recvpacket [PACKET_SIZE];  \nint sockfd, datalen=56;  \nint nsend=0, nreceived=0;  \nstruct sockaddr_in dest_addr;  \npid_t pid;  \nstruct sockaddr_in from;  \nstruct timeval tvrecv;\n\nvoid statistics(int signo);  \nunsigned short cal_chksum(unsigned short *addr, int len);  \nint pack(int pack_no);  \nvoid send_packet(void);  \nvoid recv_packet(void);  \nint unpack(char *buf, int len);  \nvoid tv_sub(struct timeval *out, struct timeval *in);\n\nvoid statistics(int signo)  \n{\n    printf(\"\\n---------PING statitics-----------\\n\");\n    printf(\"%d packets transmitted, %d received, %%%d lost\\n\", nsend, nreceived, (nsend-nreceived)/nsend*100);\n    close(sockfd);\n    exit(1);\n}\n\nunsigned short cal_chksum(unsigned short *addr, int len)  \n{\n    int nleft=len;\n    int sum=0;\n    unsigned short *w=addr;\n    unsigned short answer=0;\n    while(nleft&gt;1)\n    {\n        sum+=*w++;\n        nleft-=2;\n    }\n    if(nleft==1)\n    {\n        *(unsigned char *)(&amp;answer)=*(unsigned char *)w;\n        sum+=answer;\n    }\n    sum=(sum&gt;&gt;16)+(sum&amp;0xffff);\n    sum+=(sum&gt;&gt;16);\n    answer=~sum;\n    return answer;\n}\n\nint pack(int pack_no)  \n{\n    int i, packsize;\n    struct icmp *icmp;\n    struct timeval *tval;\n\n    icmp=(struct icmp*)sendpacket;\n    icmp-&gt;icmp_type=ICMP_ECHO;\n    icmp-&gt;icmp_code=0;\n    icmp-&gt;icmp_cksum=0;\n    icmp-&gt;icmp_seq=pack_no;\n    icmp-&gt;icmp_id=pid;\n    packsize=8+datalen;\n    tval=(struct timeval*)icmp-&gt;icmp_data;\n    gettimeofday(tval, NULL);\n    icmp-&gt;icmp_cksum=cal_chksum((unsigned short *)icmp, packsize);\n    return packsize;\n}\n\nvoid send_packet()  \n{\n    int packetsize;\n    while(nsend&lt;MAX_NO_PACKETS)\n    {\n        nsend++;\n        packetsize=pack(nsend);\n        if(sendto(sockfd, sendpacket, packetsize, 0, (struct sockaddr *)&amp;dest_addr, sizeof(dest_addr))&lt;0)\n        {\n            perror(\"sendto error\");\n            continue;\n        }\n        //此处插入调用recv_packet()函数使统计和显示更准确\n        recv_packet();\n        sleep(1);\n    }\n}\n\nvoid recv_packet()  \n{\n    int n, fromlen;\n    extern int errno;\n\n    signal(SIGALRM, statistics);\n    fromlen=sizeof(from);\n    while(nreceived&lt;nsend)\n    {\n        alarm(MAX_WAIT_TIME);\n        if((n=recvfrom(sockfd, recvpacket, sizeof(recvpacket), 0, (struct sockaddr *)&amp;from, &amp;fromlen))&lt;0)\n        {\n            if(errno==EINTR)\n                continue;\n            perror(\"recvfrom error\");\n            continue;\n        }\n        gettimeofday(&amp;tvrecv, NULL);\n        if(unpack(recvpacket, n)==-1)\n            continue;\n        nreceived++;\n    }\n}\n\nint unpack(char *buf, int len)  \n{\n    int i, iphdrlen;\n    struct ip *ip;\n    struct icmp *icmp;\n    struct timeval *tvsend;\n    double rtt;\n\n    ip=(struct ip*)buf;\n    iphdrlen=ip-&gt;ip_hl&lt;&lt;2;\n    icmp=(struct icmp*)(buf+iphdrlen);\n    len-=iphdrlen;\n    if(len&lt;8)\n    {\n        printf(\"ICMP packets\\'s length is less than 8\\n\");\n        return -1;\n    }\n    if((icmp-&gt;icmp_type==ICMP_ECHOREPLY) &amp;&amp; (icmp-&gt;icmp_id==pid))\n    {\n        tvsend=(struct timeval *)icmp-&gt;icmp_data;\n        tv_sub(&amp;tvrecv, tvsend);\n        //为了让时间显示更精确，强制转换tv_usec(long)为double类型\n        rtt=tvrecv.tv_sec*1000+(double)tvrecv.tv_usec/1000; \n        printf(\"%d bytes from %s: icmp_seq=%u ttl=%d time=%.3f ms\\n\", len, inet_ntoa(from.sin_addr), icmp-&gt;icmp_seq, ip-&gt;ip_ttl, rtt);\n    }\n    else\n        return -1;\n}\n\nvoid tv_sub(struct timeval *out, struct timeval *in)  \n{\n    if((out-&gt;tv_usec-=in-&gt;tv_usec)&lt;0)\n    {\n        --out-&gt;tv_sec;\n        out-&gt;tv_usec+=1000000;\n    }\n    out-&gt;tv_sec-=in-&gt;tv_sec;\n}\n\nmain(int argc, char *argv[])  \n{\n    struct hostent *host;\n    struct protoent *protocol;\n    unsigned long inaddr=01;\n    int waittime=MAX_WAIT_TIME;\n    int size=50*1024;\n\n    if(argc&lt;2)\n    {\n        printf(\"usage: %s hostname/IP address\\n\", argv[0]);\n        exit(1);\n    }\n    if((protocol=getprotobyname(\"icmp\"))==NULL)\n    {\n        perror(\"getprotocolbyname\");\n        exit(1);\n    }\n    if((sockfd=socket(AF_INET, SOCK_RAW, protocol-&gt;p_proto))&lt;0)\n    {\n        perror(\"socket error\");\n        exit(1);\n    }\n    setuid(getuid());\n    setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &amp;size, sizeof(size));\n    bzero(&amp;dest_addr, sizeof(dest_addr));\n    dest_addr.sin_family=AF_INET;\n    inaddr=inet_addr(argv[1]);\n    if(inaddr==INADDR_NONE)\n    {\n        if((host=gethostbyname(argv[1]))==NULL)\n        {\n            perror(\"gethostbyname error\");\n            exit(1);\n        }\n        memcpy((char *)&amp;dest_addr.sin_addr, host-&gt;h_addr, host-&gt;h_length);\n    }\n    else\n    {\n        //memcpy((char *)&amp;dest_addr, (char *)&amp;inaddr, host-&gt;h_length);\n        //源程序此处为BUG，因为if条件不满足则host-&gt;h_length为空值，运行时导致段错误\n        //此处直接用sizeof取inaddr的长度即可\n        //而且dest_addr不能强制转换为char*，应该是作者漏掉了成员.sin_addr\n        memcpy((char *)&amp;dest_addr.sin_addr, (char *)&amp;inaddr, sizeof(inaddr));\n    }\n    pid=getpid();\n    //由于在ICMP包中加入了时间统计数据，实际发送包大小是56+8字节\n    printf(\"PING %s(%s): %d bytes data in ICMP packets.\\n\", argv[1], inet_ntoa(dest_addr.sin_addr), datalen+8);\n    send_packet();\n    //recv_packet()函数放到send_packet()函数里面，让程序可以及时显示发送进度\n    //recv_packet();\n    statistics(SIGALRM);\n    return 0;\n}\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402648078810,"created_by":1,"updated_at":1402649420485,"updated_by":1,"published_at":1402648078813,"published_by":1},{"id":11,"uuid":"f54cc4dc-e880-4de9-b212-9bce648178c5","title":"GoAgentUI发布","slug":"release-goagentui","markdown":"## GoAgentUI发布\n\n[@招牌疯子](http://weibo.com/819880808)\n\n经过两个晚上的边学边做，GoAgentUI终于发布了，虽然是个小的不能再小的应用，但怎么说也是第一次写MAC和Linux下的GUI程序，还是挺有意思的。\n\n软件功能很简单，如果你是goagent翻墙用户，并且跟我一样无法忍受每次开机都要打开一个terminal，切换到goagent的目录下运行python proxy.py，同时还要一直保持这个窗口开着的话，那么GoAgentUI将是你的最好选择。先上一个运行截图：\n\n![GoAgentUI](http://single1024.qiniudn.com/wp-content/uploads/2013/05/GoAgentUI.png)\n\n需要使用的同学请到[项目主页](http://goagentui.buaa.us/)下载最新版本的安装包，第一次运行时点击Open选择你的goagent目录下的proxy.py文件，再按一下Start就行了。如果你勾选了Autostart和Autohide的话，以后每次打开GoAgentUI后程序会自动运行proxy.py并最小化，你就可以专心去做你的事情。\n\nGoAgent可以检测出启动脚本失败的原因；比如你忘记了自己什么时候手动运行过python proxy.py后8087端口被占用，GoAgentUI将无法启动proxy.py，这时你只需要点一下Stop命令就可以结束那个后台脚本，将代理程序完全接管给可视化程度更好的GoAgentUI。还可以查看运行时的日志，只需要把窗口拉大即可。\n\n备注说明：\n\n本程序只负责启动goagent脚本，仅适用于看不惯多出来的那个黑窗口的童鞋，至于如何配置goagent翻墙，请到其项目主页看详细的说明，地址是[https://code.google.com/p/goagent/](https://code.google.com/p/goagent/)。\n\n另外，MAC平台下有一个功能更为强大，大到我觉得过了的软件，GoAgentX，集下载、配置、监控、运行于一体（真想说有本事你集成上自动申请GAE的功能呀），感兴趣的亦可下载使用，地址是[https://github.com/ohdarling/GoAgentX](https://github.com/ohdarling/GoAgentX)。\n\n最后再次申明，第一次写，看不顺眼的或者遇到BUG的还请果断留言批评指正，先谢过了~\n\nUpdate，Linux版本已发布，如果你有QT环境，那么大可以只留下一个GoAgentUI二进制文件即可；没有的话，参照压缩包内的txt文件把当前目录加入到$LD_LIBRARY_PATH中。具体命令如下：\n\n```\n1.vim ~/.bashrc\n2.added this line to the end:\n        LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH\n3.save and quit vim\n4.source ~/.bashrc\n```\n\nPPS，刚才去goagent的主页看到最新的3.0测试版已经有了“完善 mac/linux 下的 GUI loader”，让人甚为欣慰，毕竟人家是官方的么，共勉了。\n\n考虑到部分windows用户也有此需求（其实goagent自带的那个exe应该也够用了吧。。。），决定加入对windows的支持，但是刚才去看了一下代码，文件系统和进程管理的方式还得改，所以各位再耐心等待一下。","html":"<h2 id=\"goagentui\">GoAgentUI发布</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>经过两个晚上的边学边做，GoAgentUI终于发布了，虽然是个小的不能再小的应用，但怎么说也是第一次写MAC和Linux下的GUI程序，还是挺有意思的。</p>\n\n<p>软件功能很简单，如果你是goagent翻墙用户，并且跟我一样无法忍受每次开机都要打开一个terminal，切换到goagent的目录下运行python proxy.py，同时还要一直保持这个窗口开着的话，那么GoAgentUI将是你的最好选择。先上一个运行截图：</p>\n\n<p><img src=\"http://single1024.qiniudn.com/wp-content/uploads/2013/05/GoAgentUI.png\" alt=\"GoAgentUI\" /></p>\n\n<p>需要使用的同学请到<a href=\"http://goagentui.buaa.us/\">项目主页</a>下载最新版本的安装包，第一次运行时点击Open选择你的goagent目录下的proxy.py文件，再按一下Start就行了。如果你勾选了Autostart和Autohide的话，以后每次打开GoAgentUI后程序会自动运行proxy.py并最小化，你就可以专心去做你的事情。</p>\n\n<p>GoAgent可以检测出启动脚本失败的原因；比如你忘记了自己什么时候手动运行过python proxy.py后8087端口被占用，GoAgentUI将无法启动proxy.py，这时你只需要点一下Stop命令就可以结束那个后台脚本，将代理程序完全接管给可视化程度更好的GoAgentUI。还可以查看运行时的日志，只需要把窗口拉大即可。</p>\n\n<p>备注说明：</p>\n\n<p>本程序只负责启动goagent脚本，仅适用于看不惯多出来的那个黑窗口的童鞋，至于如何配置goagent翻墙，请到其项目主页看详细的说明，地址是<a href=\"https://code.google.com/p/goagent/\">https://code.google.com/p/goagent/</a>。</p>\n\n<p>另外，MAC平台下有一个功能更为强大，大到我觉得过了的软件，GoAgentX，集下载、配置、监控、运行于一体（真想说有本事你集成上自动申请GAE的功能呀），感兴趣的亦可下载使用，地址是<a href=\"https://github.com/ohdarling/GoAgentX\">https://github.com/ohdarling/GoAgentX</a>。</p>\n\n<p>最后再次申明，第一次写，看不顺眼的或者遇到BUG的还请果断留言批评指正，先谢过了~</p>\n\n<p>Update，Linux版本已发布，如果你有QT环境，那么大可以只留下一个GoAgentUI二进制文件即可；没有的话，参照压缩包内的txt文件把当前目录加入到$LD<em>LIBRARY</em>PATH中。具体命令如下：</p>\n\n<pre><code>1.vim ~/.bashrc  \n2.added this line to the end:  \n        LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH\n3.save and quit vim  \n4.source ~/.bashrc  \n</code></pre>\n\n<p>PPS，刚才去goagent的主页看到最新的3.0测试版已经有了“完善 mac/linux 下的 GUI loader”，让人甚为欣慰，毕竟人家是官方的么，共勉了。</p>\n\n<p>考虑到部分windows用户也有此需求（其实goagent自带的那个exe应该也够用了吧。。。），决定加入对windows的支持，但是刚才去看了一下代码，文件系统和进程管理的方式还得改，所以各位再耐心等待一下。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402648566448,"created_by":1,"updated_at":1402652572170,"updated_by":1,"published_at":1402648566450,"published_by":1},{"id":12,"uuid":"8dc4e0d2-3e42-4fd3-bd4e-1547a87439d7","title":"进程间大数据拷贝方法调研","slug":"data-copy-between-processes","markdown":"##进程间大数据拷贝方法调研\n[@招牌疯子](http://weibo.com/819880808)\n\n###综述\n多进程程序经常会遇到数据拷贝的需求，一般情况下的进程间通信可以认为是个传递消息的场景。在此场景中，原始数据和目标数据都存储于内存里，传递的消耗比较小，而且有大量成熟的消息库可以拿来使用，本文不再赘述。但是如果我们需要传递的是一个容量很大的数据，它们不能长期存储于内存中，就会涉及到从磁盘取出，然后传递给目标进程，目标进程再把数据发给磁盘或socket fd的情景。在这个情况下，由于多次数据复制和CPU状态的切换，会导致数据传输性能低下，严重制约整体服务的负载能力。本文将针对此问题调研三种拷贝方法，并进行比较和测试，试图寻找大数据拷贝的最佳方案，以提高既有系统的效率。\n\n###传统数据传输流程\n当我们需要打开或保存磁盘上的一个文件时，最先想到的就是read()和write()方法，我们用一个进程从源文件fd中read数据，然后放到管道里，另一个进程通过管道把这些数据write到磁盘或socket fd中，完成拷贝流程。\n\nLinux中传统的 I/O 操作是一种缓冲I/O，I/O过程中产生的数据传输通常需要在缓冲区中进行多次的拷贝操作。一般来说，在传输数据的时候，用户应用程序需要分配一块大小合适的缓冲区用来存放需要传输的数据。应用程序从文件中读取一块数据，然后把这块数据通过网络发送到接收端去。用户应用程序只是需要调用两个系统调用read()和write()就可以完成这个数据传输操作，应用程序并不知晓在这个数据传输的过程中操作系统所做的数据拷贝操作。对于Linux操作系统来说，基于数据排序或者校验等各方面因素的考虑，操作系统内核会在处理数据传输的过程中进行多次拷贝操作。在某些情况下，这些数据拷贝操作会极大地降低数据传输的性能。\n\n当应用程序需要访问某块数据的时候，操作系统内核会先检查这块数据是不是因为前一次对相同文件的访问而已经被存放在操作系统内核地址空间的缓冲区内，如果在内核缓冲区中找不到这块数据，Linux操作系统内核会先将这块数据从磁盘读出来放到操作系统内核的缓冲区里去。如果这个数据读取操作是由DMA完成的，那么在DMA进行数据读取的这一过程中，CPU只是需要进行缓冲区管理，以及创建和处理DMA，除此之外，CPU不需要再做更多的事情，DMA执行完数据读取操作之后，会通知操作系统做进一步的处理。Linux操作系统会根据read()系统调用指定的应用程序地址空间的地址，把这块数据存放到请求这块数据的应用程序的地址空间中去，在接下来的处理过程中，操作系统需要将数据再一次从用户应用程序地址空间的缓冲区拷贝到与网络堆栈相关的内核缓冲区中去，这个过程也是需要占用CPU的。数据拷贝操作结束以后，数据会被打包，然后发送到网络接口卡上去。在数据传输的过程中，应用程序可以先返回进而执行其他的操作。之后，在调用write()系统调用的时候，用户应用程序缓冲区中的数据内容可以被安全的丢弃或者更改，因为操作系统已经在内核缓冲区中保留了一份数据拷贝，当数据被成功传送到硬件上之后，这份数据拷贝就可以被丢弃。\n\n从上面的描述可以看出，在这种传统的数据传输过程中，数据至少发生了四次拷贝操作，即便是使用了DMA来进行与硬件的通讯，CPU仍然需要访问数据两次。在read()读数据的过程中，数据并不是直接来自于硬盘，而是必须先经过操作系统的文件系统层。在write()写数据的过程中，为了和要传输的数据包的大小相吻合，数据必须要先被分割成块，而且还要预先考虑包头，并且要进行数据校验和操作。\n\n图 1. 传统使用read和write系统调用的数据传输\n\n![传统使用 read 和 write 系统调用的数据传输](http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/image001.jpg)\n\n上述流程还只是单进程读写操作的情况，如果是进程间数据拷贝，还需要加上用户空间往管道拷贝的过程，性能进一步下降。\n\n###零拷贝（zero copy）技术\n\n简单一点来说，零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术。针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源。这种性能的提升就是通过在数据拷贝进行的同时，允许 CPU 执行其他的任务来实现的。零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率。而且，零拷贝技术减少了用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。进行大量的数据拷贝操作其实是一件简单的任务，从操作系统的角度来说，如果 CPU 一直被占用着去执行这项简单的任务，那么这将会是很浪费资源的；如果有其他比较简单的系统部件可以代劳这件事情，从而使得 CPU 解脱出来可以做别的事情，那么系统资源的利用则会更加有效。综上所述，零拷贝技术的目标可以概括如下：\n\n**避免数据拷贝**\n\n- 避免操作系统内核缓冲区之间进行数据拷贝操作。\n\n- 避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。\n\n- 用户应用程序可以避开操作系统直接访问硬件存储。\n\n- 数据传输尽量让 DMA 来做。\n\n**将多种操作结合在一起**\n\n- 避免不必要的系统调用和上下文切换。\n\n- 需要拷贝的数据可以先被缓存起来。\n\n- 对数据进行处理尽量让硬件来做。\n\n**零拷贝技术分类**\n\n零拷贝技术的发展很多样化，现有的零拷贝技术种类也非常多，而当前并没有一个适合于所有场景的零拷贝技术的出现。对于 Linux 来说，现存的零拷贝技术也比较多，这些零拷贝技术大部分存在于不同的 Linux 内核版本，有些旧的技术在不同的 Linux 内核版本间得到了很大的发展或者已经渐渐被新的技术所代替。本文针对这些零拷贝技术所适用的不同场景对它们进行了划分。概括起来，Linux 中的零拷贝技术主要有下面这几种：\n\n- 直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输：这类零拷贝技术针对的是操作系统内核并不需要对数据进行直接处理的情况，数据可以在应用程序地址空间的缓冲区和磁盘之间直接进行传输，完全不需要 Linux 操作系统内核提供的页缓存的支持。\n\n- 在数据传输的过程中，避免数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间进行拷贝。有的时候，应用程序在数据进行传输的过程中不需要对数据进行访问，那么，将数据从 Linux 的页缓存拷贝到用户进程的缓冲区中就可以完全避免，传输的数据在页缓存中就可以得到处理。在某些特殊的情况下，这种零拷贝技术可以获得较好的性能。Linux 中提供类似的系统调用主要有 mmap()，sendfile() 以及 splice()。\n\n- 对数据在 Linux 的页缓存和用户进程的缓冲区之间的传输过程进行优化。该零拷贝技术侧重于灵活地处理数据在用户进程的缓冲区和操作系统的页缓存之间的拷贝操作。这种方法延续了传统的通信方式，但是更加灵活。在Linux中，该方法主要利用了写时复制技术。\n\n前两类方法的目的主要是为了避免应用程序地址空间和操作系统内核地址空间这两者之间的缓冲区拷贝操作。这两类零拷贝技术通常适用在某些特殊的情况下，比如要传送的数据不需要经过操作系统内核的处理或者不需要经过应用程序的处理。第三类方法则继承了传统的应用程序地址空间和操作系统内核地址空间之间数据传输的概念，进而针对数据传输本身进行优化。我们知道，硬件和软件之间的数据传输可以通过使用 DMA 来进行，DMA 进行数据传输的过程中几乎不需要CPU 参与，这样就可以把 CPU 解放出来去做更多其他的事情，但是当数据需要在用户地址空间的缓冲区和Linux 操作系统内核的页缓存之间进行传输的时候，并没有类似DMA 这种工具可以使用，CPU 需要全程参与到这种数据拷贝操作中，所以这第三类方法的目的是可以有效地改善数据在用户地址空间和操作系统内核地址空间之间传递的效率。\n\n*注：在本文下面部分中将主要调研和对比第二类零拷贝技术。*\n\n###三种方法对比\n\n####1. 传统方法\n\n如本文第二部分所描述的那样，父进程读取原始数据，拷贝至管道中，子进程从管道中获取数据，再写到磁盘上，使用read()和write()方法。\n\n*process_1 sendfile():*\n\n\tchar buffer[BUF_SIZE];\n    while((bytes = read(in_fd,buffer,sizeof(buffer))) >0)\n    {\n        if(write(pipefd[1],buffer,bytes) != bytes)\n        {\n            perror(\"write pipe errno\");\n            exit(1);\n        }\n    }\n\n*process_2 getfile():*\n\n    char buffer[BUF_SIZE];\n    while(len > 0)\n    {\n        if((bytes = read(pipefd[0],buffer,sizeof(buffer))) < 0)\n        {\n            perror(\"read pipefd error\");\n            exit(1);\n        }\n        if((write(out_fd1,buffer,bytes)) != bytes)\n        {\n            perror(\"write out_fd1 error\");\n            exit(1);\n        }\n        else\n            len -= bytes;\n    }\n\n####2. mmap和共享内存方法\n\n此方法采用mmap将源数据fd映射至内存中，然后进行memcpy拷贝给共享内存，其他进程也将至目标数据fd进行mmap映射至内存中，再从共享内存memcpy出来，这样当memcpy结束时，数据就已经拷贝至目标fd中，减少了拷贝次数。\n\n*process_1 sendfile():*\n\n    void *src = mmap(NULL, len, PROT_READ, MAP_SHARED, in_fd, 0);\n    if(src==MAP_FAILED) {\n        perror(\"mmap map src faild\");\n        return;\n    }\n\n    void *shm = shared_memory;\n\n    int size=BUF_SIZE,total=0;\n    while(total < len)\n    {\n        size = len - total > BUF_SIZE ? BUF_SIZE : len-total;\n        memcpy(shm,src,size);\n        shm += size;\n        src += size;\n        total += size;\n        //printf(\"total_write=%d size=%d\\n\", total, size);\n    }\n\n    munmap(src, len);\n\n*process_2 getfile():*\n\n    if(ftruncate(out_fd2, len) < 0) {\n        perror(\"ftruncate faild\");\n        return;\n    }\n    void *dst = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_SHARED, out_fd2, 0);\n    if(dst==MAP_FAILED) {\n        perror(\"mmap map dst faild\");\n        return;\n    }\n\n    void *shm = shared_memory;\n\n    int size=BUF_SIZE,total=0;\n    while(total < len)\n    {\n        size = len - total > BUF_SIZE ? BUF_SIZE : len-total;\n        memcpy(dst, shm, size);\n        shm += size;\n        dst += size;\n        total += size;\n        //printf(\"total_read=%d size=%d\\n\", total, size);\n    }\n\n    munmap(dst, len);\n\n**mmap()详解**\n\n在 Linux 中，减少拷贝次数的一种方法是调用 mmap() 来代替调用 read，比如：\n\n    tmp_buf = mmap(file, len); \n    write(socket, tmp_buf, len);\n\n首先，应用程序调用了 mmap() 之后，数据会先通过 DMA 拷贝到操作系统内核的缓冲区中去。接着，应用程序跟操作系统共享这个缓冲区，这样，操作系统内核和应用程序存储空间就不需要再进行任何的数据拷贝操作。应用程序调用了 write() 之后，操作系统内核将数据从原来的内核缓冲区中拷贝到与 socket 相关的内核缓冲区中。接下来，数据从内核 socket 缓冲区拷贝到协议引擎中去，这是第三次数据拷贝操作。\n\n图 2. 利用 mmap() 代替 read()\n\n![利用 mmap() 代替 read()](http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/image002.jpg)\n\n####3. splice()方法\n\n该方法中我们先启用一对管道，在父进程中将原始数据fd和pipe[1]进行splice，然后在子进程中将pipe[0]和目标数据fd进行splice，代码简单，在两次splice之后就完成了数据的进程间拷贝。\n\n\n*process_1 sendfile():*\n\n    while(len > 0)\n    {\n        if((bytes=splice(in_fd,NULL,pipefd[1],NULL,len,0x1))<0)\n        {\n            perror(\"splice in_fd faild\");\n            return;\n        }\n        else\n            len -= bytes;\n    }\n\n*process_2 getfile():*\n\n    while(len > 0)\n    {\n        if((bytes=splice(pipefd[0],NULL,out_fd3,NULL,len,0x1))<0)\n        {\n            perror(\"splice out_fd3 faild\");\n            return;\n        }\n        else\n            len -= bytes;\n    }\n\n**splice()详解**\n\nsplice() 是Linux中与 mmap() 和 sendfile() 类似的一种方法。它也可以用于用户应用程序地址空间和操作系统地址空间之间的数据传输。splice() 适用于可以确定数据传输路径的用户应用程序，它不需要利用用户地址空间的缓冲区进行显式的数据传输操作。那么，当数据只是从一个地方传送到另一个地方，过程中所传输的数据不需要经过用户应用程序的处理的时候，spice() 就成为了一种比较好的选择。splice() 可以在操作系统地址空间中整块地移动数据，从而减少大多数数据拷贝操作。而且，splice() 进行数据传输可以通过异步的方式来进行，用户应用程序可以先从系统调用返回，而操作系统内核进程会控制数据传输过程继续进行下去。splice() 可以被看成是类似于基于流的管道的实现，管道可以使得两个文件描述符相互连接，splice 的调用者则可以控制两个设备（或者协议栈）在操作系统内核中的相互连接。\n\nsplice() 系统调用和 sendfile() 非常类似，用户应用程序必须拥有两个已经打开的文件描述符，一个用于表示输入设备，一个用于表示输出设备。与 sendfile() 不同的是，splice() 允许任意两个文件之间互相连接，而并不只是文件到 socket 进行数据传输。对于从一个文件描述符发送数据到 socket 这种特例来说，一直都是使用 sendfile() 这个系统调用，而 splice 一直以来就只是一种机制，它并不仅限于 sendfile() 的功能。也就是说，sendfile() 只是 splice() 的一个子集，在 Linux 2.6.23 中，sendfile() 这种机制的实现已经没有了，但是这个 API 以及相应的功能还存在，只不过 API 以及相应的功能是利用了 splice() 这种机制来实现的。\n\n在数据传输的过程中，splice() 机制交替地发送相关的文件描述符的读写操作，并且可以将读缓冲区重新用于写操作。它也利用了一种简单的流控制，通过预先定义的水印（ watermark ）来阻塞写请求。有实验表明，利用这种方法将数据从一个磁盘传输到另一个磁盘会增加 30% 到 70% 的吞吐量，数据传输的过程中，CPU 的负载也会减少一半。\n\nLinux 2.6.17 内核引入了 splice() 系统调用，但是，这个概念在此之前其实已经存在了很长一段时间了。1988 年，Larry McVoy 提出了这个概念，它被看成是一种改进服务器端系统的 I/O 性能的一种技术，尽管在之后的若干年中经常被提及，但是 splice 系统调用从来没有在主流的 Linux 操作系统内核中实现过，一直到 Linux 2.6.17 版本的出现。splice 系统调用需要用到四个参数，其中两个是文件描述符，一个表示文件长度，还有一个用于控制如何进行数据拷贝。splice 系统调用可以同步实现，也可以使用异步方式来实现。在使用异步方式的时候，用户应用程序会通过信号 SIGIO 来获知数据传输已经终止。splice() 系统调用的接口如下所示：\n\n    long splice(int fdin, int fdout, size_t len, unsigned int flags);\n\n调用 splice() 系统调用会导致操作系统内核从数据源 fdin 移动最多 len 个字节的数据到 fdout 中去，这个数据的移动过程只是经过操作系统内核空间，需要最少的拷贝次数。使用 splice() 系统调用需要这两个文件描述符中的一个必须是用来表示一个管道设备的。splice() 系统调用利用了 Linux 提出的管道缓冲区（ pipe buffer ）机制，这就是为什么这个系统调用的两个文件描述符参数中至少有一个必须要指代管道设备的原因。\n\n####4. 性能对比测试\n\n依照本文上面提出的思路，将三种拷贝方法用代码（测试代码在最后给出）实现出来，并统计每种方法所使用的时间，为了更精确地反映三种拷贝方法的优劣，统计时间只包括父进程发送数据的时间sendfile和子进程接收数据的时间getfile，其他CPU消耗不进行统计。\n\n测试对象是一个1.4G的二进制文件，上述三种方法各进行5次拷贝，计算平均值，测试结果如下：\n\n![copy_test_result](http://ww1.sinaimg.cn/large/599be90djw1ec951ddquij20wd0a0goe.jpg)\n\n###总结\n\n说得再多，也抵不过测试结果，从测试数据来看，splice方法完胜其它两种，性能提升约30%～55%。\n\nmmap+memcpy的方法也还不错，但是这是在完全理想的情况下的结果，本测试中忽略了共享内存同步问题，规避了大量的加减锁操作，直接申请了2G的共享内存，而实际工程中会涉及到大量的锁操作，效果会下降不少。\n\n传统read/write方法完败，并不是说无用武之地，而只是在拷贝这个特殊情景下效果比较差，如果是普通应用中只涉及单次读写或者应用程序对数据需要进行操作的场景，零复制方法并不适合。\n\n最后，如果你所参与的项目也是类似微博图床这样需要存储和处理海量图片数据，需要从前到后不断优化和提升的系统，欢迎留下微博ID，有机会相互探讨。\n\n> 如需转载，请注明出处，谢谢。\n\n###参考资料和测试代码\n\n[Linux 中的零拷贝技术，第 1 部分](http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html)\n\n[Linux 中的零拷贝技术，第 2 部分](https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/)\n\n[文中所使用的测试代码](https://github.com/buaazp/work/blob/master/copy_test.c)\n\n[测试结果原始数据](https://github.com/buaazp/work)","html":"<h2 id=\"\">进程间大数据拷贝方法调研</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<h3 id=\"\">综述</h3>\n\n<p>多进程程序经常会遇到数据拷贝的需求，一般情况下的进程间通信可以认为是个传递消息的场景。在此场景中，原始数据和目标数据都存储于内存里，传递的消耗比较小，而且有大量成熟的消息库可以拿来使用，本文不再赘述。但是如果我们需要传递的是一个容量很大的数据，它们不能长期存储于内存中，就会涉及到从磁盘取出，然后传递给目标进程，目标进程再把数据发给磁盘或socket fd的情景。在这个情况下，由于多次数据复制和CPU状态的切换，会导致数据传输性能低下，严重制约整体服务的负载能力。本文将针对此问题调研三种拷贝方法，并进行比较和测试，试图寻找大数据拷贝的最佳方案，以提高既有系统的效率。</p>\n\n<h3 id=\"\">传统数据传输流程</h3>\n\n<p>当我们需要打开或保存磁盘上的一个文件时，最先想到的就是read()和write()方法，我们用一个进程从源文件fd中read数据，然后放到管道里，另一个进程通过管道把这些数据write到磁盘或socket fd中，完成拷贝流程。</p>\n\n<p>Linux中传统的 I/O 操作是一种缓冲I/O，I/O过程中产生的数据传输通常需要在缓冲区中进行多次的拷贝操作。一般来说，在传输数据的时候，用户应用程序需要分配一块大小合适的缓冲区用来存放需要传输的数据。应用程序从文件中读取一块数据，然后把这块数据通过网络发送到接收端去。用户应用程序只是需要调用两个系统调用read()和write()就可以完成这个数据传输操作，应用程序并不知晓在这个数据传输的过程中操作系统所做的数据拷贝操作。对于Linux操作系统来说，基于数据排序或者校验等各方面因素的考虑，操作系统内核会在处理数据传输的过程中进行多次拷贝操作。在某些情况下，这些数据拷贝操作会极大地降低数据传输的性能。</p>\n\n<p>当应用程序需要访问某块数据的时候，操作系统内核会先检查这块数据是不是因为前一次对相同文件的访问而已经被存放在操作系统内核地址空间的缓冲区内，如果在内核缓冲区中找不到这块数据，Linux操作系统内核会先将这块数据从磁盘读出来放到操作系统内核的缓冲区里去。如果这个数据读取操作是由DMA完成的，那么在DMA进行数据读取的这一过程中，CPU只是需要进行缓冲区管理，以及创建和处理DMA，除此之外，CPU不需要再做更多的事情，DMA执行完数据读取操作之后，会通知操作系统做进一步的处理。Linux操作系统会根据read()系统调用指定的应用程序地址空间的地址，把这块数据存放到请求这块数据的应用程序的地址空间中去，在接下来的处理过程中，操作系统需要将数据再一次从用户应用程序地址空间的缓冲区拷贝到与网络堆栈相关的内核缓冲区中去，这个过程也是需要占用CPU的。数据拷贝操作结束以后，数据会被打包，然后发送到网络接口卡上去。在数据传输的过程中，应用程序可以先返回进而执行其他的操作。之后，在调用write()系统调用的时候，用户应用程序缓冲区中的数据内容可以被安全的丢弃或者更改，因为操作系统已经在内核缓冲区中保留了一份数据拷贝，当数据被成功传送到硬件上之后，这份数据拷贝就可以被丢弃。</p>\n\n<p>从上面的描述可以看出，在这种传统的数据传输过程中，数据至少发生了四次拷贝操作，即便是使用了DMA来进行与硬件的通讯，CPU仍然需要访问数据两次。在read()读数据的过程中，数据并不是直接来自于硬盘，而是必须先经过操作系统的文件系统层。在write()写数据的过程中，为了和要传输的数据包的大小相吻合，数据必须要先被分割成块，而且还要预先考虑包头，并且要进行数据校验和操作。</p>\n\n<p>图 1. 传统使用read和write系统调用的数据传输</p>\n\n<p><img src=\"http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/image001.jpg\" alt=\"传统使用 read 和 write 系统调用的数据传输\" /></p>\n\n<p>上述流程还只是单进程读写操作的情况，如果是进程间数据拷贝，还需要加上用户空间往管道拷贝的过程，性能进一步下降。</p>\n\n<h3 id=\"zerocopy\">零拷贝（zero copy）技术</h3>\n\n<p>简单一点来说，零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术。针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源。这种性能的提升就是通过在数据拷贝进行的同时，允许 CPU 执行其他的任务来实现的。零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率。而且，零拷贝技术减少了用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。进行大量的数据拷贝操作其实是一件简单的任务，从操作系统的角度来说，如果 CPU 一直被占用着去执行这项简单的任务，那么这将会是很浪费资源的；如果有其他比较简单的系统部件可以代劳这件事情，从而使得 CPU 解脱出来可以做别的事情，那么系统资源的利用则会更加有效。综上所述，零拷贝技术的目标可以概括如下：</p>\n\n<p><strong>避免数据拷贝</strong></p>\n\n<ul>\n<li><p>避免操作系统内核缓冲区之间进行数据拷贝操作。</p></li>\n<li><p>避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。</p></li>\n<li><p>用户应用程序可以避开操作系统直接访问硬件存储。</p></li>\n<li><p>数据传输尽量让 DMA 来做。</p></li>\n</ul>\n\n<p><strong>将多种操作结合在一起</strong></p>\n\n<ul>\n<li><p>避免不必要的系统调用和上下文切换。</p></li>\n<li><p>需要拷贝的数据可以先被缓存起来。</p></li>\n<li><p>对数据进行处理尽量让硬件来做。</p></li>\n</ul>\n\n<p><strong>零拷贝技术分类</strong></p>\n\n<p>零拷贝技术的发展很多样化，现有的零拷贝技术种类也非常多，而当前并没有一个适合于所有场景的零拷贝技术的出现。对于 Linux 来说，现存的零拷贝技术也比较多，这些零拷贝技术大部分存在于不同的 Linux 内核版本，有些旧的技术在不同的 Linux 内核版本间得到了很大的发展或者已经渐渐被新的技术所代替。本文针对这些零拷贝技术所适用的不同场景对它们进行了划分。概括起来，Linux 中的零拷贝技术主要有下面这几种：</p>\n\n<ul>\n<li><p>直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输：这类零拷贝技术针对的是操作系统内核并不需要对数据进行直接处理的情况，数据可以在应用程序地址空间的缓冲区和磁盘之间直接进行传输，完全不需要 Linux 操作系统内核提供的页缓存的支持。</p></li>\n<li><p>在数据传输的过程中，避免数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间进行拷贝。有的时候，应用程序在数据进行传输的过程中不需要对数据进行访问，那么，将数据从 Linux 的页缓存拷贝到用户进程的缓冲区中就可以完全避免，传输的数据在页缓存中就可以得到处理。在某些特殊的情况下，这种零拷贝技术可以获得较好的性能。Linux 中提供类似的系统调用主要有 mmap()，sendfile() 以及 splice()。</p></li>\n<li><p>对数据在 Linux 的页缓存和用户进程的缓冲区之间的传输过程进行优化。该零拷贝技术侧重于灵活地处理数据在用户进程的缓冲区和操作系统的页缓存之间的拷贝操作。这种方法延续了传统的通信方式，但是更加灵活。在Linux中，该方法主要利用了写时复制技术。</p></li>\n</ul>\n\n<p>前两类方法的目的主要是为了避免应用程序地址空间和操作系统内核地址空间这两者之间的缓冲区拷贝操作。这两类零拷贝技术通常适用在某些特殊的情况下，比如要传送的数据不需要经过操作系统内核的处理或者不需要经过应用程序的处理。第三类方法则继承了传统的应用程序地址空间和操作系统内核地址空间之间数据传输的概念，进而针对数据传输本身进行优化。我们知道，硬件和软件之间的数据传输可以通过使用 DMA 来进行，DMA 进行数据传输的过程中几乎不需要CPU 参与，这样就可以把 CPU 解放出来去做更多其他的事情，但是当数据需要在用户地址空间的缓冲区和Linux 操作系统内核的页缓存之间进行传输的时候，并没有类似DMA 这种工具可以使用，CPU 需要全程参与到这种数据拷贝操作中，所以这第三类方法的目的是可以有效地改善数据在用户地址空间和操作系统内核地址空间之间传递的效率。</p>\n\n<p><em>注：在本文下面部分中将主要调研和对比第二类零拷贝技术。</em></p>\n\n<h3 id=\"\">三种方法对比</h3>\n\n<h4 id=\"1\">1. 传统方法</h4>\n\n<p>如本文第二部分所描述的那样，父进程读取原始数据，拷贝至管道中，子进程从管道中获取数据，再写到磁盘上，使用read()和write()方法。</p>\n\n<p><em>process_1 sendfile():</em></p>\n\n<pre><code>char buffer[BUF_SIZE];\nwhile((bytes = read(in_fd,buffer,sizeof(buffer))) &gt;0)\n{\n    if(write(pipefd[1],buffer,bytes) != bytes)\n    {\n        perror(\"write pipe errno\");\n        exit(1);\n    }\n}\n</code></pre>\n\n<p><em>process_2 getfile():</em></p>\n\n<pre><code>char buffer[BUF_SIZE];\nwhile(len &gt; 0)\n{\n    if((bytes = read(pipefd[0],buffer,sizeof(buffer))) &lt; 0)\n    {\n        perror(\"read pipefd error\");\n        exit(1);\n    }\n    if((write(out_fd1,buffer,bytes)) != bytes)\n    {\n        perror(\"write out_fd1 error\");\n        exit(1);\n    }\n    else\n        len -= bytes;\n}\n</code></pre>\n\n<h4 id=\"2mmap\">2. mmap和共享内存方法</h4>\n\n<p>此方法采用mmap将源数据fd映射至内存中，然后进行memcpy拷贝给共享内存，其他进程也将至目标数据fd进行mmap映射至内存中，再从共享内存memcpy出来，这样当memcpy结束时，数据就已经拷贝至目标fd中，减少了拷贝次数。</p>\n\n<p><em>process_1 sendfile():</em></p>\n\n<pre><code>void *src = mmap(NULL, len, PROT_READ, MAP_SHARED, in_fd, 0);\nif(src==MAP_FAILED) {\n    perror(\"mmap map src faild\");\n    return;\n}\n\nvoid *shm = shared_memory;\n\nint size=BUF_SIZE,total=0;\nwhile(total &lt; len)\n{\n    size = len - total &gt; BUF_SIZE ? BUF_SIZE : len-total;\n    memcpy(shm,src,size);\n    shm += size;\n    src += size;\n    total += size;\n    //printf(\"total_write=%d size=%d\\n\", total, size);\n}\n\nmunmap(src, len);\n</code></pre>\n\n<p><em>process_2 getfile():</em></p>\n\n<pre><code>if(ftruncate(out_fd2, len) &lt; 0) {\n    perror(\"ftruncate faild\");\n    return;\n}\nvoid *dst = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_SHARED, out_fd2, 0);\nif(dst==MAP_FAILED) {\n    perror(\"mmap map dst faild\");\n    return;\n}\n\nvoid *shm = shared_memory;\n\nint size=BUF_SIZE,total=0;\nwhile(total &lt; len)\n{\n    size = len - total &gt; BUF_SIZE ? BUF_SIZE : len-total;\n    memcpy(dst, shm, size);\n    shm += size;\n    dst += size;\n    total += size;\n    //printf(\"total_read=%d size=%d\\n\", total, size);\n}\n\nmunmap(dst, len);\n</code></pre>\n\n<p><strong>mmap()详解</strong></p>\n\n<p>在 Linux 中，减少拷贝次数的一种方法是调用 mmap() 来代替调用 read，比如：</p>\n\n<pre><code>tmp_buf = mmap(file, len); \nwrite(socket, tmp_buf, len);\n</code></pre>\n\n<p>首先，应用程序调用了 mmap() 之后，数据会先通过 DMA 拷贝到操作系统内核的缓冲区中去。接着，应用程序跟操作系统共享这个缓冲区，这样，操作系统内核和应用程序存储空间就不需要再进行任何的数据拷贝操作。应用程序调用了 write() 之后，操作系统内核将数据从原来的内核缓冲区中拷贝到与 socket 相关的内核缓冲区中。接下来，数据从内核 socket 缓冲区拷贝到协议引擎中去，这是第三次数据拷贝操作。</p>\n\n<p>图 2. 利用 mmap() 代替 read()</p>\n\n<p><img src=\"http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/image002.jpg\" alt=\"利用 mmap() 代替 read()\" /></p>\n\n<h4 id=\"3splice\">3. splice()方法</h4>\n\n<p>该方法中我们先启用一对管道，在父进程中将原始数据fd和pipe[1]进行splice，然后在子进程中将pipe[0]和目标数据fd进行splice，代码简单，在两次splice之后就完成了数据的进程间拷贝。</p>\n\n<p><em>process_1 sendfile():</em></p>\n\n<pre><code>while(len &gt; 0)\n{\n    if((bytes=splice(in_fd,NULL,pipefd[1],NULL,len,0x1))&lt;0)\n    {\n        perror(\"splice in_fd faild\");\n        return;\n    }\n    else\n        len -= bytes;\n}\n</code></pre>\n\n<p><em>process_2 getfile():</em></p>\n\n<pre><code>while(len &gt; 0)\n{\n    if((bytes=splice(pipefd[0],NULL,out_fd3,NULL,len,0x1))&lt;0)\n    {\n        perror(\"splice out_fd3 faild\");\n        return;\n    }\n    else\n        len -= bytes;\n}\n</code></pre>\n\n<p><strong>splice()详解</strong></p>\n\n<p>splice() 是Linux中与 mmap() 和 sendfile() 类似的一种方法。它也可以用于用户应用程序地址空间和操作系统地址空间之间的数据传输。splice() 适用于可以确定数据传输路径的用户应用程序，它不需要利用用户地址空间的缓冲区进行显式的数据传输操作。那么，当数据只是从一个地方传送到另一个地方，过程中所传输的数据不需要经过用户应用程序的处理的时候，spice() 就成为了一种比较好的选择。splice() 可以在操作系统地址空间中整块地移动数据，从而减少大多数数据拷贝操作。而且，splice() 进行数据传输可以通过异步的方式来进行，用户应用程序可以先从系统调用返回，而操作系统内核进程会控制数据传输过程继续进行下去。splice() 可以被看成是类似于基于流的管道的实现，管道可以使得两个文件描述符相互连接，splice 的调用者则可以控制两个设备（或者协议栈）在操作系统内核中的相互连接。</p>\n\n<p>splice() 系统调用和 sendfile() 非常类似，用户应用程序必须拥有两个已经打开的文件描述符，一个用于表示输入设备，一个用于表示输出设备。与 sendfile() 不同的是，splice() 允许任意两个文件之间互相连接，而并不只是文件到 socket 进行数据传输。对于从一个文件描述符发送数据到 socket 这种特例来说，一直都是使用 sendfile() 这个系统调用，而 splice 一直以来就只是一种机制，它并不仅限于 sendfile() 的功能。也就是说，sendfile() 只是 splice() 的一个子集，在 Linux 2.6.23 中，sendfile() 这种机制的实现已经没有了，但是这个 API 以及相应的功能还存在，只不过 API 以及相应的功能是利用了 splice() 这种机制来实现的。</p>\n\n<p>在数据传输的过程中，splice() 机制交替地发送相关的文件描述符的读写操作，并且可以将读缓冲区重新用于写操作。它也利用了一种简单的流控制，通过预先定义的水印（ watermark ）来阻塞写请求。有实验表明，利用这种方法将数据从一个磁盘传输到另一个磁盘会增加 30% 到 70% 的吞吐量，数据传输的过程中，CPU 的负载也会减少一半。</p>\n\n<p>Linux 2.6.17 内核引入了 splice() 系统调用，但是，这个概念在此之前其实已经存在了很长一段时间了。1988 年，Larry McVoy 提出了这个概念，它被看成是一种改进服务器端系统的 I/O 性能的一种技术，尽管在之后的若干年中经常被提及，但是 splice 系统调用从来没有在主流的 Linux 操作系统内核中实现过，一直到 Linux 2.6.17 版本的出现。splice 系统调用需要用到四个参数，其中两个是文件描述符，一个表示文件长度，还有一个用于控制如何进行数据拷贝。splice 系统调用可以同步实现，也可以使用异步方式来实现。在使用异步方式的时候，用户应用程序会通过信号 SIGIO 来获知数据传输已经终止。splice() 系统调用的接口如下所示：</p>\n\n<pre><code>long splice(int fdin, int fdout, size_t len, unsigned int flags);\n</code></pre>\n\n<p>调用 splice() 系统调用会导致操作系统内核从数据源 fdin 移动最多 len 个字节的数据到 fdout 中去，这个数据的移动过程只是经过操作系统内核空间，需要最少的拷贝次数。使用 splice() 系统调用需要这两个文件描述符中的一个必须是用来表示一个管道设备的。splice() 系统调用利用了 Linux 提出的管道缓冲区（ pipe buffer ）机制，这就是为什么这个系统调用的两个文件描述符参数中至少有一个必须要指代管道设备的原因。</p>\n\n<h4 id=\"4\">4. 性能对比测试</h4>\n\n<p>依照本文上面提出的思路，将三种拷贝方法用代码（测试代码在最后给出）实现出来，并统计每种方法所使用的时间，为了更精确地反映三种拷贝方法的优劣，统计时间只包括父进程发送数据的时间sendfile和子进程接收数据的时间getfile，其他CPU消耗不进行统计。</p>\n\n<p>测试对象是一个1.4G的二进制文件，上述三种方法各进行5次拷贝，计算平均值，测试结果如下：</p>\n\n<p><img src=\"http://ww1.sinaimg.cn/large/599be90djw1ec951ddquij20wd0a0goe.jpg\" alt=\"copy_test_result\" /></p>\n\n<h3 id=\"\">总结</h3>\n\n<p>说得再多，也抵不过测试结果，从测试数据来看，splice方法完胜其它两种，性能提升约30%～55%。</p>\n\n<p>mmap+memcpy的方法也还不错，但是这是在完全理想的情况下的结果，本测试中忽略了共享内存同步问题，规避了大量的加减锁操作，直接申请了2G的共享内存，而实际工程中会涉及到大量的锁操作，效果会下降不少。</p>\n\n<p>传统read/write方法完败，并不是说无用武之地，而只是在拷贝这个特殊情景下效果比较差，如果是普通应用中只涉及单次读写或者应用程序对数据需要进行操作的场景，零复制方法并不适合。</p>\n\n<p>最后，如果你所参与的项目也是类似微博图床这样需要存储和处理海量图片数据，需要从前到后不断优化和提升的系统，欢迎留下微博ID，有机会相互探讨。</p>\n\n<blockquote>\n  <p>如需转载，请注明出处，谢谢。</p>\n</blockquote>\n\n<h3 id=\"\">参考资料和测试代码</h3>\n\n<p><a href=\"http://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html\">Linux 中的零拷贝技术，第 1 部分</a></p>\n\n<p><a href=\"https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/\">Linux 中的零拷贝技术，第 2 部分</a></p>\n\n<p><a href=\"https://github.com/buaazp/work/blob/master/copy_test.c\">文中所使用的测试代码</a></p>\n\n<p><a href=\"https://github.com/buaazp/work\">测试结果原始数据</a></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402648688668,"created_by":1,"updated_at":1402649399925,"updated_by":1,"published_at":1402648688675,"published_by":1},{"id":13,"uuid":"064b2e11-384a-4174-b9bc-0af7a7f332bb","title":"内存拷贝memcpy()与vmsplice()性能对比","slug":"memcpy-vs-vmsplice","markdown":"# 内存拷贝memcpy()与vmsplice()性能对比\n\n[@招牌疯子](http://weibo.com/buaazp)\n\n## 综述\n\n在上一篇文章[《进程间大数据拷贝方法调研》](http://blog.buaa.us/data-copy-between-processes/)中介绍和对比了三种A进程读取文件然后拷贝给B进程的方法，测试结果显示在涉及到内存与磁盘间的数据传输时，splice方法由于避免了内核缓冲区与用户缓冲区之间的多次数据拷贝，表现最好。但是由于这种对比限定在包含I/O读写，且进程不能对数据进行修改的特殊情景中，毕竟在实际情况下不太常见，理论意义大于实际意义。\n\n那本文要探讨的情景，在实际编程过程中就十分常见了：\n\n**A进程的内存中有一大块数据，要传递给B进程。**\n\n为了解决这种情景下的数据传输问题，本文将要对比的两种方法为：\n\n1. 申请共享内存，A进程将数据memcpy进去，B进程再memcpy出来。\n2. 申请一对管道，A进程将数据vmsplice到管道写端，B进程从读端再vmsplice出来放入自己的用户空间。\n\n实际应用中我们大多使用第一种方法，因为感觉上memcpy已经足够快了，那么它们到底谁更快呢，我们还是要用测试数据来说话。\n\n## 测试方法\n\n### memcpy方法\n\n对于memcpy方法，统计的是从A进程用户空间拷贝数据到共享内存所消耗的时间和从共享内存拷贝数据到B进程用户空间的时间。\n\n> 可能有同学会问，为啥不直接将变量定义到共享内存中，这样A修改了之后B直接使用即可，无需拷贝。\n> \n> 这是因为我们期望的状态是A进程有一份自己的数据，B进程也有一份自己的数据，它们是互不干扰的，在大部分时间里是可以保持数据不一致这个状态的，只有需要的时候才进行拷贝。\n\n代码如下：\n\n*sendfile()*\n\n    if(mode == 1)\n    {\n        void *shm = shared_memory;\n        void *src = usrmem;\n\n        int size=BUF_SIZE,total=0;\n        while(total < len)\n        {\n            size = len - total > BUF_SIZE ? BUF_SIZE : len-total;\n            memcpy(shm,src,size);\n            shm += size;\n            src += size;\n            total += size;\n            //printf(\"mode[%d] sendfile() total=%d size=%d\\n\", mode, total, size);\n        }\n        pthread_mutex_unlock(&lock);\n        //cal_md5(shared_memory, len, md5);\n    }\n\nusrmem是用户空间中的内存，在sendfile之前已经灌入了数据。并且在A进程向共享内存拷贝过程中上锁，以阻塞B进程对共享内存的读操作，结束之后解锁。\n\n*getfile()*\n\n    if(mode == 1)\n    {\n        void *shm = shared_memory;\n        void *dst = usrmem;\n\n        pthread_mutex_lock(&lock);\n        int size=BUF_SIZE,total=0;\n        while(total < len)\n        {\n            size = len - total > BUF_SIZE ? BUF_SIZE : len-total;\n            memcpy(dst, shm, size);\n            shm += size;\n            dst += size;\n            total += size;\n            //printf(\"mode[%d] getfile() total=%d size=%d\\n\", mode, total, size);\n        }\n        pthread_mutex_unlock(&lock);\n    }\n    \n读数据的时候也上锁，防止过程中被写入数据造成污染，结束之后解锁。\n\n### vmsplice方法\n\nvmsplice是splice函数族中的一个，它用于映射用户空间到内核空间（载体是管道），使得用户进程可以直接操作内核缓冲区的数据。\n\nsplice函数族包括：\n \n> long splice(int fd_in, off_t *off_in,int fd_out, off_t *off_out,size_t len,unsignedint flags);\n> \n> long tee(int fd_in,int fd_out,size_t len,unsignedint flags);\n> \n> long vmsplice(int fd,conststruct iovec *iov, unsignedlong nr_segs,unsignedint flags);\n> \n> ssize_t sendfile(int out_fd,int in_fd, off_t *offset,size_tcount);\n\n可以看到这一系列函数把操作对象为用户空间、内核空间、文件流fd的映射操作全覆盖了，具体各个函数的用法可以查看man文档。\n\n*sendfile()*\n\n    else if(mode == 2)\n    {\n        char *p = usrmem;\n        size_t send;\n        size_t left = len;\n        long ret;\n        struct iovec iov;\n        long nr_segs = 1;\n        int flags = 0x1;\n\n        while (left > 0)\n        {\n            send = left > PIPE_BUF ? PIPE_BUF : left;\n            iov.iov_base = p;\n            iov.iov_len = send;\n\n            ret = vmsplice(pipefd[1], &iov, nr_segs, flags);\n            if (ret == -1)\n            {\n                perror(\"vmsplice failed\");\n                return;\n            }\n\n            left -= ret;\n            p += ret;\n            //printf(\"mode[%d] sendfile() left=%d ret=%d\\n\", mode, left, ret);\n        }\n    }\n\nvmsplice想要操作用户空间依赖于struct iovec对象，它有两个成员，iov_base是指向用户空间的指针，iov_len是用户空间的大小，由于管道有大小限制（Linux下默认4096），因此在做vmsplice的时候需要不断更新struct iovec的内容。\n\n*getfile()*\n\n    else if(mode == 2)\n    {\n        char *p = usrmem;\n        size_t get;\n        size_t left = len;\n        long ret;\n        struct iovec iov;\n        long nr_segs = 1;\n        int flags = 0x1;\n\n        while (left > 0)\n        {\n            get = left > PIPE_BUF ? PIPE_BUF : left;\n            iov.iov_base = p;\n            iov.iov_len = get;\n\n            ret = vmsplice(pipefd[0], &iov, nr_segs, flags);\n            if (ret == -1)\n            {\n                perror(\"vmsplice failed\");\n                return;\n            }\n\n            left -= ret;\n            p += ret;\n            //printf(\"mode[%d] getfile() left=%d ret=%d\\n\", mode, left, ret);\n        }\n    }\n\ngetfile的时候只需要把第一个参数设为管道的读端，因为vmsplice实际上是一个映射过程，与拷贝不同，它不关心两个参数谁是source谁是destination。\n\n### 对比方法\n\n对比的是上述四个过程所消耗的CPU时间，为了检验数据拷贝是否完成，在拷贝开始前和结束后对A、B进程的用户空间计算MD5值。编译测试程序：\n\n    gcc zmd5.c memcpy_test.c -o memcpy_test\n\n测试对象原始数据是一个1G大小的文件，每种方法各测试20次，统计sendfile过程和getfile过程的时间，两者相加是整个数据拷贝的时间，计算平均值进行对比。代码所在文件夹下有一个脚本run_memcpy_test.sh来做这件事：\n\n    #!/bin/zsh\n    for((i=1;i<=20;i++));\n    do\n        echo \"#\"$i >> ret_mem_1;\n        ./memcpy_test bigfile 1 >> ret_mem_1;\n    done\n\n    for((i=1;i<=20;i++));\n    do\n        echo \"#\"$i >> ret_mem_2;\n        ./memcpy_test bigfile 2 >> ret_mem_2;\n    done\n\n> 为什么不实用K-BEST的方法？\n> \n> 因为测试数据出来之后发现所有数据都在合理范围之内，但最小值明显小于平均值，从实际意义出发，不具备代表性。\n\n## 测试结果\n\n经过统计的计算结果如下：\n\n![性能对比测试结果](http://ww4.sinaimg.cn/large/599be90djw1ecfualjxwcj20wg0khagc.jpg)\n\n在sendfile过程中，memcpy比vmsplice要快；而getfile过程中vmsplice却比memcpy快。\n\n总时间上memcpy小幅优于vmsplice，大约仅有2.6%的领先。\n\n### 结论\n\n数据上看memcpy好于vmsplice，但前提是内存够用，在我们的测试中即使是1G的数据依然直接全部放置于共享内存中，这在实际编程中是不允许的，肯定要分段进行拷贝，必然会增加锁消耗；而vmsplice之所以慢就是受限于管道的大小，数据实际上是分段传递的，因此在内存占用上，vmsplice会比memcpy少1/3。\n\n因此我的结论是，**在纯内存拷贝这个业务场景下，memcpy与vmsplice基本等效**，并没有谁可以明显由于对手。\n\nsplice函数族更加适用的场景应该是涉及到磁盘数据拷贝的情景中，如果你是犹豫需不需要用vmsplice方法替换已有的memcpy代码的话，我的建议是不需要，memcpy确实已经足够快了。\n\n> 如需转载，请注明出处，谢谢！\n\n### One more thing\n\n更新于1.12\n\n上述内容完成之后我发现自己已经走火入魔了。。睡觉中觉得不甘心，如果不涉及进程间数据拷贝，而是采用splice的机制来完全替换memcpy()函数会是什么情况呢，于是又封装了一个mymemcpy()，功能和参数与memcpy完全一致，代码如下：\n\n    void *mymemcpy(void *dest, const void *src, size_t n)\n    {\n        void *p = src, *q = dest;\n        size_t send;\n        size_t left = n;\n        long ret;\n        struct iovec iov[2];\n        long nr_segs = 1;\n        int flags = 0x1;\n\n        pipe(pipefd);\n\n        while (left > 0)\n        {\n            send = left > PIPE_BUF ? PIPE_BUF : left;\n            iov[0].iov_base = p;\n            iov[0].iov_len = send;\n\n            ret = vmsplice(pipefd[1], &iov[0], nr_segs, flags);\n            if (ret == -1)\n            {\n                perror(\"vmsplice failed\");\n                return NULL;\n            }\n            p += ret;\n\n            iov[1].iov_base = q;\n            iov[1].iov_len = send;\n            ret = vmsplice(pipefd[0], &iov[1], nr_segs, flags);\n            if (ret == -1)\n            {\n                perror(\"vmsplice failed\");\n                return NULL;\n            }\n            q += ret;\n\n            left -= ret;\n            //printf(\"mode[%d] sendfile() left=%d ret=%d\\n\", mode, left, ret);\n        }\n\n        close(pipefd[0]);\n        close(pipefd[1]);\n        return dest;\n    }\n\n具体过程是从src映射到pipe，然后再从pipe映射到dest，等于是把memcpy的操作进行了两遍。\n\n哦了，开始编译：\n\n    gcc zmd5.c mymemcpy.c -o mymemcpy \n\n生成一个随机内容的文件：\n\n    dd if=/dev/urandom of=random bs=1M count=1000\n\n执行测试脚本另一个run_mymemcpy_test.sh：\n\n    #!/bin/zsh\n    for((i=1;i<=20;i++));\n    do\n        echo \"#\"$i >> ret_mymem_1;\n        ./mymemcpy random >> ret_mymem_1;\n    done\n\n抽出测试数据中的有用部分：\n\n    cat ret_mymem_1 | grep time\n\n得到的结果如下，由于结果非常稳定，我就不进行统计计算了，直接看原始数据：\n\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.150000s\n    mymemcpy() CPU time: 0.230000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.250000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.250000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.250000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.250000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.250000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.250000s\n    memcpy() CPU time: 0.140000s\n    mymemcpy() CPU time: 0.240000s\n\n可以看到采用vmsplice封装的mymemcpy()方法消耗的时间（0.24~0.25）接近于memcpy()的两倍（稳定0.14），这也应证了上面的结论，即**memcpy过程中并没有进行用户空间到内核空间的拷贝，而是直接在用户空间之间进行**。大家可以放心地使用memcpy了。\n\n### 测试代码和原始数据\n\nmemcpy vs vmsplice [测试代码](https://github.com/buaazp/work/blob/master/memcpy_test.c)\n\nmymemcpy vs memcpy [测试代码](https://github.com/buaazp/work/blob/master/mymemcpy.c)\n\n测试结果原始数据，ret_mem_1、ret_mem_2和ret_mymem_1文件 [github地址](https://github.com/buaazp/work)\n","html":"<h1 id=\"memcpyvmsplice\">内存拷贝memcpy()与vmsplice()性能对比</h1>\n\n<p><a href=\"http://weibo.com/buaazp\">@招牌疯子</a></p>\n\n<h2 id=\"\">综述</h2>\n\n<p>在上一篇文章<a href=\"http://blog.buaa.us/data-copy-between-processes/\">《进程间大数据拷贝方法调研》</a>中介绍和对比了三种A进程读取文件然后拷贝给B进程的方法，测试结果显示在涉及到内存与磁盘间的数据传输时，splice方法由于避免了内核缓冲区与用户缓冲区之间的多次数据拷贝，表现最好。但是由于这种对比限定在包含I/O读写，且进程不能对数据进行修改的特殊情景中，毕竟在实际情况下不太常见，理论意义大于实际意义。</p>\n\n<p>那本文要探讨的情景，在实际编程过程中就十分常见了：</p>\n\n<p><strong>A进程的内存中有一大块数据，要传递给B进程。</strong></p>\n\n<p>为了解决这种情景下的数据传输问题，本文将要对比的两种方法为：</p>\n\n<ol>\n<li>申请共享内存，A进程将数据memcpy进去，B进程再memcpy出来。  </li>\n<li>申请一对管道，A进程将数据vmsplice到管道写端，B进程从读端再vmsplice出来放入自己的用户空间。</li>\n</ol>\n\n<p>实际应用中我们大多使用第一种方法，因为感觉上memcpy已经足够快了，那么它们到底谁更快呢，我们还是要用测试数据来说话。</p>\n\n<h2 id=\"\">测试方法</h2>\n\n<h3 id=\"memcpy\">memcpy方法</h3>\n\n<p>对于memcpy方法，统计的是从A进程用户空间拷贝数据到共享内存所消耗的时间和从共享内存拷贝数据到B进程用户空间的时间。</p>\n\n<blockquote>\n  <p>可能有同学会问，为啥不直接将变量定义到共享内存中，这样A修改了之后B直接使用即可，无需拷贝。</p>\n  \n  <p>这是因为我们期望的状态是A进程有一份自己的数据，B进程也有一份自己的数据，它们是互不干扰的，在大部分时间里是可以保持数据不一致这个状态的，只有需要的时候才进行拷贝。</p>\n</blockquote>\n\n<p>代码如下：</p>\n\n<p><em>sendfile()</em></p>\n\n<pre><code>if(mode == 1)\n{\n    void *shm = shared_memory;\n    void *src = usrmem;\n\n    int size=BUF_SIZE,total=0;\n    while(total &lt; len)\n    {\n        size = len - total &gt; BUF_SIZE ? BUF_SIZE : len-total;\n        memcpy(shm,src,size);\n        shm += size;\n        src += size;\n        total += size;\n        //printf(\"mode[%d] sendfile() total=%d size=%d\\n\", mode, total, size);\n    }\n    pthread_mutex_unlock(&amp;lock);\n    //cal_md5(shared_memory, len, md5);\n}\n</code></pre>\n\n<p>usrmem是用户空间中的内存，在sendfile之前已经灌入了数据。并且在A进程向共享内存拷贝过程中上锁，以阻塞B进程对共享内存的读操作，结束之后解锁。</p>\n\n<p><em>getfile()</em></p>\n\n<pre><code>if(mode == 1)\n{\n    void *shm = shared_memory;\n    void *dst = usrmem;\n\n    pthread_mutex_lock(&amp;lock);\n    int size=BUF_SIZE,total=0;\n    while(total &lt; len)\n    {\n        size = len - total &gt; BUF_SIZE ? BUF_SIZE : len-total;\n        memcpy(dst, shm, size);\n        shm += size;\n        dst += size;\n        total += size;\n        //printf(\"mode[%d] getfile() total=%d size=%d\\n\", mode, total, size);\n    }\n    pthread_mutex_unlock(&amp;lock);\n}\n</code></pre>\n\n<p>读数据的时候也上锁，防止过程中被写入数据造成污染，结束之后解锁。</p>\n\n<h3 id=\"vmsplice\">vmsplice方法</h3>\n\n<p>vmsplice是splice函数族中的一个，它用于映射用户空间到内核空间（载体是管道），使得用户进程可以直接操作内核缓冲区的数据。</p>\n\n<p>splice函数族包括：</p>\n\n<blockquote>\n  <p>long splice(int fd<em>in, off</em>t *off<em>in,int fd</em>out, off<em>t *off</em>out,size_t len,unsignedint flags);</p>\n  \n  <p>long tee(int fd<em>in,int fd</em>out,size_t len,unsignedint flags);</p>\n  \n  <p>long vmsplice(int fd,conststruct iovec *iov, unsignedlong nr_segs,unsignedint flags);</p>\n  \n  <p>ssize<em>t sendfile(int out</em>fd,int in<em>fd, off</em>t *offset,size_tcount);</p>\n</blockquote>\n\n<p>可以看到这一系列函数把操作对象为用户空间、内核空间、文件流fd的映射操作全覆盖了，具体各个函数的用法可以查看man文档。</p>\n\n<p><em>sendfile()</em></p>\n\n<pre><code>else if(mode == 2)\n{\n    char *p = usrmem;\n    size_t send;\n    size_t left = len;\n    long ret;\n    struct iovec iov;\n    long nr_segs = 1;\n    int flags = 0x1;\n\n    while (left &gt; 0)\n    {\n        send = left &gt; PIPE_BUF ? PIPE_BUF : left;\n        iov.iov_base = p;\n        iov.iov_len = send;\n\n        ret = vmsplice(pipefd[1], &amp;iov, nr_segs, flags);\n        if (ret == -1)\n        {\n            perror(\"vmsplice failed\");\n            return;\n        }\n\n        left -= ret;\n        p += ret;\n        //printf(\"mode[%d] sendfile() left=%d ret=%d\\n\", mode, left, ret);\n    }\n}\n</code></pre>\n\n<p>vmsplice想要操作用户空间依赖于struct iovec对象，它有两个成员，iov<em>base是指向用户空间的指针，iov</em>len是用户空间的大小，由于管道有大小限制（Linux下默认4096），因此在做vmsplice的时候需要不断更新struct iovec的内容。</p>\n\n<p><em>getfile()</em></p>\n\n<pre><code>else if(mode == 2)\n{\n    char *p = usrmem;\n    size_t get;\n    size_t left = len;\n    long ret;\n    struct iovec iov;\n    long nr_segs = 1;\n    int flags = 0x1;\n\n    while (left &gt; 0)\n    {\n        get = left &gt; PIPE_BUF ? PIPE_BUF : left;\n        iov.iov_base = p;\n        iov.iov_len = get;\n\n        ret = vmsplice(pipefd[0], &amp;iov, nr_segs, flags);\n        if (ret == -1)\n        {\n            perror(\"vmsplice failed\");\n            return;\n        }\n\n        left -= ret;\n        p += ret;\n        //printf(\"mode[%d] getfile() left=%d ret=%d\\n\", mode, left, ret);\n    }\n}\n</code></pre>\n\n<p>getfile的时候只需要把第一个参数设为管道的读端，因为vmsplice实际上是一个映射过程，与拷贝不同，它不关心两个参数谁是source谁是destination。</p>\n\n<h3 id=\"\">对比方法</h3>\n\n<p>对比的是上述四个过程所消耗的CPU时间，为了检验数据拷贝是否完成，在拷贝开始前和结束后对A、B进程的用户空间计算MD5值。编译测试程序：</p>\n\n<pre><code>gcc zmd5.c memcpy_test.c -o memcpy_test\n</code></pre>\n\n<p>测试对象原始数据是一个1G大小的文件，每种方法各测试20次，统计sendfile过程和getfile过程的时间，两者相加是整个数据拷贝的时间，计算平均值进行对比。代码所在文件夹下有一个脚本run<em>memcpy</em>test.sh来做这件事：</p>\n\n<pre><code>#!/bin/zsh\nfor((i=1;i&lt;=20;i++));\ndo\n    echo \"#\"$i &gt;&gt; ret_mem_1;\n    ./memcpy_test bigfile 1 &gt;&gt; ret_mem_1;\ndone\n\nfor((i=1;i&lt;=20;i++));\ndo\n    echo \"#\"$i &gt;&gt; ret_mem_2;\n    ./memcpy_test bigfile 2 &gt;&gt; ret_mem_2;\ndone\n</code></pre>\n\n<blockquote>\n  <p>为什么不实用K-BEST的方法？</p>\n  \n  <p>因为测试数据出来之后发现所有数据都在合理范围之内，但最小值明显小于平均值，从实际意义出发，不具备代表性。</p>\n</blockquote>\n\n<h2 id=\"\">测试结果</h2>\n\n<p>经过统计的计算结果如下：</p>\n\n<p><img src=\"http://ww4.sinaimg.cn/large/599be90djw1ecfualjxwcj20wg0khagc.jpg\" alt=\"性能对比测试结果\" /></p>\n\n<p>在sendfile过程中，memcpy比vmsplice要快；而getfile过程中vmsplice却比memcpy快。</p>\n\n<p>总时间上memcpy小幅优于vmsplice，大约仅有2.6%的领先。</p>\n\n<h3 id=\"\">结论</h3>\n\n<p>数据上看memcpy好于vmsplice，但前提是内存够用，在我们的测试中即使是1G的数据依然直接全部放置于共享内存中，这在实际编程中是不允许的，肯定要分段进行拷贝，必然会增加锁消耗；而vmsplice之所以慢就是受限于管道的大小，数据实际上是分段传递的，因此在内存占用上，vmsplice会比memcpy少1/3。</p>\n\n<p>因此我的结论是，<strong>在纯内存拷贝这个业务场景下，memcpy与vmsplice基本等效</strong>，并没有谁可以明显由于对手。</p>\n\n<p>splice函数族更加适用的场景应该是涉及到磁盘数据拷贝的情景中，如果你是犹豫需不需要用vmsplice方法替换已有的memcpy代码的话，我的建议是不需要，memcpy确实已经足够快了。</p>\n\n<blockquote>\n  <p>如需转载，请注明出处，谢谢！</p>\n</blockquote>\n\n<h3 id=\"onemorething\">One more thing</h3>\n\n<p>更新于1.12</p>\n\n<p>上述内容完成之后我发现自己已经走火入魔了。。睡觉中觉得不甘心，如果不涉及进程间数据拷贝，而是采用splice的机制来完全替换memcpy()函数会是什么情况呢，于是又封装了一个mymemcpy()，功能和参数与memcpy完全一致，代码如下：</p>\n\n<pre><code>void *mymemcpy(void *dest, const void *src, size_t n)\n{\n    void *p = src, *q = dest;\n    size_t send;\n    size_t left = n;\n    long ret;\n    struct iovec iov[2];\n    long nr_segs = 1;\n    int flags = 0x1;\n\n    pipe(pipefd);\n\n    while (left &gt; 0)\n    {\n        send = left &gt; PIPE_BUF ? PIPE_BUF : left;\n        iov[0].iov_base = p;\n        iov[0].iov_len = send;\n\n        ret = vmsplice(pipefd[1], &amp;iov[0], nr_segs, flags);\n        if (ret == -1)\n        {\n            perror(\"vmsplice failed\");\n            return NULL;\n        }\n        p += ret;\n\n        iov[1].iov_base = q;\n        iov[1].iov_len = send;\n        ret = vmsplice(pipefd[0], &amp;iov[1], nr_segs, flags);\n        if (ret == -1)\n        {\n            perror(\"vmsplice failed\");\n            return NULL;\n        }\n        q += ret;\n\n        left -= ret;\n        //printf(\"mode[%d] sendfile() left=%d ret=%d\\n\", mode, left, ret);\n    }\n\n    close(pipefd[0]);\n    close(pipefd[1]);\n    return dest;\n}\n</code></pre>\n\n<p>具体过程是从src映射到pipe，然后再从pipe映射到dest，等于是把memcpy的操作进行了两遍。</p>\n\n<p>哦了，开始编译：</p>\n\n<pre><code>gcc zmd5.c mymemcpy.c -o mymemcpy \n</code></pre>\n\n<p>生成一个随机内容的文件：</p>\n\n<pre><code>dd if=/dev/urandom of=random bs=1M count=1000\n</code></pre>\n\n<p>执行测试脚本另一个run<em>mymemcpy</em>test.sh：</p>\n\n<pre><code>#!/bin/zsh\nfor((i=1;i&lt;=20;i++));\ndo\n    echo \"#\"$i &gt;&gt; ret_mymem_1;\n    ./mymemcpy random &gt;&gt; ret_mymem_1;\ndone\n</code></pre>\n\n<p>抽出测试数据中的有用部分：</p>\n\n<pre><code>cat ret_mymem_1 | grep time\n</code></pre>\n\n<p>得到的结果如下，由于结果非常稳定，我就不进行统计计算了，直接看原始数据：</p>\n\n<pre><code>memcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.150000s\nmymemcpy() CPU time: 0.230000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.250000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.250000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.250000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.250000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.250000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.250000s\nmemcpy() CPU time: 0.140000s\nmymemcpy() CPU time: 0.240000s\n</code></pre>\n\n<p>可以看到采用vmsplice封装的mymemcpy()方法消耗的时间（0.24~0.25）接近于memcpy()的两倍（稳定0.14），这也应证了上面的结论，即<strong>memcpy过程中并没有进行用户空间到内核空间的拷贝，而是直接在用户空间之间进行</strong>。大家可以放心地使用memcpy了。</p>\n\n<h3 id=\"\">测试代码和原始数据</h3>\n\n<p>memcpy vs vmsplice <a href=\"https://github.com/buaazp/work/blob/master/memcpy_test.c\">测试代码</a></p>\n\n<p>mymemcpy vs memcpy <a href=\"https://github.com/buaazp/work/blob/master/mymemcpy.c\">测试代码</a></p>\n\n<p>测试结果原始数据，ret<em>mem</em>1、ret<em>mem</em>2和ret<em>mymem</em>1文件 <a href=\"https://github.com/buaazp/work\">github地址</a></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402648814188,"created_by":1,"updated_at":1402652360691,"updated_by":1,"published_at":1402648814202,"published_by":1},{"id":14,"uuid":"15a70d28-fe61-4d09-a499-32db621fc31b","title":"搭gitlab的那些坑","slug":"how-to-setup-gitlab","markdown":"## 搭gitlab的那些坑\n\n[@招牌疯子](http://weibo.com/819880808)\n\n手动搭建gitlab的复杂程度想必大家也有所耳闻，我也曾有过在测试机上搭建成功的经验，后来想放到线上机器给团队用，搭建之初，综合利弊，选择了最简单方便的bitnami-gitlab一键安装包，本以为这样就可以轻松加愉悦了，事实证明我还是图样图森破。\n\n首先，我厂的线上机器不允许非自然人账户通过ssh登陆（想ssh登陆服务器的帐号必须通过微盾认证），你新建的git帐号会在系统定期检查时被清理掉，OP给的建议是申请一个假账户绑定一个微盾，作为一个假人存在，但弊端是我得随身携带两个微盾，蛋疼。\n\n于是想别的办法，发现一个线上服务器清理账户的bug，然后采用了一个非常hack的手段保证了这个git帐号长期存在，具体过程由于安全原因在此就不透漏了。\n\n然后是sshd服务的问题，本来想的是从别的机器上复制一份白名单文件`authorized_keys`过来，省的这些老用户再次添加`id_rsa.pub`，同时拷贝的还有`ssh_host_dsa_key`、`ssh_host_rsa_key`等文件，启动sshd服务之后ssh就是连不上，最后发现是这些文件的权限给高了，chown和chmod之后终于ssh连接成功。 \n\n```\n-rw-r–r– 1 git git 275 Mar 10 15:40 authorized_keys\n```\n\n接着发现git push的时候死活就是提交不上去，报repo路径不对神马的错误，按着这条错误的道路追了好久都没法解决。最后偶然想到`authorized_keys`不是空的，于是删掉重新生成之后终于可以了。这个错误是由于`authorized_keys`文件中的pub key跟gitlab数据库中保存的白名单不一致，略坑。\n\ngitlab总算跑起来了，心情舒畅地回家过了个周末，然后准备搭gitlabci。\n\n第一次搭建的时候没想到要装ci组建，bitnami也不支持增量安装，只能想到先备份后恢复的办法。于是先将gitlab数据库dump出来，再把gitlab目录备份，然后重新安装带ci组建的gitlab，导入数据库备份，复制旧的repos和头像等文件到新目录中，恢复gitlab和gitlab-shell的配置文件：\n\n```\nsudo ./ctlscript.sh stop \n./bitnami-gitlab-6.6.3-0-linux-x64-installer.run \nSelect a folder [/opt/gitlab-6.6.3-0]: /opt/gitlab-6.6.3-0 \nsudo ./ctlscript.sh stop \ncd /opt/gitlab-6.6.3-0/apps/gitlab/gitlab-shell \nsudo cp ~/config.yml . \ncd /opt/gitlab-6.6.3-0/apps/gitlab/htdocs/config \nsudo cp ~/gitlab.yml . \ncd /opt/gitlab-6.6.3-0/apps/gitlab/htdocs/public/uploads \nsudo cp -r /opt/gitlab-test/apps/gitlab/htdocs/public/uploads/* . \nsudo ./ctlscript.sh start mysql \nsudo ./use_gitlab mysql -u bitnami -p bitnami_gitlab < ../bitnami_gitlab_bk.sql\nsudo ./ctlscript.sh restart\n```\n\n重新启动，结果，git push又不行了。。。\n\n准备`tail -f`一下gitlab-shell的日志看看，结果新路径下的gitlab-shell竟然完全没有日志，我整个人都不好了，查遍了gitlab下的配置，确认设定的路径没有问题，着实让人费解。\n\n没办法，去洗了把脸继续搞，发现旧的gitlab安装目录下gitlab-shell在打错误日志，实在想不通为什么会调用旧的shell，果断停掉gitlab所有服务，将旧的目录重命名，直接使用`ssh git@10.XX.XXX.XXX`登陆一下，发现打出来 \n\n```\nPTY allocation request failed on channel 0 \n/opt/gitlab-test/ruby/lib/ruby/1.9.1/net/http.rb:763:in `initialize’: Connection refused – connect(2) (Errno::ECONNREFUSED) \n```\n\n的错误，这个路径还是旧的，说明ssh连上的第一时间就定向了错误的地方，开始怀疑是`bashrc`的问题，删掉，依旧不行。\n\n再去洗个脸，终于灵机一动发现gitlab web端添加ssh key进白名单文件的时候，会加上全路径的shell命令，示例如下：\n\n```\ncommand=”/opt/gitlab-test/apps/gitlab/gitlab-shell/bin/gitlab-shell key-1″,no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDQZL/X3JVbozKKQ7MKgfnh+j/uU3m0MQLbwCfxHc3Xf4c2R7wNl6Pr0DXhnX1eBE0Dg0fCLfckMJa2V zippo@MacBook-Pro.local\n```\n\n这下总算找到问题所在了，如果你新安装的路径与旧的不一致，就不能使用备份（光修改`authorized_keys`文件中的shell路径也是不行的，因为数据库已经被恢复，两者不一致又会导致上面所说的问题）。无奈，全新安装不恢复，好在刚搭起来没两天，影响不大，重新启动后ci和gitlab都可用了。\n\n注：忽然想到可以按照原路径重新安装一下，也许可行，明天试试。\n\n补充：2014-03-19已测试，果然按照原路径重新安装一下是可以的。\n\n坑填得好累，不知道该怎么去爱了。。。","html":"<h2 id=\"gitlab\">搭gitlab的那些坑</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>手动搭建gitlab的复杂程度想必大家也有所耳闻，我也曾有过在测试机上搭建成功的经验，后来想放到线上机器给团队用，搭建之初，综合利弊，选择了最简单方便的bitnami-gitlab一键安装包，本以为这样就可以轻松加愉悦了，事实证明我还是图样图森破。</p>\n\n<p>首先，我厂的线上机器不允许非自然人账户通过ssh登陆（想ssh登陆服务器的帐号必须通过微盾认证），你新建的git帐号会在系统定期检查时被清理掉，OP给的建议是申请一个假账户绑定一个微盾，作为一个假人存在，但弊端是我得随身携带两个微盾，蛋疼。</p>\n\n<p>于是想别的办法，发现一个线上服务器清理账户的bug，然后采用了一个非常hack的手段保证了这个git帐号长期存在，具体过程由于安全原因在此就不透漏了。</p>\n\n<p>然后是sshd服务的问题，本来想的是从别的机器上复制一份白名单文件<code>authorized_keys</code>过来，省的这些老用户再次添加<code>id_rsa.pub</code>，同时拷贝的还有<code>ssh_host_dsa_key</code>、<code>ssh_host_rsa_key</code>等文件，启动sshd服务之后ssh就是连不上，最后发现是这些文件的权限给高了，chown和chmod之后终于ssh连接成功。 </p>\n\n<pre><code>-rw-r–r– 1 git git 275 Mar 10 15:40 authorized_keys\n</code></pre>\n\n<p>接着发现git push的时候死活就是提交不上去，报repo路径不对神马的错误，按着这条错误的道路追了好久都没法解决。最后偶然想到<code>authorized_keys</code>不是空的，于是删掉重新生成之后终于可以了。这个错误是由于<code>authorized_keys</code>文件中的pub key跟gitlab数据库中保存的白名单不一致，略坑。</p>\n\n<p>gitlab总算跑起来了，心情舒畅地回家过了个周末，然后准备搭gitlabci。</p>\n\n<p>第一次搭建的时候没想到要装ci组建，bitnami也不支持增量安装，只能想到先备份后恢复的办法。于是先将gitlab数据库dump出来，再把gitlab目录备份，然后重新安装带ci组建的gitlab，导入数据库备份，复制旧的repos和头像等文件到新目录中，恢复gitlab和gitlab-shell的配置文件：</p>\n\n<pre><code>sudo ./ctlscript.sh stop  \n./bitnami-gitlab-6.6.3-0-linux-x64-installer.run \nSelect a folder [/opt/gitlab-6.6.3-0]: /opt/gitlab-6.6.3-0  \nsudo ./ctlscript.sh stop  \ncd /opt/gitlab-6.6.3-0/apps/gitlab/gitlab-shell  \nsudo cp ~/config.yml .  \ncd /opt/gitlab-6.6.3-0/apps/gitlab/htdocs/config  \nsudo cp ~/gitlab.yml .  \ncd /opt/gitlab-6.6.3-0/apps/gitlab/htdocs/public/uploads  \nsudo cp -r /opt/gitlab-test/apps/gitlab/htdocs/public/uploads/* .  \nsudo ./ctlscript.sh start mysql  \nsudo ./use_gitlab mysql -u bitnami -p bitnami_gitlab &lt; ../bitnami_gitlab_bk.sql  \nsudo ./ctlscript.sh restart  \n</code></pre>\n\n<p>重新启动，结果，git push又不行了。。。</p>\n\n<p>准备<code>tail -f</code>一下gitlab-shell的日志看看，结果新路径下的gitlab-shell竟然完全没有日志，我整个人都不好了，查遍了gitlab下的配置，确认设定的路径没有问题，着实让人费解。</p>\n\n<p>没办法，去洗了把脸继续搞，发现旧的gitlab安装目录下gitlab-shell在打错误日志，实在想不通为什么会调用旧的shell，果断停掉gitlab所有服务，将旧的目录重命名，直接使用<code>ssh git@10.XX.XXX.XXX</code>登陆一下，发现打出来 </p>\n\n<pre><code>PTY allocation request failed on channel 0  \n/opt/gitlab-test/ruby/lib/ruby/1.9.1/net/http.rb:763:in `initialize’: Connection refused – connect(2) (Errno::ECONNREFUSED) \n</code></pre>\n\n<p>的错误，这个路径还是旧的，说明ssh连上的第一时间就定向了错误的地方，开始怀疑是<code>bashrc</code>的问题，删掉，依旧不行。</p>\n\n<p>再去洗个脸，终于灵机一动发现gitlab web端添加ssh key进白名单文件的时候，会加上全路径的shell命令，示例如下：</p>\n\n<pre><code>command=”/opt/gitlab-test/apps/gitlab/gitlab-shell/bin/gitlab-shell key-1″,no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDQZL/X3JVbozKKQ7MKgfnh+j/uU3m0MQLbwCfxHc3Xf4c2R7wNl6Pr0DXhnX1eBE0Dg0fCLfckMJa2V zippo@MacBook-Pro.local  \n</code></pre>\n\n<p>这下总算找到问题所在了，如果你新安装的路径与旧的不一致，就不能使用备份（光修改<code>authorized_keys</code>文件中的shell路径也是不行的，因为数据库已经被恢复，两者不一致又会导致上面所说的问题）。无奈，全新安装不恢复，好在刚搭起来没两天，影响不大，重新启动后ci和gitlab都可用了。</p>\n\n<p>注：忽然想到可以按照原路径重新安装一下，也许可行，明天试试。</p>\n\n<p>补充：2014-03-19已测试，果然按照原路径重新安装一下是可以的。</p>\n\n<p>坑填得好累，不知道该怎么去爱了。。。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402649246841,"created_by":1,"updated_at":1403003030232,"updated_by":1,"published_at":1402649246843,"published_by":1},{"id":15,"uuid":"d7095c4b-01d9-4941-a8b4-d5d7a10f8211","title":"benchmark of zimg v2 storage backends","slug":"benchmark-of-zimg-v2","markdown":"## benchmark of zimg v2 storage backends\n\n[@招牌疯子](http://weibo.com/819880808)\n\n这个测试是为了检验zimg采用不同存储后端时读取数据时的性能差异，以便根据测试数据选择最佳的存储方案。\n\n#### 测试方案\n\n提前向三种存储后端（本地磁盘、beansdb和SSDB）中存入测试图片，zimg分别以不同的模式启动并进行测试。测试工具为ab（httpd v2.2.24自带）。为了避免测试工具对测试对象的影响，ab位于测试机A中，zimg及其存储后端位于测试机B中，AB位于同一机房中。  \n测试分别在并发10~1000水平下进行，每种模式在每个并发水平下测试11次，每次发送20000个请求，记录每次测试的QPS和单请求平均完成时间。  \n为了避免误差带来的影响，手工摘除掉11此测试中结果最差的一次，然后取剩余10次的平均值，以代表该模式在对应并发水平下的处理能力。  \n\n#### 测试机\n\n测试机为中等配置的服务器：  \n\n```\nCPU: Intel(R) Xeon(R) CPU E5-2620 @ 2.00GHz, 12 Cores  \nMemory: 32GB  \nOS: CentOS release 5.8  \nDisk: \n  buffered disk reads: 156.39 MB/sec\n  cached reads: 11024.11 MB/sec\nzimg: version 2.0.0 Beta\n```\n\n#### 测试结果\n\n在所有并发水平下local disk模式都优于其他两种存储后端，beansdb和SSDB相差不大，这主要是因为zimg的处理能力还远远达不到其二者的理论极限（beansdb可达到24428，SSDB更是到了夸张的55541），所以成了水桶效应中的短板。  \n以下所有测试结果都是限定zimg请求响应时间在300ms以内级别，更高的并发水平下请求响应时间变长，已经脱离了现实意义。  \n测试结果如下图所示，原始数据链接在最后。  \n\n![qps](http://ww1.sinaimg.cn/large/4c422e03tw1efvtggyr4pj216k0vitec.jpg)\n\n![time](http://ww2.sinaimg.cn/large/4c422e03tw1efvtgg1xsfj216k0vm77f.jpg)\n\n[测试结果详细数据](http://ww4.sinaimg.cn/large/4c422e03tw1efvtzen5soj216v32gkjl.jpg)","html":"<h2 id=\"benchmarkofzimgv2storagebackends\">benchmark of zimg v2 storage backends</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>这个测试是为了检验zimg采用不同存储后端时读取数据时的性能差异，以便根据测试数据选择最佳的存储方案。</p>\n\n<h4 id=\"\">测试方案</h4>\n\n<p>提前向三种存储后端（本地磁盘、beansdb和SSDB）中存入测试图片，zimg分别以不同的模式启动并进行测试。测试工具为ab（httpd v2.2.24自带）。为了避免测试工具对测试对象的影响，ab位于测试机A中，zimg及其存储后端位于测试机B中，AB位于同一机房中。 <br />\n测试分别在并发10~1000水平下进行，每种模式在每个并发水平下测试11次，每次发送20000个请求，记录每次测试的QPS和单请求平均完成时间。 <br />\n为了避免误差带来的影响，手工摘除掉11此测试中结果最差的一次，然后取剩余10次的平均值，以代表该模式在对应并发水平下的处理能力。  </p>\n\n<h4 id=\"\">测试机</h4>\n\n<p>测试机为中等配置的服务器：  </p>\n\n<pre><code>CPU: Intel(R) Xeon(R) CPU E5-2620 @ 2.00GHz, 12 Cores  \nMemory: 32GB  \nOS: CentOS release 5.8  \nDisk:  \n  buffered disk reads: 156.39 MB/sec\n  cached reads: 11024.11 MB/sec\nzimg: version 2.0.0 Beta  \n</code></pre>\n\n<h4 id=\"\">测试结果</h4>\n\n<p>在所有并发水平下local disk模式都优于其他两种存储后端，beansdb和SSDB相差不大，这主要是因为zimg的处理能力还远远达不到其二者的理论极限（beansdb可达到24428，SSDB更是到了夸张的55541），所以成了水桶效应中的短板。 <br />\n以下所有测试结果都是限定zimg请求响应时间在300ms以内级别，更高的并发水平下请求响应时间变长，已经脱离了现实意义。 <br />\n测试结果如下图所示，原始数据链接在最后。  </p>\n\n<p><img src=\"http://ww1.sinaimg.cn/large/4c422e03tw1efvtggyr4pj216k0vitec.jpg\" alt=\"qps\" /></p>\n\n<p><img src=\"http://ww2.sinaimg.cn/large/4c422e03tw1efvtgg1xsfj216k0vm77f.jpg\" alt=\"time\" /></p>\n\n<p><a href=\"http://ww4.sinaimg.cn/large/4c422e03tw1efvtzen5soj216v32gkjl.jpg\">测试结果详细数据</a></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402649334290,"created_by":1,"updated_at":1402652215361,"updated_by":1,"published_at":1402649334292,"published_by":1},{"id":16,"uuid":"773f1190-d618-4f37-abeb-502039984c08","title":"zimg新版本发布，支持分布式存储","slug":"zimg-v2-release","markdown":"## zimg新版本发布，支持分布式存储\n\n#### Distributed Image Storage Server: zimg\n\n[@招牌疯子](http://weibo.com/buaazp)\n\n## 关于zimg\n[zimg](http://zimg.buaa.us/)是我去年开源的一个图片存储程序，主要的优点是可以根据请求实时处理图片，并且进行压缩和存储，一是方便前端用户，二来降低流量。zimg设计之初就是面向中小型应用，是存储量小于TB级别的单机存储方案。它的1.0版本主要竞争对手是基于Nginx+PHP的图片服务器，因为采用了特殊的策略，zimg会比PHP快出很多。  \n然而由于移动设备普及等原因，对图片服务器的存储能力提出了更高的要求，不仅需要支持更大的容量，还要具备冗余备份等功能。我也一直在寻找解决办法，希望使zimg满足大家的需求。  \n\n更新：增加介绍zimg使用手册  \n[http://zimg.buaa.us/guidebook.html](http://zimg.buaa.us/guidebook.html)\n\n## 分布式存储\n无论硬盘多么廉价，单机存储容量总是会有上限的，现在可能连一个中小网站所需要的图片都存不下了。分布式存储将数据分布存储于多台服务器上，一台不够了再加一台，理论上讲存储容量是无上限的。而且提高了存储服务的可靠性，如果机器够多，存储都做了冗余，那么即使某些机器出现故障无法服务，也可以方便地切换到备用服务器上，保证整体服务可用。  \n更高级别的分布式存储是跨IDC的，要实现这样的分布式，不是写一两个程序就可以完成的。像微博图床这种量级的存储，涉及服务器几千台，由多层架构组成，虽然看起来很是高大上，但其实并不适用于中小企业和开发者，或者说我们也不需要这样复杂的架构来存储几百GB的数据。  \n\n## 战斗吧，新版本！\n接下来介绍一下zimg 2.0的存储方案，看它有何新特性，以及是如何实现分布式存储的。  \n### beansdb\n在去年1.0刚发布之后，就有同学给我推荐支持豆瓣的[beansdb](https://github.com/douban/beansdb)，因为beansdb本身就是豆瓣用来存图片的，在线上服务中经过了检验，我也去调研了一番，但是发现他们的分布式策略是用一个又慢又简陋的python脚本来实现的，经过这层proxy之后处理能力只有可怜的几十QPS，这样的性能跟zimg上千的图片处理能力相比实在太鸡肋，于是一度将beansdb放在了考虑范围之外。  \n后来机缘巧合，看到了采用go语言写的beansdb代理程序[beanseye](https://github.com/douban/beanseye)，虽然不具备分布式功能，但可以用来做replication，因为我个人对go语言的喜爱，立刻跑起来进行测试，发现这层代理性能还不错，值得一用。再加上beansdb底层所采用Bitcask引擎，确实是非常适合用来做图片的持久存储，这些原因促使我开始重新考虑是否采用beansdb来作为存储后端。  \n> Bitcask最大的特点是将全部内容的索引存于内存中，数据存于硬盘上，而且数据只追加不删除，使得数据一直是顺序存储的，具有减少磁盘碎片，降低磁头寻址时间（大量写入时）等优点。  \n> 而且beansdb对bitcask的key进行了改进，使得单机存储容量大幅度提高，即使是一台只有8G内存的服务器，也可以存储80TB的图片（平均大小200K）。关于bitcask的内容可以参阅beansdb作者的博文：[《beansdb 卷土重来》](http://www.douban.com/note/122507891/)  \n\nzimg增加beansdb后端是很容易的，因为它采用的是memcached协议，而zimg第一版就支持memcached做cache。如果你希望采用beansdb做后端来存储图片，建议启用两台beansdb服务器做存储，再起一个beanseye做replication，这样数据会同时写入所有存储机上，实现了主从备份，即使一台挂掉beanseye也会自动从其他存储机上获取数据。  \n### SSDB\n[SSDB](https://github.com/ideawu/ssdb)也是一款能持久存储数据的NoSQL数据库，支持的是Redis协议，它的优点是既有丰富数据类型的支持，也能高效的存储（底层采用[LevelDB](https://code.google.com/p/leveldb/)甚至是[rocksdb](https://github.com/facebook/rocksdb)做存储引擎），对于zimg来说，其实只需要用到set和get两个命令，更多的是对它的存储和在线备份功能的期待。  \n> 根据我的简单了解，与bitcask引擎不同，leveldb并不是将所有的key都放在内存中，而是采用多级level的方式进行存储，为了提高内存的利用率，部分热数据会存在内存里，也会极大地提高性能。  \n\n在SSDB支持备份功能后我也曾引入到zimg上进行了简单的测试，发现读取性能跟硬盘存储相近，也是十分值得一用。  \n### 如何选择\nBeansdb和SSDB不仅分别代表了memcached协议和Redis协议，也代表了两种完全不同的底层存储方案，都能极大地利用内存和磁盘，都有主从备份功能，到底谁优谁劣实在难以分辨。于是我对他们进行了测试，希望通过数据来表现各自的能力，进行最终的抉择。  \n> 关于测试工具  \n> 要对图片服务器这种东西进行测试，一直也没有好用的工具，以前我一直使用ab进行性能测试，但是它的缺陷是请求单一，只能代表极限状态，不能很好地还原实际应用场景。  \n> 为此我在Vegeta的基础上写了一个自己的压测工具[stress](https://github.com/buaazp/stress)，stress不仅可以像ab那样对单个请求进行高并发压力测试，也可以读取一个请求序列文件，随机选择其中的请求进行发送，它支持GET和POST方式，自定义header，设定并发和请求数等，同时结果输出也很nice。你可以在一个文件中写入类似下面这些格式的请求进行测试： \n>  \n> ```\n> GET http://127.0.0.1:4869/a87665d54a8c0dcaab04fa88b323eba1?w=200&g=1  \n> ```  \n> ```\n> POST http://127.0.0.1:4869/upload form:5f189.jpeg  \n> ```  \n> 因此stress可以用来做功能、性能、压力和稳定性测试，目前还在开发中，本文中涉及的测试进行时尚未使用，后续增加更多好用的功能。\n\n【第一次简陋的测试】测试在一台服务器上进行，由于图片服务是一种读请求远远大于写请求的特殊存储服务，我的测试就只进行高并发读取，测试结果如下图所示：  \n\n![zimg_storage_mode_test](http://ww1.sinaimg.cn/large/4c422e03tw1efpkuhy2p0j20or0jmwg6.jpg)  \n\n【第二次测试】由于第一次测试样本太小，毕竟选择后端这样重要的事情需要慎重，于是我又进行了一次更加详细的测试，这次测试光是ab执行次数就达到了330次之多，整理统计这些数据都花费了相当长的时间。测试详细情况见[测试报告](http://blog.buaa.us/benchmark-of-zimg-v2/)，此处只贴出关键数据图：  \n\n![qps](http://ww1.sinaimg.cn/large/4c422e03tw1efvtggyr4pj216k0vitec.jpg)\n\n根据上述测试结果显示，本地存储性能最好，SSDB略优于beansdb，但差别微乎其微，考虑到他们各有自己的优点，于是我最终决定：  \n**zimg同时支持beansdb和SSDB**。   \n同时支持多种存储模式的另一个优点是适用性广，因为同时支持了memcached协议和Redis协议，你可以采用其他任何用得顺手的存储后端，比如[Memcachedb](http://memcachedb.org/)、甚至是Redis本身，**以及谁能知道将来会不会又出现其他更加优秀的存储呢**。  \n直接存取磁盘的模式也没有移除，方便那些只想用一台起了zimg的单机做图片服务器的朋友们。  \n### 数据分片\n确定了存储后端之后，其实已经拥有了replication的能力，zimg急需具备数据水平分片的功能。Memcached和Redis协议，数据分片，这些需求放在一起之后有没有觉得很眼熟，没错，如果你的服务中有用到过这两款NoSQL数据库，你肯定也曾想办法解决过它们的分片问题，那么最简单的方案就出来了：twemproxy。  \n[Twemproxy](https://github.com/twitter/twemproxy)是Twitter出的memcached和redis代理，支持数据分片，而且还受到过redis作者本人的赞扬，可见其设计的独到之处。我觉得即使是自己在zimg里写一套分片逻辑，也肯定没有twemproxy性能好。twemproxy使用非常方便，参照zimg包内自带的[样例配置](https://github.com/buaazp/zimg/blob/master/test/zimg.yml)简单修改之后就可以使用。  \n> 需要注意的是，twemproxy不支持memcached的binary protocol，[详情在此](https://github.com/twitter/twemproxy/blob/master/notes/memcache.txt)，因此zimg连接beansdb时默认不使用二进制协议，如果你不需要数据分片，可以简单修改源码来启用。  \n\n由于引入了twemproxy，毕竟是多了一层代理，虽然有人说它最多只会比裸连慢20%，但本着实事求是的原则，我们还是要亲自测试一下才能知道，于是我又做了相关的测试，twemproxy后面各带两个beansdb和SSDB实例，测试结果如图所示：  \n\n![twemproxy_test](http://ww4.sinaimg.cn/large/4c422e03tw1efpkuiq5dpj20lh0jigms.jpg)  \n\n图中结果是三次测试的平均值，可以看到，无论是beansdb还是SSDB，加了twemproxy性能下降极小，直接可以忽略不计。因此我非常建议各位用户在zimg和后端存储之间引入twemproxy代理。\n### 架构\nzimg v2的部署力求简单，最佳的方案是zimg和后端存储分开在不同的机器上，因为zimg涉及压图，属于计算密集型，存储层无论是beansdb还是SSDB，都属于I/O密集型，而且由于zimg可以启用memcached做缓存，正好也可以充分利用机器上的内存，而存储机上的内存会被beansdb和SSDB用到，互不影响同时也不会浪费。那么一图胜千言，请看：  \n\n![architecture_of_zimg_v2](http://ww2.sinaimg.cn/large/4c422e03gw1efpmngazc0j21ik1e6dnk.jpg)\n\n### 其他改动\n至此zimg新版本最重要的功能就介绍完毕了，这段时间零零散散还改了许多其他东西，也一并列在此处：  \n\n```\nNew Features:\n存储key生成规则调整，即使将来引入更多特性也可兼容旧数据\n代码结构调整，用户无需安装libevhtp和hiredis\n连接后端采用长连接提高性能\n简化log输出内容\n消除所有编译警告\n采用lua做配置文件，主要是为以后支持压图脚本\n修复一些遗留bug\n```\n## 别的什么\n首先是后续计划。在存储稳定之后，更多的精力将向图片处理方向上转移，除了支持更多的图片处理功能，现在能想到的有以下几点：  \n\n- 将图片处理的逻辑往lua脚本中转移，这样可以使zimg在无需重新编译甚至无需重启的情况下改变图片处理规则，或者增加新规则，用户也可以自定义自己的处理逻辑，满足不同的需求。  \n- 大家可能已经有所耳闻，我们公司这边有自己开发的图片处理库webimg，性能好于imagemagick，而且不会内存泄露，使用起来也极其方便（webimg库支持c和PHP，最近我又给它增加了lua和go接口），一直有人呼吁开源出来给大家使用，我当然也非常希望，但这是公司的东西，可能需要老大们做决定。如果一旦webimg开源，zimg将果断抛弃imagemagick转向它。\n- 还有一个是老问题了，zimg处理上传的逻辑写得非常搓，又笨又简陋，已经修复的BUG中有好几个与它相关，需要改进。\n- 完善工具和文档，看着别人做的开源项目（比如SSDB）这也有那也有，感觉非常高端大气上档次，只能慢慢补充吧。\n\n然后有人向我咨询zimg的license问题，问我是否可以商业化，答案是可以的，希望大家随便用随便改，如果你的公司或者APP采用了zimg，希望在此处留言告知，或者发邮件通知我一声，邮箱是```zp@buaa.us```。  \n开源项目zimg是我利用业余时间无偿完成的，很多个深夜都在写代码中度过，作者并不指望靠它获利，只是诚惶诚恐。如果使用它可以给你带来一点便利，我已非常满足。  \n大家五一假期快乐！  \n\n\n","html":"<h2 id=\"zimg\">zimg新版本发布，支持分布式存储</h2>\n\n<h4 id=\"distributedimagestorageserverzimg\">Distributed Image Storage Server: zimg</h4>\n\n<p><a href=\"http://weibo.com/buaazp\">@招牌疯子</a></p>\n\n<h2 id=\"zimg\">关于zimg</h2>\n\n<p><a href=\"http://zimg.buaa.us/\">zimg</a>是我去年开源的一个图片存储程序，主要的优点是可以根据请求实时处理图片，并且进行压缩和存储，一是方便前端用户，二来降低流量。zimg设计之初就是面向中小型应用，是存储量小于TB级别的单机存储方案。它的1.0版本主要竞争对手是基于Nginx+PHP的图片服务器，因为采用了特殊的策略，zimg会比PHP快出很多。 <br />\n然而由于移动设备普及等原因，对图片服务器的存储能力提出了更高的要求，不仅需要支持更大的容量，还要具备冗余备份等功能。我也一直在寻找解决办法，希望使zimg满足大家的需求。  </p>\n\n<p>更新：增加介绍zimg使用手册 <br />\n<a href=\"http://zimg.buaa.us/guidebook.html\">http://zimg.buaa.us/guidebook.html</a></p>\n\n<h2 id=\"\">分布式存储</h2>\n\n<p>无论硬盘多么廉价，单机存储容量总是会有上限的，现在可能连一个中小网站所需要的图片都存不下了。分布式存储将数据分布存储于多台服务器上，一台不够了再加一台，理论上讲存储容量是无上限的。而且提高了存储服务的可靠性，如果机器够多，存储都做了冗余，那么即使某些机器出现故障无法服务，也可以方便地切换到备用服务器上，保证整体服务可用。 <br />\n更高级别的分布式存储是跨IDC的，要实现这样的分布式，不是写一两个程序就可以完成的。像微博图床这种量级的存储，涉及服务器几千台，由多层架构组成，虽然看起来很是高大上，但其实并不适用于中小企业和开发者，或者说我们也不需要这样复杂的架构来存储几百GB的数据。  </p>\n\n<h2 id=\"\">战斗吧，新版本！</h2>\n\n<p>接下来介绍一下zimg 2.0的存储方案，看它有何新特性，以及是如何实现分布式存储的。  </p>\n\n<h3 id=\"beansdb\">beansdb</h3>\n\n<p>在去年1.0刚发布之后，就有同学给我推荐支持豆瓣的<a href=\"https://github.com/douban/beansdb\">beansdb</a>，因为beansdb本身就是豆瓣用来存图片的，在线上服务中经过了检验，我也去调研了一番，但是发现他们的分布式策略是用一个又慢又简陋的python脚本来实现的，经过这层proxy之后处理能力只有可怜的几十QPS，这样的性能跟zimg上千的图片处理能力相比实在太鸡肋，于是一度将beansdb放在了考虑范围之外。 <br />\n后来机缘巧合，看到了采用go语言写的beansdb代理程序<a href=\"https://github.com/douban/beanseye\">beanseye</a>，虽然不具备分布式功能，但可以用来做replication，因为我个人对go语言的喜爱，立刻跑起来进行测试，发现这层代理性能还不错，值得一用。再加上beansdb底层所采用Bitcask引擎，确实是非常适合用来做图片的持久存储，这些原因促使我开始重新考虑是否采用beansdb来作为存储后端。  </p>\n\n<blockquote>\n  <p>Bitcask最大的特点是将全部内容的索引存于内存中，数据存于硬盘上，而且数据只追加不删除，使得数据一直是顺序存储的，具有减少磁盘碎片，降低磁头寻址时间（大量写入时）等优点。 <br />\n  而且beansdb对bitcask的key进行了改进，使得单机存储容量大幅度提高，即使是一台只有8G内存的服务器，也可以存储80TB的图片（平均大小200K）。关于bitcask的内容可以参阅beansdb作者的博文：<a href=\"http://www.douban.com/note/122507891/\">《beansdb 卷土重来》</a>  </p>\n</blockquote>\n\n<p>zimg增加beansdb后端是很容易的，因为它采用的是memcached协议，而zimg第一版就支持memcached做cache。如果你希望采用beansdb做后端来存储图片，建议启用两台beansdb服务器做存储，再起一个beanseye做replication，这样数据会同时写入所有存储机上，实现了主从备份，即使一台挂掉beanseye也会自动从其他存储机上获取数据。  </p>\n\n<h3 id=\"ssdb\">SSDB</h3>\n\n<p><a href=\"https://github.com/ideawu/ssdb\">SSDB</a>也是一款能持久存储数据的NoSQL数据库，支持的是Redis协议，它的优点是既有丰富数据类型的支持，也能高效的存储（底层采用<a href=\"https://code.google.com/p/leveldb/\">LevelDB</a>甚至是<a href=\"https://github.com/facebook/rocksdb\">rocksdb</a>做存储引擎），对于zimg来说，其实只需要用到set和get两个命令，更多的是对它的存储和在线备份功能的期待。  </p>\n\n<blockquote>\n  <p>根据我的简单了解，与bitcask引擎不同，leveldb并不是将所有的key都放在内存中，而是采用多级level的方式进行存储，为了提高内存的利用率，部分热数据会存在内存里，也会极大地提高性能。  </p>\n</blockquote>\n\n<p>在SSDB支持备份功能后我也曾引入到zimg上进行了简单的测试，发现读取性能跟硬盘存储相近，也是十分值得一用。  </p>\n\n<h3 id=\"\">如何选择</h3>\n\n<p>Beansdb和SSDB不仅分别代表了memcached协议和Redis协议，也代表了两种完全不同的底层存储方案，都能极大地利用内存和磁盘，都有主从备份功能，到底谁优谁劣实在难以分辨。于是我对他们进行了测试，希望通过数据来表现各自的能力，进行最终的抉择。  </p>\n\n<blockquote>\n  <p>关于测试工具 <br />\n  要对图片服务器这种东西进行测试，一直也没有好用的工具，以前我一直使用ab进行性能测试，但是它的缺陷是请求单一，只能代表极限状态，不能很好地还原实际应用场景。 <br />\n  为此我在Vegeta的基础上写了一个自己的压测工具<a href=\"https://github.com/buaazp/stress\">stress</a>，stress不仅可以像ab那样对单个请求进行高并发压力测试，也可以读取一个请求序列文件，随机选择其中的请求进行发送，它支持GET和POST方式，自定义header，设定并发和请求数等，同时结果输出也很nice。你可以在一个文件中写入类似下面这些格式的请求进行测试： </p>\n  \n  <p><code>\n  GET http://127.0.0.1:4869/a87665d54a8c0dcaab04fa88b323eba1?w=200&amp;g=1 <br />\n  </code> <br />\n  <code>\n  POST http://127.0.0.1:4869/upload form:5f189.jpeg <br />\n  </code> <br />\n  因此stress可以用来做功能、性能、压力和稳定性测试，目前还在开发中，本文中涉及的测试进行时尚未使用，后续增加更多好用的功能。</p>\n</blockquote>\n\n<p>【第一次简陋的测试】测试在一台服务器上进行，由于图片服务是一种读请求远远大于写请求的特殊存储服务，我的测试就只进行高并发读取，测试结果如下图所示：  </p>\n\n<p><img src=\"http://ww1.sinaimg.cn/large/4c422e03tw1efpkuhy2p0j20or0jmwg6.jpg\" alt=\"zimg<em>storage</em>mode_test\" title=\"\" />  </p>\n\n<p>【第二次测试】由于第一次测试样本太小，毕竟选择后端这样重要的事情需要慎重，于是我又进行了一次更加详细的测试，这次测试光是ab执行次数就达到了330次之多，整理统计这些数据都花费了相当长的时间。测试详细情况见<a href=\"http://blog.buaa.us/benchmark-of-zimg-v2/\">测试报告</a>，此处只贴出关键数据图：  </p>\n\n<p><img src=\"http://ww1.sinaimg.cn/large/4c422e03tw1efvtggyr4pj216k0vitec.jpg\" alt=\"qps\" /></p>\n\n<p>根据上述测试结果显示，本地存储性能最好，SSDB略优于beansdb，但差别微乎其微，考虑到他们各有自己的优点，于是我最终决定： <br />\n<strong>zimg同时支持beansdb和SSDB</strong>。 <br />\n同时支持多种存储模式的另一个优点是适用性广，因为同时支持了memcached协议和Redis协议，你可以采用其他任何用得顺手的存储后端，比如<a href=\"http://memcachedb.org/\">Memcachedb</a>、甚至是Redis本身，<strong>以及谁能知道将来会不会又出现其他更加优秀的存储呢</strong>。 <br />\n直接存取磁盘的模式也没有移除，方便那些只想用一台起了zimg的单机做图片服务器的朋友们。  </p>\n\n<h3 id=\"\">数据分片</h3>\n\n<p>确定了存储后端之后，其实已经拥有了replication的能力，zimg急需具备数据水平分片的功能。Memcached和Redis协议，数据分片，这些需求放在一起之后有没有觉得很眼熟，没错，如果你的服务中有用到过这两款NoSQL数据库，你肯定也曾想办法解决过它们的分片问题，那么最简单的方案就出来了：twemproxy。 <br />\n<a href=\"https://github.com/twitter/twemproxy\">Twemproxy</a>是Twitter出的memcached和redis代理，支持数据分片，而且还受到过redis作者本人的赞扬，可见其设计的独到之处。我觉得即使是自己在zimg里写一套分片逻辑，也肯定没有twemproxy性能好。twemproxy使用非常方便，参照zimg包内自带的<a href=\"https://github.com/buaazp/zimg/blob/master/test/zimg.yml\">样例配置</a>简单修改之后就可以使用。  </p>\n\n<blockquote>\n  <p>需要注意的是，twemproxy不支持memcached的binary protocol，<a href=\"https://github.com/twitter/twemproxy/blob/master/notes/memcache.txt\">详情在此</a>，因此zimg连接beansdb时默认不使用二进制协议，如果你不需要数据分片，可以简单修改源码来启用。  </p>\n</blockquote>\n\n<p>由于引入了twemproxy，毕竟是多了一层代理，虽然有人说它最多只会比裸连慢20%，但本着实事求是的原则，我们还是要亲自测试一下才能知道，于是我又做了相关的测试，twemproxy后面各带两个beansdb和SSDB实例，测试结果如图所示：  </p>\n\n<p><img src=\"http://ww4.sinaimg.cn/large/4c422e03tw1efpkuiq5dpj20lh0jigms.jpg\" alt=\"twemproxy_test\" title=\"\" />  </p>\n\n<p>图中结果是三次测试的平均值，可以看到，无论是beansdb还是SSDB，加了twemproxy性能下降极小，直接可以忽略不计。因此我非常建议各位用户在zimg和后端存储之间引入twemproxy代理。</p>\n\n<h3 id=\"\">架构</h3>\n\n<p>zimg v2的部署力求简单，最佳的方案是zimg和后端存储分开在不同的机器上，因为zimg涉及压图，属于计算密集型，存储层无论是beansdb还是SSDB，都属于I/O密集型，而且由于zimg可以启用memcached做缓存，正好也可以充分利用机器上的内存，而存储机上的内存会被beansdb和SSDB用到，互不影响同时也不会浪费。那么一图胜千言，请看：  </p>\n\n<p><img src=\"http://ww2.sinaimg.cn/large/4c422e03gw1efpmngazc0j21ik1e6dnk.jpg\" alt=\"architecture_of_zimg_v2\" /></p>\n\n<h3 id=\"\">其他改动</h3>\n\n<p>至此zimg新版本最重要的功能就介绍完毕了，这段时间零零散散还改了许多其他东西，也一并列在此处：  </p>\n\n<pre><code>New Features:  \n存储key生成规则调整，即使将来引入更多特性也可兼容旧数据\n代码结构调整，用户无需安装libevhtp和hiredis\n连接后端采用长连接提高性能\n简化log输出内容\n消除所有编译警告\n采用lua做配置文件，主要是为以后支持压图脚本\n修复一些遗留bug\n</code></pre>\n\n<h2 id=\"\">别的什么</h2>\n\n<p>首先是后续计划。在存储稳定之后，更多的精力将向图片处理方向上转移，除了支持更多的图片处理功能，现在能想到的有以下几点：  </p>\n\n<ul>\n<li>将图片处理的逻辑往lua脚本中转移，这样可以使zimg在无需重新编译甚至无需重启的情况下改变图片处理规则，或者增加新规则，用户也可以自定义自己的处理逻辑，满足不同的需求。  </li>\n<li>大家可能已经有所耳闻，我们公司这边有自己开发的图片处理库webimg，性能好于imagemagick，而且不会内存泄露，使用起来也极其方便（webimg库支持c和PHP，最近我又给它增加了lua和go接口），一直有人呼吁开源出来给大家使用，我当然也非常希望，但这是公司的东西，可能需要老大们做决定。如果一旦webimg开源，zimg将果断抛弃imagemagick转向它。</li>\n<li>还有一个是老问题了，zimg处理上传的逻辑写得非常搓，又笨又简陋，已经修复的BUG中有好几个与它相关，需要改进。</li>\n<li>完善工具和文档，看着别人做的开源项目（比如SSDB）这也有那也有，感觉非常高端大气上档次，只能慢慢补充吧。</li>\n</ul>\n\n<p>然后有人向我咨询zimg的license问题，问我是否可以商业化，答案是可以的，希望大家随便用随便改，如果你的公司或者APP采用了zimg，希望在此处留言告知，或者发邮件通知我一声，邮箱是<code>zp@buaa.us</code>。 <br />\n开源项目zimg是我利用业余时间无偿完成的，很多个深夜都在写代码中度过，作者并不指望靠它获利，只是诚惶诚恐。如果使用它可以给你带来一点便利，我已非常满足。 <br />\n大家五一假期快乐！  </p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402649746254,"created_by":1,"updated_at":1402652225544,"updated_by":1,"published_at":1402649746260,"published_by":1},{"id":17,"uuid":"6bd1a11f-1eac-4165-aaf5-186551e7e8f1","title":"HTTP压测工具stress发布","slug":"stress-release","markdown":"## HTTP压测工具stress发布\n\n[@招牌疯子](http://weibo.com/819880808) zp@buaa.us  \n\n#### 概述\n\nstress是一个HTTP测试工具，采用Go语言编写，它是从[vegeta](https://github.com/tsenart/vegeta)项目改造而成的，但是由于和vegeta的理念不同，底层改动较大，而且以后会向不同的方向发展，所以我将其重新命名，作为新的开源软件进行维护。  \n\nGithub地址：[https://github.com/buaazp/stress](https://github.com/buaazp/stress)\n\n#### 目标\n\nstress的设计目标是支持HTTP协议的各种测试需求，尤其以压力测试、稳定性测试为主，同时可以输出美观的测试结果，便于后期统计和对比。  \n如果你用过ab，常常会苦恼于它只能对一个固定的请求进行压力测试，如果能有一个工具可以对一系列的请求进行测试，则更能反应出应用程序对外服务时所要面临的情况，那么stress就是你所要寻找的东西。\n\n#### 功能\n\nstress拥有vegeta的全部功能，同时增加了一些用起来更加顺手的东西，下面将一一介绍。\n\n- 按指定速率进行压力测试\n\t\n\t`-rate`参数可以指定每秒发送多少个请求，`-duration`参数指定持续时间，用来构造一次恒定速率的测试。`-rate`值并非可以随意设置，它的上限取决与机器性能和操作系统的文件打开数限制。\n\t\n- 按指定并发数进行压力测试\n\t\n\t此模式与ab工具相同，`-c`表示并发数，`-n`指定请求数，用来构造一次模拟高并发情形的压力测试。并发模式与速率模式冲突，不可同时进行。并发模式好比一次启动一定数量的线程，每个线程都在依次地发送请求，直到所有请求发完；而速率模式是恒定每隔一定时间（= 1s / rate）发一个请求出去，这个请求消耗多少时间不会影响下一个请求的发送。\n\n- 自定义header\n\n\t有两种方式进行自定义header：全局header和局部header，两者可同时生效。  \n\t全局header在stress attack命令后加参数`-header=\"host:www.baidu.com\"`进行设定，一旦设定之后本次测试中的所有请求都会带有此header。示例：\n\t\n```\nstress attack -header=\"client:iPhone5S\" -targets=down.txt -c=40 -n=100\n```\n\n局部header只针对单条测试请求生效，设定方法是在METHOD之后直接写入KV对即可，示例：\n\t\n```\nGET client:iPhone5S resize-type:square http://127.0.0.1:8088/6xxkqpcm7j20b40e7myz.jpg\n```\n\t\n- GET请求支持MD5校验\n\n\t在很多测试场景中，我们不能只依靠服务器返回的HTTP status code来判断请求是否正确，比如要测试图片裁剪，虽然返回200 OK了，但是图片不对，依然需要标记为错误，因此stress增加了请求结束后的MD5校验功能，使用方法是在URL之后设定以md5:开头的预期MD5值，示例：\n\t\n```\nGET http://127.0.0.1:4869/5f189d8ec57f5a5a0d3dcba47fa797e2 md5:5f189d8ec57f5a5a0d3dcba47fa797e3\n```\n\t\n如果在MD5校验中失败，该请求的结果会被标记为一个特殊的结果码`250`，stress会认为结果码为`250`的case为MD5校验失败。\n\n\n- POST请求支持以 binary/mutipart-form 形式上传文件\n\n\t如果要测试POST请求发送数据，stress支持在URL之后设定文件名，例如：\n\t\n```\nPOST http://127.0.0.1:4869/ password.txt\n```\n\t\n如果要以form表单形式上传文件，则在文件名之前加`form`关键字即可：\n\t\n```\nPOST http://127.0.0.1:4869/ form:5f189.jpeg\n```\n\t\n如果需要设定form表单中的文件名关键字（默认为filename，具体内容在RFC1867协议中），可以这样构造：\n\t\t\n```\nPOST http://127.0.0.1:4869/ form:yourfilename:5f189.jpeg\n```\n\t\n- 设定测试请求来源\n\n\t默认来自标准输入stdin，因此单条测试请求可以直接通过管道输入给stress，例如：  \n\t\n```\necho \"GET http://127.0.0.1:8088/6xxkqpcm7j20b40e7myz.jpg\" | stress attack  -c=30 -n=1000\n```\n\t\n也可以将一系列的请求写在文件中，stress通过`-targets`参数打开目标文件进行测试。请求文件`down2.txt`示例：\n\t\n```\nGET HOST:ww4.sinaimg.cn resize-type:wap320\nhttp://127.0.0.1:8088/large/a13a0128jw1e6xxkqpcm7j20b40e7myz.jpg\nmd5:ee08f10750475ad209a822ffe24f4e78\nPOST http://127.0.0.1:4869/ form:filename:5f189.jpeg\nGET http://127.0.0.1:4869/5f189d8ec57f5a5a0d3dcba47fa797e2 md5:5f189d8ec57f5a5a0d3dcba47fa797e3\n...\n```\n\t\n请求文件没有大小限制，构造成百上千条请求进行测试毫无压力。然后在stress attack命令中指定该文件即可开始测试：\n\t\n```\nstress attack -targets=down2.txt -c=40 -n=10000\n```\n\t\ntarget文件中的请求默认会以随机顺序进行测试，如有必要可以单独设定发送顺序为依次执行，加上`-ordering=\"sequential\"`即可，示例：\n\t\n```\nstress attack -targets=down2.txt -c=40 -n=10000 -ordering=\"sequential\"\n```\n\t\n- report工具\n\n\tstress attack工具是测试工具，stress report是结果输出工具。在每次attack测试结束之后，原始测试结果默认写入result.json文件中，当然你也可以通过增加`-output=\"result.json\"`参数进行自定义。如果你保存了多次测试的数据，可以通过report工具来转换成更容易阅读的格式：\n\t\n```\nstress report -input=result.json,result2.json,result3.json -output=output.json -reporter=json\n```\n\t\n`-reporter`支持三种格式的输出`[text, json, plot]`，满足不同的需求。\n\t\n- 多核支持\n\n\t为了更好地利用多核的优势，stress支持设定使用核心数，可使用的CPU核心越多，并发测试速度越快，示例如下：\n\t\n```\nstress -cpus=4 attack -targets=down2.txt -c=100 -n=10000\n```\n\n#### 展望\n\n虽然现在的stress已经可以方便地使用，但是我心中还有一些对后续版本的期望。\n\n- 增强型数据校验\n\n\tMD5校验无法适用于动态生成的网页（如包含了时间信息和用户信息的返回结果），最好是支持结果内的关键字匹配。这个功能实现容易，但是会严重降低压力测试的效率，所以还没有加入。\n\n- 动态生成请求\n\n\t有这么一种特殊的需求，请求地址是符合某种规则的一系列地址（例如不同分辨率的图片请求），除了自己生成然后全部写到targets文件中这种笨办法外，最好可以是一条包含正则表达式匹配的请求，由stress动态生成并进行测试。  \n\t再不济，也要支持随机请求，可以称为“乱入模式”，stress构造随机请求压往服务器，看服务器是否出现意外错误。\n\n#### 总结\n\nstress可以用来做并发压力测试（`-c -n`模式），也可以用来做长时间的稳定性、准确性测试（`-rate -duration`模式），测试请求构造简单，功能丰富，尤其是还支持批量测试和数据校验，测试结果美观易读，可以说是测试HTTP服务的一把利器。欢迎试用和反馈。\n\t\n\t\n\t","html":"<h2 id=\"httpstress\">HTTP压测工具stress发布</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a> zp@buaa.us  </p>\n\n<h4 id=\"\">概述</h4>\n\n<p>stress是一个HTTP测试工具，采用Go语言编写，它是从<a href=\"https://github.com/tsenart/vegeta\">vegeta</a>项目改造而成的，但是由于和vegeta的理念不同，底层改动较大，而且以后会向不同的方向发展，所以我将其重新命名，作为新的开源软件进行维护。  </p>\n\n<p>Github地址：<a href=\"https://github.com/buaazp/stress\">https://github.com/buaazp/stress</a></p>\n\n<h4 id=\"\">目标</h4>\n\n<p>stress的设计目标是支持HTTP协议的各种测试需求，尤其以压力测试、稳定性测试为主，同时可以输出美观的测试结果，便于后期统计和对比。 <br />\n如果你用过ab，常常会苦恼于它只能对一个固定的请求进行压力测试，如果能有一个工具可以对一系列的请求进行测试，则更能反应出应用程序对外服务时所要面临的情况，那么stress就是你所要寻找的东西。</p>\n\n<h4 id=\"\">功能</h4>\n\n<p>stress拥有vegeta的全部功能，同时增加了一些用起来更加顺手的东西，下面将一一介绍。</p>\n\n<ul>\n<li><p>按指定速率进行压力测试</p>\n\n<p><code>-rate</code>参数可以指定每秒发送多少个请求，<code>-duration</code>参数指定持续时间，用来构造一次恒定速率的测试。<code>-rate</code>值并非可以随意设置，它的上限取决与机器性能和操作系统的文件打开数限制。</p></li>\n<li><p>按指定并发数进行压力测试</p>\n\n<p>此模式与ab工具相同，<code>-c</code>表示并发数，<code>-n</code>指定请求数，用来构造一次模拟高并发情形的压力测试。并发模式与速率模式冲突，不可同时进行。并发模式好比一次启动一定数量的线程，每个线程都在依次地发送请求，直到所有请求发完；而速率模式是恒定每隔一定时间（= 1s / rate）发一个请求出去，这个请求消耗多少时间不会影响下一个请求的发送。</p></li>\n<li><p>自定义header</p>\n\n<p>有两种方式进行自定义header：全局header和局部header，两者可同时生效。 <br />\n全局header在stress attack命令后加参数<code>-header=\"host:www.baidu.com\"</code>进行设定，一旦设定之后本次测试中的所有请求都会带有此header。示例：</p></li>\n</ul>\n\n<pre><code>stress attack -header=\"client:iPhone5S\" -targets=down.txt -c=40 -n=100  \n</code></pre>\n\n<p>局部header只针对单条测试请求生效，设定方法是在METHOD之后直接写入KV对即可，示例：</p>\n\n<pre><code>GET client:iPhone5S resize-type:square http://127.0.0.1:8088/6xxkqpcm7j20b40e7myz.jpg  \n</code></pre>\n\n<ul>\n<li><p>GET请求支持MD5校验</p>\n\n<p>在很多测试场景中，我们不能只依靠服务器返回的HTTP status code来判断请求是否正确，比如要测试图片裁剪，虽然返回200 OK了，但是图片不对，依然需要标记为错误，因此stress增加了请求结束后的MD5校验功能，使用方法是在URL之后设定以md5:开头的预期MD5值，示例：</p></li>\n</ul>\n\n<pre><code>GET http://127.0.0.1:4869/5f189d8ec57f5a5a0d3dcba47fa797e2 md5:5f189d8ec57f5a5a0d3dcba47fa797e3  \n</code></pre>\n\n<p>如果在MD5校验中失败，该请求的结果会被标记为一个特殊的结果码<code>250</code>，stress会认为结果码为<code>250</code>的case为MD5校验失败。</p>\n\n<ul>\n<li><p>POST请求支持以 binary/mutipart-form 形式上传文件</p>\n\n<p>如果要测试POST请求发送数据，stress支持在URL之后设定文件名，例如：</p></li>\n</ul>\n\n<pre><code>POST http://127.0.0.1:4869/ password.txt  \n</code></pre>\n\n<p>如果要以form表单形式上传文件，则在文件名之前加<code>form</code>关键字即可：</p>\n\n<pre><code>POST http://127.0.0.1:4869/ form:5f189.jpeg  \n</code></pre>\n\n<p>如果需要设定form表单中的文件名关键字（默认为filename，具体内容在RFC1867协议中），可以这样构造：</p>\n\n<pre><code>POST http://127.0.0.1:4869/ form:yourfilename:5f189.jpeg  \n</code></pre>\n\n<ul>\n<li><p>设定测试请求来源</p>\n\n<p>默认来自标准输入stdin，因此单条测试请求可以直接通过管道输入给stress，例如：  </p></li>\n</ul>\n\n<pre><code>echo \"GET http://127.0.0.1:8088/6xxkqpcm7j20b40e7myz.jpg\" | stress attack  -c=30 -n=1000  \n</code></pre>\n\n<p>也可以将一系列的请求写在文件中，stress通过<code>-targets</code>参数打开目标文件进行测试。请求文件<code>down2.txt</code>示例：</p>\n\n<pre><code>GET HOST:ww4.sinaimg.cn resize-type:wap320  \nhttp://127.0.0.1:8088/large/a13a0128jw1e6xxkqpcm7j20b40e7myz.jpg  \nmd5:ee08f10750475ad209a822ffe24f4e78  \nPOST http://127.0.0.1:4869/ form:filename:5f189.jpeg  \nGET http://127.0.0.1:4869/5f189d8ec57f5a5a0d3dcba47fa797e2 md5:5f189d8ec57f5a5a0d3dcba47fa797e3  \n...\n</code></pre>\n\n<p>请求文件没有大小限制，构造成百上千条请求进行测试毫无压力。然后在stress attack命令中指定该文件即可开始测试：</p>\n\n<pre><code>stress attack -targets=down2.txt -c=40 -n=10000  \n</code></pre>\n\n<p>target文件中的请求默认会以随机顺序进行测试，如有必要可以单独设定发送顺序为依次执行，加上<code>-ordering=\"sequential\"</code>即可，示例：</p>\n\n<pre><code>stress attack -targets=down2.txt -c=40 -n=10000 -ordering=\"sequential\"  \n</code></pre>\n\n<ul>\n<li><p>report工具</p>\n\n<p>stress attack工具是测试工具，stress report是结果输出工具。在每次attack测试结束之后，原始测试结果默认写入result.json文件中，当然你也可以通过增加<code>-output=\"result.json\"</code>参数进行自定义。如果你保存了多次测试的数据，可以通过report工具来转换成更容易阅读的格式：</p></li>\n</ul>\n\n<pre><code>stress report -input=result.json,result2.json,result3.json -output=output.json -reporter=json  \n</code></pre>\n\n<p><code>-reporter</code>支持三种格式的输出<code>[text, json, plot]</code>，满足不同的需求。</p>\n\n<ul>\n<li><p>多核支持</p>\n\n<p>为了更好地利用多核的优势，stress支持设定使用核心数，可使用的CPU核心越多，并发测试速度越快，示例如下：</p></li>\n</ul>\n\n<pre><code>stress -cpus=4 attack -targets=down2.txt -c=100 -n=10000  \n</code></pre>\n\n<h4 id=\"\">展望</h4>\n\n<p>虽然现在的stress已经可以方便地使用，但是我心中还有一些对后续版本的期望。</p>\n\n<ul>\n<li><p>增强型数据校验</p>\n\n<p>MD5校验无法适用于动态生成的网页（如包含了时间信息和用户信息的返回结果），最好是支持结果内的关键字匹配。这个功能实现容易，但是会严重降低压力测试的效率，所以还没有加入。</p></li>\n<li><p>动态生成请求</p>\n\n<p>有这么一种特殊的需求，请求地址是符合某种规则的一系列地址（例如不同分辨率的图片请求），除了自己生成然后全部写到targets文件中这种笨办法外，最好可以是一条包含正则表达式匹配的请求，由stress动态生成并进行测试。 <br />\n再不济，也要支持随机请求，可以称为“乱入模式”，stress构造随机请求压往服务器，看服务器是否出现意外错误。</p></li>\n</ul>\n\n<h4 id=\"\">总结</h4>\n\n<p>stress可以用来做并发压力测试（<code>-c -n</code>模式），也可以用来做长时间的稳定性、准确性测试（<code>-rate -duration</code>模式），测试请求构造简单，功能丰富，尤其是还支持批量测试和数据校验，测试结果美观易读，可以说是测试HTTP服务的一把利器。欢迎试用和反馈。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402649865833,"created_by":1,"updated_at":1402678512169,"updated_by":1,"published_at":1402649865837,"published_by":1},{"id":18,"uuid":"a3087089-ced7-4f0b-a467-398782b637ce","title":"个人博客转移至ghost","slug":"use-ghost-blog","markdown":"## 个人博客转移至ghost\n\n[@招牌疯子](http://weibo.com/819880808)\n\n自己这个博客从拿到`buaa.us`域名开始，就一直用的wordpress，用了好几年了，虽然老是觉得又丑又慢，却也一直懒得整，直到前阵子看到@罗罗磊磊 的[博客](http://luolei.org/)，直接一种被shock到的感觉。然后抽空了解了一下ghost的东西，觉得自己搭一个也未尝不可，心里想着反正罗磊同学已经将自己的主题[开源了](https://github.com/foru17/ghostwill)，直接拿来使用应该会很方便的。\n\n但实际操作起来发现不太合适，毕竟人家是搞前端的，这套主题也是非常花哨，图片超多，我并不喜欢，于是就暂停了迁移计划。后来贼心不死，又陆陆续续尝试了许多主题，总是觉得不太顺手。最后偶然发现了[Ellie](https://github.com/Thomascullen92/Ellie)这个主题，非常清新雅致，深得我心，于是开始改拨起来。\n\n先是增加了多说评论框，由于ghost的URL规则和wordpress非常不同，要想把以前的评论全部转移过来十分费劲，于是索性放弃了，反正之前的留言也不多。\n\n然后开始把罗磊那套主题里的社交分享插件转移到Ellie中来，由于他用了[一淘UX图标](http://ued.alimama.com/fonts/)，对于我这个从来没有搞过前端的门外汉来说可是费了一番功夫，手忙脚乱地把CSS一顿改，好在调好之后效果不错，算是给Ellie主题很好地本地化了一番。\n\n主题能用了之后把以前WP里的文章全部用markdown重写了一遍导入，本地调试通了扔到VPS上一跑，发现慢的吓死人，简直不能忍。于是开始优化，把Google字体库替换成[360的](http://libs.useso.com/)，把jquery和prettify之类的js替换成[staticfile.org](http://www.staticfile.org/)上CDN加速过的，最后把首页的背景图和一淘的字体文件扔到七牛云存储上，使得整个博客加载速度大幅度提升，在此感谢以上各家企业所提供的免费服务，真真是有良心。\n\n经过这一番折腾，我感觉自己已然是一个下三流的前端工程师了。。。。","html":"<h2 id=\"ghost\">个人博客转移至ghost</h2>\n\n<p><a href=\"http://weibo.com/819880808\">@招牌疯子</a></p>\n\n<p>自己这个博客从拿到<code>buaa.us</code>域名开始，就一直用的wordpress，用了好几年了，虽然老是觉得又丑又慢，却也一直懒得整，直到前阵子看到@罗罗磊磊 的<a href=\"http://luolei.org/\">博客</a>，直接一种被shock到的感觉。然后抽空了解了一下ghost的东西，觉得自己搭一个也未尝不可，心里想着反正罗磊同学已经将自己的主题<a href=\"https://github.com/foru17/ghostwill\">开源了</a>，直接拿来使用应该会很方便的。</p>\n\n<p>但实际操作起来发现不太合适，毕竟人家是搞前端的，这套主题也是非常花哨，图片超多，我并不喜欢，于是就暂停了迁移计划。后来贼心不死，又陆陆续续尝试了许多主题，总是觉得不太顺手。最后偶然发现了<a href=\"https://github.com/Thomascullen92/Ellie\">Ellie</a>这个主题，非常清新雅致，深得我心，于是开始改拨起来。</p>\n\n<p>先是增加了多说评论框，由于ghost的URL规则和wordpress非常不同，要想把以前的评论全部转移过来十分费劲，于是索性放弃了，反正之前的留言也不多。</p>\n\n<p>然后开始把罗磊那套主题里的社交分享插件转移到Ellie中来，由于他用了<a href=\"http://ued.alimama.com/fonts/\">一淘UX图标</a>，对于我这个从来没有搞过前端的门外汉来说可是费了一番功夫，手忙脚乱地把CSS一顿改，好在调好之后效果不错，算是给Ellie主题很好地本地化了一番。</p>\n\n<p>主题能用了之后把以前WP里的文章全部用markdown重写了一遍导入，本地调试通了扔到VPS上一跑，发现慢的吓死人，简直不能忍。于是开始优化，把Google字体库替换成<a href=\"http://libs.useso.com/\">360的</a>，把jquery和prettify之类的js替换成<a href=\"http://www.staticfile.org/\">staticfile.org</a>上CDN加速过的，最后把首页的背景图和一淘的字体文件扔到七牛云存储上，使得整个博客加载速度大幅度提升，在此感谢以上各家企业所提供的免费服务，真真是有良心。</p>\n\n<p>经过这一番折腾，我感觉自己已然是一个下三流的前端工程师了。。。。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1402681176069,"created_by":1,"updated_at":1402681176069,"updated_by":1,"published_at":1402681176071,"published_by":1}],"users":[{"id":1,"uuid":"2276fe99-2d00-47a2-969a-dbfdd9dcdf7d","name":"zippo","slug":"zippo","password":"$2a$10$DFzR.vG5Ljv0atctuoRYiOiUUnl9Ohvq6GVCpsXGDdLdwg7jM.nFW","email":"zp@buaa.us","image":"/content/images/2014/Jun/IMG_0016-1.JPG","cover":"/content/images/2014/Jun/h_large_DyZi_4762000024392f75.jpg","bio":"Coder, OpenSource, DataStorageEngeneer. Sina@Beijing 开源爱好者，zimg作者，大规模数据存储工程师。","website":"zp@buaa.us","location":"Beijing","accessibility":null,"status":"active","language":"en_US","meta_title":null,"meta_description":null,"last_login":null,"created_at":1402546665087,"created_by":1,"updated_at":1403002894493,"updated_by":1}],"roles":[{"id":1,"uuid":"0c2d11c8-4e94-4962-b7b9-82c2fd51b834","name":"Administrator","description":"Administrators","created_at":1402546059480,"created_by":1,"updated_at":1402546059480,"updated_by":1},{"id":2,"uuid":"2cde5f4e-2f18-4de6-a0b8-229982cdd06f","name":"Editor","description":"Editors","created_at":1402546059485,"created_by":1,"updated_at":1402546059485,"updated_by":1},{"id":3,"uuid":"ee6de571-cfbd-419d-ae1a-2059b97851e7","name":"Author","description":"Authors","created_at":1402546059489,"created_by":1,"updated_at":1402546059489,"updated_by":1}],"roles_users":[{"id":1,"role_id":1,"user_id":1}],"permissions":[{"id":1,"uuid":"8773b5af-62b2-41bd-aec4-d6f88a740441","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":1402546059493,"created_by":1,"updated_at":1402546059493,"updated_by":1},{"id":2,"uuid":"0be4070c-3e38-4a78-89cd-91c869dfc643","name":"Remove posts","object_type":"post","action_type":"remove","object_id":null,"created_at":1402546059497,"created_by":1,"updated_at":1402546059497,"updated_by":1},{"id":3,"uuid":"27f5b92e-73d9-4554-a9de-0af2787c8170","name":"Create posts","object_type":"post","action_type":"create","object_id":null,"created_at":1402546059503,"created_by":1,"updated_at":1402546059503,"updated_by":1}],"permissions_users":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3}],"sessions":[{"id":"Tiwu0U4fcoBRUsZeF6XpB3T9","expires":1403049857489,"sess":"{\"cookie\":{\"originalMaxAge\":43200000,\"expires\":\"2014-06-18T00:04:17.489Z\",\"httpOnly\":true,\"path\":\"/ghost/\"},\"user\":1,\"_csrfSecret\":\"qzs9371lXnCrJ4HmXG7gF6TG\"}"}],"settings":[{"id":1,"uuid":"37c3be8a-fe67-465d-92ed-1dfd8cbf5a21","key":"databaseVersion","value":"002","type":"core","created_at":1402546059526,"created_by":1,"updated_at":1402546059526,"updated_by":1},{"id":2,"uuid":"77a660e4-c07b-46bb-9e26-a992bc69c3a7","key":"dbHash","value":"895ef27b-e54f-4a9c-b419-ce44b5bd3ee6","type":"core","created_at":1402546059526,"created_by":1,"updated_at":1402546059630,"updated_by":1},{"id":3,"uuid":"95aba65d-c506-4eb3-851c-33496e074f46","key":"nextUpdateCheck","value":"1403089298","type":"core","created_at":1402546059526,"created_by":1,"updated_at":1403002899193,"updated_by":1},{"id":4,"uuid":"f1f4244f-d2fe-44f2-a8fb-0f1bf865be71","key":"displayUpdateNotification","value":"0.4.2","type":"core","created_at":1402546059527,"created_by":1,"updated_at":1403002899194,"updated_by":1},{"id":5,"uuid":"52c2ea8f-4319-4c70-bece-61a9b7994eb8","key":"title","value":"BUAAZP.","type":"blog","created_at":1402546059527,"created_by":1,"updated_at":1402678253349,"updated_by":1},{"id":6,"uuid":"9566f42d-f3f8-427f-8308-d049573046b0","key":"description","value":"Codes Speak Louder than Words.","type":"blog","created_at":1402546059527,"created_by":1,"updated_at":1402678253351,"updated_by":1},{"id":7,"uuid":"8853d09c-a97f-4846-af53-5706505d0337","key":"email","value":"zp@buaa.us","type":"blog","created_at":1402546059527,"created_by":1,"updated_at":1402678253352,"updated_by":1},{"id":8,"uuid":"f416418f-11a8-4413-9cc2-88f6594cb484","key":"logo","value":"/content/images/2014/Jun/IMG_0016.JPG","type":"blog","created_at":1402546059528,"created_by":1,"updated_at":1402678253353,"updated_by":1},{"id":9,"uuid":"6835a563-006f-45bb-bf34-fb8ac6fdc3ef","key":"cover","value":"/content/images/2014/Jun/wallpaper_1923233.jpg","type":"blog","created_at":1402546059529,"created_by":1,"updated_at":1402678253354,"updated_by":1},{"id":10,"uuid":"36d81262-fdef-422f-83cf-49a638861c02","key":"defaultLang","value":"en_US","type":"blog","created_at":1402546059529,"created_by":1,"updated_at":1402678253357,"updated_by":1},{"id":11,"uuid":"501a1442-56c2-49b2-a096-05316f2ed4ab","key":"postsPerPage","value":"6","type":"blog","created_at":1402546059530,"created_by":1,"updated_at":1402678253358,"updated_by":1},{"id":12,"uuid":"70e350cf-926d-4a07-a2fc-125d3e28d97c","key":"forceI18n","value":"true","type":"blog","created_at":1402546059531,"created_by":1,"updated_at":1402678253359,"updated_by":1},{"id":13,"uuid":"e6e6d88d-a0be-403e-96bc-0c1d99e28125","key":"permalinks","value":"/:slug/","type":"blog","created_at":1402546059531,"created_by":1,"updated_at":1402678253360,"updated_by":1},{"id":14,"uuid":"07d5eae3-461d-4845-8660-f1a757c69267","key":"activeTheme","value":"Ellie","type":"theme","created_at":1402546059532,"created_by":1,"updated_at":1402678253361,"updated_by":1},{"id":15,"uuid":"d552f550-0847-4fae-a868-7c637911f9ab","key":"activeApps","value":"[]","type":"app","created_at":1402546059532,"created_by":1,"updated_at":1402678253363,"updated_by":1},{"id":16,"uuid":"b11d63e9-113e-4786-9e4b-73f88d86096d","key":"installedApps","value":"[]","type":"app","created_at":1402546059532,"created_by":1,"updated_at":1403007182422,"updated_by":1}],"tags":[{"id":1,"uuid":"73fc2f6f-7ba7-480a-bbdd-e99685915980","name":"Getting Started","slug":"getting-started","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1402546059473,"created_by":1,"updated_at":1402546059473,"updated_by":1}],"posts_tags":[]}}